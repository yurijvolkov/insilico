{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T20:24:22.332728Z",
     "start_time": "2018-05-01T20:24:20.272663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"526f8682-c44f-493e-aea0-0c1a7914981b\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '526f8682-c44f-493e-aea0-0c1a7914981b' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '526f8682-c44f-493e-aea0-0c1a7914981b' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.14.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.14.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.14.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"526f8682-c44f-493e-aea0-0c1a7914981b\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as mtcs\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.feature_selection as fs\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model.base import BaseEstimator\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_notebook\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from xgboost import XGBClassifier\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "%matplotlib inline\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:07:44.320796Z",
     "start_time": "2018-05-01T21:07:43.940092Z"
    }
   },
   "outputs": [],
   "source": [
    "ALL = 1524\n",
    "\n",
    "def weight_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.random_normal(shape=shape,\n",
    "                                                              stddev=0.1))\n",
    "def bias_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.constant(0.1, shape=shape))\n",
    "\n",
    "def elastic_net(x, l1, l2):\n",
    "    return l1 * ( (1-l2) / 2 * tf.norm(x, 2) ** 2 + \n",
    "                   l2 * tf.norm(x, 1))\n",
    "\n",
    "def batch_data(*matrxs, batch_size):\n",
    "    for batch_i in range(matrxs[0].shape[0] // batch_size):\n",
    "        yield tuple(x.iloc[batch_i * batch_size : (batch_i + 1) * batch_size] \n",
    "                     for x in matrxs)\n",
    "        \n",
    "def split_data3(data, train_size=0.5, test_size=0.25, validate_size=0.25):\n",
    "    train_data, test_val_data = ms.train_test_split(data, train_size=train_size)\n",
    "    test_data, val_data = ms.train_test_split(test_val_data,\n",
    "                                              train_size=test_size / (test_size+validate_size))\n",
    "    \n",
    "    train_X, train_y = train_data.drop('y', axis=1), train_data[['y']]\n",
    "    test_X, test_y = test_data.drop('y', axis=1), test_data[['y']]\n",
    "    validate_X, validate_y = val_data.drop('y', axis=1), val_data[['y']]\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y, validate_X, validate_y)\n",
    "\n",
    "def split_data2(data, train_size, test_size):\n",
    "    train_data, test_data = ms.train_test_split(data, train_size=train_size,\n",
    "                                                stratify=data.y, shuffle=True)\n",
    "    train_X, train_y = train_data.drop('y', axis=1), train_data[['y']]\n",
    "    test_X, test_y = test_data.drop('y', axis=1), test_data[['y']]\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y)\n",
    "    \n",
    "def drop_correllated_ftrs(data, corr):\n",
    "    y = data.y\n",
    "    data = data.drop('y', axis=1)\n",
    "    \n",
    "    corr_matrix = data.corr(corr).abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]\n",
    "    const = list(filter(lambda col: len(data[col].unique()) == 1, data.columns ))\n",
    "    to_drop += const\n",
    "    data = data.drop(to_drop, axis=1)\n",
    "    \n",
    "    data['y'] = y\n",
    "    print(f\"Dropped {len(to_drop)} correlated features\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def pp_pipeline(data):\n",
    "    scaler = pp.StandardScaler()\n",
    "    \n",
    "    data_pp = scaler.fit_transform(data)\n",
    "    \n",
    "    data_pp = pd.DataFrame(data_pp,\n",
    "                        index=data.index,\n",
    "                        columns=data.columns)\n",
    "    \n",
    "    #restore target\n",
    "    data_pp.y = data.y\n",
    "    \n",
    "    data_pp = drop_correllated_ftrs(data_pp, 'pearson')\n",
    "    data_pp = drop_correllated_ftrs(data_pp, 'spearman')\n",
    "    \n",
    "    return data_pp\n",
    "\n",
    "def main_metric(y_true, y_pred):\n",
    "    return mtcs.accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading&preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:07:47.598311Z",
     "start_time": "2018-05-01T21:07:46.723154Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature-0</th>\n",
       "      <th>feature-1</th>\n",
       "      <th>feature-2</th>\n",
       "      <th>feature-3</th>\n",
       "      <th>feature-4</th>\n",
       "      <th>feature-5</th>\n",
       "      <th>feature-6</th>\n",
       "      <th>feature-7</th>\n",
       "      <th>feature-8</th>\n",
       "      <th>feature-9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature-1515</th>\n",
       "      <th>feature-1516</th>\n",
       "      <th>feature-1517</th>\n",
       "      <th>feature-1518</th>\n",
       "      <th>feature-1519</th>\n",
       "      <th>feature-1520</th>\n",
       "      <th>feature-1521</th>\n",
       "      <th>feature-1522</th>\n",
       "      <th>feature-1523</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.977273</td>\n",
       "      <td>6.758452</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>10.792929</td>\n",
       "      <td>160.801682</td>\n",
       "      <td>151.109783</td>\n",
       "      <td>1.791689</td>\n",
       "      <td>6.818675</td>\n",
       "      <td>8.138413</td>\n",
       "      <td>8.270161</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658393</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.540632</td>\n",
       "      <td>4.953183</td>\n",
       "      <td>5.351562</td>\n",
       "      <td>5.311048</td>\n",
       "      <td>5.560922</td>\n",
       "      <td>5.643015</td>\n",
       "      <td>5.715999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.408163</td>\n",
       "      <td>5.933978</td>\n",
       "      <td>2.816327</td>\n",
       "      <td>5.877551</td>\n",
       "      <td>162.949911</td>\n",
       "      <td>76.153796</td>\n",
       "      <td>1.381401</td>\n",
       "      <td>6.002651</td>\n",
       "      <td>5.080499</td>\n",
       "      <td>7.514421</td>\n",
       "      <td>...</td>\n",
       "      <td>4.830811</td>\n",
       "      <td>3.817712</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.426343</td>\n",
       "      <td>4.823804</td>\n",
       "      <td>4.652173</td>\n",
       "      <td>4.795274</td>\n",
       "      <td>4.860781</td>\n",
       "      <td>5.001426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.265306</td>\n",
       "      <td>7.425645</td>\n",
       "      <td>3.734694</td>\n",
       "      <td>13.160998</td>\n",
       "      <td>172.099640</td>\n",
       "      <td>161.790879</td>\n",
       "      <td>1.603976</td>\n",
       "      <td>7.410120</td>\n",
       "      <td>10.114794</td>\n",
       "      <td>8.805738</td>\n",
       "      <td>...</td>\n",
       "      <td>6.397659</td>\n",
       "      <td>4.223177</td>\n",
       "      <td>4.685597</td>\n",
       "      <td>5.116870</td>\n",
       "      <td>5.333926</td>\n",
       "      <td>5.504569</td>\n",
       "      <td>5.797956</td>\n",
       "      <td>6.009581</td>\n",
       "      <td>6.200889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.976744</td>\n",
       "      <td>7.648293</td>\n",
       "      <td>3.837209</td>\n",
       "      <td>14.392765</td>\n",
       "      <td>168.885456</td>\n",
       "      <td>175.277251</td>\n",
       "      <td>1.622298</td>\n",
       "      <td>7.629033</td>\n",
       "      <td>12.180817</td>\n",
       "      <td>9.070719</td>\n",
       "      <td>...</td>\n",
       "      <td>5.879135</td>\n",
       "      <td>4.280132</td>\n",
       "      <td>4.563045</td>\n",
       "      <td>5.007714</td>\n",
       "      <td>5.159773</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.640132</td>\n",
       "      <td>5.472271</td>\n",
       "      <td>5.741399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.320988</td>\n",
       "      <td>6.534011</td>\n",
       "      <td>3.567901</td>\n",
       "      <td>8.913580</td>\n",
       "      <td>163.076959</td>\n",
       "      <td>96.019681</td>\n",
       "      <td>1.380679</td>\n",
       "      <td>6.566695</td>\n",
       "      <td>4.417010</td>\n",
       "      <td>8.058783</td>\n",
       "      <td>...</td>\n",
       "      <td>8.148663</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.173321</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>6.259342</td>\n",
       "      <td>6.626469</td>\n",
       "      <td>7.062406</td>\n",
       "      <td>7.472998</td>\n",
       "      <td>7.829842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.924051</td>\n",
       "      <td>6.134299</td>\n",
       "      <td>3.037975</td>\n",
       "      <td>6.506329</td>\n",
       "      <td>165.707039</td>\n",
       "      <td>82.761541</td>\n",
       "      <td>1.381957</td>\n",
       "      <td>6.187547</td>\n",
       "      <td>4.684599</td>\n",
       "      <td>7.660347</td>\n",
       "      <td>...</td>\n",
       "      <td>6.087556</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.820282</td>\n",
       "      <td>5.183187</td>\n",
       "      <td>5.595176</td>\n",
       "      <td>5.489454</td>\n",
       "      <td>5.604998</td>\n",
       "      <td>5.847522</td>\n",
       "      <td>5.987080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.150000</td>\n",
       "      <td>6.740695</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.214815</td>\n",
       "      <td>164.252922</td>\n",
       "      <td>135.639059</td>\n",
       "      <td>1.620887</td>\n",
       "      <td>6.781702</td>\n",
       "      <td>8.631090</td>\n",
       "      <td>8.248393</td>\n",
       "      <td>...</td>\n",
       "      <td>6.198225</td>\n",
       "      <td>4.471639</td>\n",
       "      <td>4.801970</td>\n",
       "      <td>5.237107</td>\n",
       "      <td>5.493833</td>\n",
       "      <td>5.573816</td>\n",
       "      <td>5.764799</td>\n",
       "      <td>5.865760</td>\n",
       "      <td>5.998937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.833333</td>\n",
       "      <td>6.395508</td>\n",
       "      <td>3.141026</td>\n",
       "      <td>8.717949</td>\n",
       "      <td>163.221967</td>\n",
       "      <td>94.106131</td>\n",
       "      <td>1.435936</td>\n",
       "      <td>6.443753</td>\n",
       "      <td>5.834402</td>\n",
       "      <td>7.904135</td>\n",
       "      <td>...</td>\n",
       "      <td>6.582328</td>\n",
       "      <td>4.600158</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.499726</td>\n",
       "      <td>5.978728</td>\n",
       "      <td>5.995208</td>\n",
       "      <td>6.179952</td>\n",
       "      <td>6.364051</td>\n",
       "      <td>6.481290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.380952</td>\n",
       "      <td>6.152543</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>6.402116</td>\n",
       "      <td>164.380868</td>\n",
       "      <td>128.391104</td>\n",
       "      <td>1.687697</td>\n",
       "      <td>6.232890</td>\n",
       "      <td>4.476844</td>\n",
       "      <td>7.736528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.449988</td>\n",
       "      <td>3.865979</td>\n",
       "      <td>4.506730</td>\n",
       "      <td>4.765906</td>\n",
       "      <td>4.965028</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.595598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.228571</td>\n",
       "      <td>6.608449</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>9.180952</td>\n",
       "      <td>159.167580</td>\n",
       "      <td>180.141749</td>\n",
       "      <td>1.981354</td>\n",
       "      <td>6.690537</td>\n",
       "      <td>8.428546</td>\n",
       "      <td>8.221041</td>\n",
       "      <td>...</td>\n",
       "      <td>5.214936</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.682131</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>5.342334</td>\n",
       "      <td>5.402677</td>\n",
       "      <td>5.303305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.757143</td>\n",
       "      <td>6.241471</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>6.949206</td>\n",
       "      <td>167.005923</td>\n",
       "      <td>97.692813</td>\n",
       "      <td>1.408460</td>\n",
       "      <td>6.289021</td>\n",
       "      <td>5.919754</td>\n",
       "      <td>7.789862</td>\n",
       "      <td>...</td>\n",
       "      <td>6.175997</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>4.716264</td>\n",
       "      <td>5.155457</td>\n",
       "      <td>5.591686</td>\n",
       "      <td>5.680173</td>\n",
       "      <td>5.977302</td>\n",
       "      <td>6.030986</td>\n",
       "      <td>6.214671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.096774</td>\n",
       "      <td>7.206768</td>\n",
       "      <td>3.838710</td>\n",
       "      <td>13.806452</td>\n",
       "      <td>160.469926</td>\n",
       "      <td>127.646528</td>\n",
       "      <td>1.591140</td>\n",
       "      <td>7.228381</td>\n",
       "      <td>11.071685</td>\n",
       "      <td>8.598289</td>\n",
       "      <td>...</td>\n",
       "      <td>7.242977</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>4.639572</td>\n",
       "      <td>5.151845</td>\n",
       "      <td>5.678037</td>\n",
       "      <td>5.850765</td>\n",
       "      <td>6.134888</td>\n",
       "      <td>6.451753</td>\n",
       "      <td>6.793466</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29.833333</td>\n",
       "      <td>6.436864</td>\n",
       "      <td>3.476190</td>\n",
       "      <td>8.296296</td>\n",
       "      <td>162.859109</td>\n",
       "      <td>118.257591</td>\n",
       "      <td>1.600911</td>\n",
       "      <td>6.497038</td>\n",
       "      <td>6.283812</td>\n",
       "      <td>7.960386</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>4.051785</td>\n",
       "      <td>4.519067</td>\n",
       "      <td>4.935373</td>\n",
       "      <td>5.281616</td>\n",
       "      <td>5.221369</td>\n",
       "      <td>5.465948</td>\n",
       "      <td>5.520210</td>\n",
       "      <td>5.499982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.500000</td>\n",
       "      <td>6.700508</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>10.709402</td>\n",
       "      <td>163.669041</td>\n",
       "      <td>145.244215</td>\n",
       "      <td>1.785651</td>\n",
       "      <td>6.764423</td>\n",
       "      <td>7.295706</td>\n",
       "      <td>8.158660</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768126</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>5.326662</td>\n",
       "      <td>5.340239</td>\n",
       "      <td>5.395331</td>\n",
       "      <td>5.454787</td>\n",
       "      <td>5.557552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.851852</td>\n",
       "      <td>7.402900</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>12.979424</td>\n",
       "      <td>170.018323</td>\n",
       "      <td>154.527750</td>\n",
       "      <td>1.476580</td>\n",
       "      <td>7.381493</td>\n",
       "      <td>11.933931</td>\n",
       "      <td>8.867678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.543295</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.534483</td>\n",
       "      <td>6.269883</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>7.973180</td>\n",
       "      <td>161.381053</td>\n",
       "      <td>109.038626</td>\n",
       "      <td>1.618478</td>\n",
       "      <td>6.345124</td>\n",
       "      <td>4.994148</td>\n",
       "      <td>7.791228</td>\n",
       "      <td>...</td>\n",
       "      <td>8.934004</td>\n",
       "      <td>4.644391</td>\n",
       "      <td>5.241747</td>\n",
       "      <td>5.930586</td>\n",
       "      <td>6.610360</td>\n",
       "      <td>7.088878</td>\n",
       "      <td>7.641069</td>\n",
       "      <td>8.134765</td>\n",
       "      <td>8.607916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.837500</td>\n",
       "      <td>6.722804</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>172.043543</td>\n",
       "      <td>93.939718</td>\n",
       "      <td>1.156748</td>\n",
       "      <td>6.710586</td>\n",
       "      <td>6.512587</td>\n",
       "      <td>8.234018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.638355</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>5.043425</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.337538</td>\n",
       "      <td>5.497168</td>\n",
       "      <td>5.568345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55.263158</td>\n",
       "      <td>7.389305</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>12.257310</td>\n",
       "      <td>170.139845</td>\n",
       "      <td>220.606751</td>\n",
       "      <td>1.969099</td>\n",
       "      <td>7.412105</td>\n",
       "      <td>10.847813</td>\n",
       "      <td>8.885381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.617652</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.357510</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>4.789573</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>3.944006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.057692</td>\n",
       "      <td>7.061350</td>\n",
       "      <td>3.942308</td>\n",
       "      <td>11.871795</td>\n",
       "      <td>163.441127</td>\n",
       "      <td>171.586122</td>\n",
       "      <td>1.834505</td>\n",
       "      <td>7.104088</td>\n",
       "      <td>9.456211</td>\n",
       "      <td>8.542274</td>\n",
       "      <td>...</td>\n",
       "      <td>6.706174</td>\n",
       "      <td>4.508108</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.374989</td>\n",
       "      <td>5.679959</td>\n",
       "      <td>5.755742</td>\n",
       "      <td>6.060582</td>\n",
       "      <td>6.240763</td>\n",
       "      <td>6.434747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.886792</td>\n",
       "      <td>7.052642</td>\n",
       "      <td>3.943396</td>\n",
       "      <td>12.100629</td>\n",
       "      <td>164.829574</td>\n",
       "      <td>170.922688</td>\n",
       "      <td>1.833416</td>\n",
       "      <td>7.095513</td>\n",
       "      <td>8.143800</td>\n",
       "      <td>8.514148</td>\n",
       "      <td>...</td>\n",
       "      <td>6.976085</td>\n",
       "      <td>4.572130</td>\n",
       "      <td>5.115746</td>\n",
       "      <td>5.566195</td>\n",
       "      <td>5.936711</td>\n",
       "      <td>6.080505</td>\n",
       "      <td>6.198796</td>\n",
       "      <td>6.333335</td>\n",
       "      <td>6.652742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35.761905</td>\n",
       "      <td>6.858500</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>9.640212</td>\n",
       "      <td>168.739205</td>\n",
       "      <td>141.875852</td>\n",
       "      <td>1.456355</td>\n",
       "      <td>6.864043</td>\n",
       "      <td>11.470610</td>\n",
       "      <td>8.439979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.528571</td>\n",
       "      <td>6.482779</td>\n",
       "      <td>2.942857</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>171.920444</td>\n",
       "      <td>84.572241</td>\n",
       "      <td>1.127633</td>\n",
       "      <td>6.481077</td>\n",
       "      <td>5.269841</td>\n",
       "      <td>8.031493</td>\n",
       "      <td>...</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.672829</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>4.969813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.054795</td>\n",
       "      <td>6.395890</td>\n",
       "      <td>3.164384</td>\n",
       "      <td>8.301370</td>\n",
       "      <td>162.696886</td>\n",
       "      <td>90.885447</td>\n",
       "      <td>1.370996</td>\n",
       "      <td>6.435699</td>\n",
       "      <td>6.796613</td>\n",
       "      <td>7.942436</td>\n",
       "      <td>...</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.652489</td>\n",
       "      <td>5.831882</td>\n",
       "      <td>6.040255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.160000</td>\n",
       "      <td>6.067396</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>167.413784</td>\n",
       "      <td>75.058797</td>\n",
       "      <td>1.241288</td>\n",
       "      <td>6.107552</td>\n",
       "      <td>6.423333</td>\n",
       "      <td>7.651508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.131868</td>\n",
       "      <td>5.762305</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.109890</td>\n",
       "      <td>165.080662</td>\n",
       "      <td>66.885356</td>\n",
       "      <td>1.294529</td>\n",
       "      <td>5.828765</td>\n",
       "      <td>2.738324</td>\n",
       "      <td>7.375662</td>\n",
       "      <td>...</td>\n",
       "      <td>6.448889</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.351858</td>\n",
       "      <td>5.590987</td>\n",
       "      <td>5.834811</td>\n",
       "      <td>6.077642</td>\n",
       "      <td>6.293419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28.432432</td>\n",
       "      <td>6.603514</td>\n",
       "      <td>4.324324</td>\n",
       "      <td>11.027027</td>\n",
       "      <td>153.571767</td>\n",
       "      <td>112.900411</td>\n",
       "      <td>1.738632</td>\n",
       "      <td>6.683730</td>\n",
       "      <td>6.055743</td>\n",
       "      <td>8.059202</td>\n",
       "      <td>...</td>\n",
       "      <td>8.293510</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>5.012301</td>\n",
       "      <td>5.675469</td>\n",
       "      <td>6.310259</td>\n",
       "      <td>6.686641</td>\n",
       "      <td>7.157565</td>\n",
       "      <td>7.635515</td>\n",
       "      <td>8.018008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>6.207650</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>164.257367</td>\n",
       "      <td>85.172313</td>\n",
       "      <td>1.392726</td>\n",
       "      <td>6.258106</td>\n",
       "      <td>5.808081</td>\n",
       "      <td>7.738226</td>\n",
       "      <td>...</td>\n",
       "      <td>7.443490</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>5.422745</td>\n",
       "      <td>6.018593</td>\n",
       "      <td>6.373640</td>\n",
       "      <td>6.751321</td>\n",
       "      <td>7.048332</td>\n",
       "      <td>7.302612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.428571</td>\n",
       "      <td>6.252383</td>\n",
       "      <td>2.885714</td>\n",
       "      <td>7.384127</td>\n",
       "      <td>164.322747</td>\n",
       "      <td>112.473778</td>\n",
       "      <td>1.536709</td>\n",
       "      <td>6.312869</td>\n",
       "      <td>6.333953</td>\n",
       "      <td>7.825781</td>\n",
       "      <td>...</td>\n",
       "      <td>3.791267</td>\n",
       "      <td>3.772761</td>\n",
       "      <td>4.081766</td>\n",
       "      <td>4.514972</td>\n",
       "      <td>4.873765</td>\n",
       "      <td>4.997212</td>\n",
       "      <td>4.848606</td>\n",
       "      <td>4.455074</td>\n",
       "      <td>4.352694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38.658537</td>\n",
       "      <td>6.752610</td>\n",
       "      <td>3.682927</td>\n",
       "      <td>10.753388</td>\n",
       "      <td>159.812730</td>\n",
       "      <td>153.837907</td>\n",
       "      <td>1.814410</td>\n",
       "      <td>6.815551</td>\n",
       "      <td>8.392444</td>\n",
       "      <td>8.278695</td>\n",
       "      <td>...</td>\n",
       "      <td>5.800228</td>\n",
       "      <td>4.098503</td>\n",
       "      <td>4.492841</td>\n",
       "      <td>4.938513</td>\n",
       "      <td>5.120237</td>\n",
       "      <td>5.262042</td>\n",
       "      <td>5.561162</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.681878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.670588</td>\n",
       "      <td>6.292286</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>6.776471</td>\n",
       "      <td>167.982829</td>\n",
       "      <td>85.273347</td>\n",
       "      <td>1.296235</td>\n",
       "      <td>6.326075</td>\n",
       "      <td>4.991830</td>\n",
       "      <td>7.822375</td>\n",
       "      <td>...</td>\n",
       "      <td>6.577992</td>\n",
       "      <td>4.385147</td>\n",
       "      <td>4.798885</td>\n",
       "      <td>5.216633</td>\n",
       "      <td>5.602695</td>\n",
       "      <td>5.857442</td>\n",
       "      <td>6.151851</td>\n",
       "      <td>6.484856</td>\n",
       "      <td>6.339973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>23.849057</td>\n",
       "      <td>6.443296</td>\n",
       "      <td>3.584906</td>\n",
       "      <td>8.490566</td>\n",
       "      <td>165.656310</td>\n",
       "      <td>94.855312</td>\n",
       "      <td>1.429258</td>\n",
       "      <td>6.483247</td>\n",
       "      <td>5.732180</td>\n",
       "      <td>7.917090</td>\n",
       "      <td>...</td>\n",
       "      <td>8.145414</td>\n",
       "      <td>4.375757</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.702949</td>\n",
       "      <td>6.339698</td>\n",
       "      <td>6.834210</td>\n",
       "      <td>7.294229</td>\n",
       "      <td>7.631557</td>\n",
       "      <td>8.007074</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>20.528302</td>\n",
       "      <td>6.044432</td>\n",
       "      <td>3.603774</td>\n",
       "      <td>6.075472</td>\n",
       "      <td>162.424590</td>\n",
       "      <td>80.699025</td>\n",
       "      <td>1.402911</td>\n",
       "      <td>6.109594</td>\n",
       "      <td>3.789570</td>\n",
       "      <td>7.610526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.481855</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.734003</td>\n",
       "      <td>5.287636</td>\n",
       "      <td>5.850225</td>\n",
       "      <td>6.211102</td>\n",
       "      <td>6.591717</td>\n",
       "      <td>6.991033</td>\n",
       "      <td>7.274674</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>20.440000</td>\n",
       "      <td>5.968498</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>161.064334</td>\n",
       "      <td>80.380964</td>\n",
       "      <td>1.456437</td>\n",
       "      <td>6.045898</td>\n",
       "      <td>3.411944</td>\n",
       "      <td>7.531384</td>\n",
       "      <td>...</td>\n",
       "      <td>7.520150</td>\n",
       "      <td>4.166665</td>\n",
       "      <td>4.725173</td>\n",
       "      <td>5.302683</td>\n",
       "      <td>5.881406</td>\n",
       "      <td>6.250458</td>\n",
       "      <td>6.631384</td>\n",
       "      <td>7.030719</td>\n",
       "      <td>7.325005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>5.957881</td>\n",
       "      <td>3.062500</td>\n",
       "      <td>5.509259</td>\n",
       "      <td>164.383290</td>\n",
       "      <td>95.963738</td>\n",
       "      <td>1.496938</td>\n",
       "      <td>6.033144</td>\n",
       "      <td>3.958207</td>\n",
       "      <td>7.545887</td>\n",
       "      <td>...</td>\n",
       "      <td>7.057353</td>\n",
       "      <td>4.131159</td>\n",
       "      <td>4.601413</td>\n",
       "      <td>5.158696</td>\n",
       "      <td>5.690359</td>\n",
       "      <td>6.054403</td>\n",
       "      <td>6.455334</td>\n",
       "      <td>6.873095</td>\n",
       "      <td>6.939417</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>23.375000</td>\n",
       "      <td>6.322417</td>\n",
       "      <td>3.203125</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>162.588954</td>\n",
       "      <td>92.261393</td>\n",
       "      <td>1.451475</td>\n",
       "      <td>6.377366</td>\n",
       "      <td>5.672743</td>\n",
       "      <td>7.837564</td>\n",
       "      <td>...</td>\n",
       "      <td>6.824985</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>5.421641</td>\n",
       "      <td>5.892335</td>\n",
       "      <td>6.042336</td>\n",
       "      <td>6.333613</td>\n",
       "      <td>6.572807</td>\n",
       "      <td>6.736893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>9.073735</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>180.387654</td>\n",
       "      <td>166.508642</td>\n",
       "      <td>1.000430</td>\n",
       "      <td>8.896735</td>\n",
       "      <td>19.768056</td>\n",
       "      <td>10.349211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>18.390244</td>\n",
       "      <td>5.605241</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.731707</td>\n",
       "      <td>158.624476</td>\n",
       "      <td>72.159797</td>\n",
       "      <td>1.549658</td>\n",
       "      <td>5.717783</td>\n",
       "      <td>2.198509</td>\n",
       "      <td>7.184370</td>\n",
       "      <td>...</td>\n",
       "      <td>7.163209</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>4.559126</td>\n",
       "      <td>5.140200</td>\n",
       "      <td>5.685703</td>\n",
       "      <td>5.885409</td>\n",
       "      <td>6.272759</td>\n",
       "      <td>6.609097</td>\n",
       "      <td>6.919128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>23.458333</td>\n",
       "      <td>6.397181</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>166.060607</td>\n",
       "      <td>93.350066</td>\n",
       "      <td>1.430009</td>\n",
       "      <td>6.439448</td>\n",
       "      <td>5.322917</td>\n",
       "      <td>7.869501</td>\n",
       "      <td>...</td>\n",
       "      <td>8.177485</td>\n",
       "      <td>4.297285</td>\n",
       "      <td>4.978456</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>6.322790</td>\n",
       "      <td>6.818753</td>\n",
       "      <td>7.288522</td>\n",
       "      <td>7.655010</td>\n",
       "      <td>8.058613</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>25.809524</td>\n",
       "      <td>5.873886</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>5.962963</td>\n",
       "      <td>162.051123</td>\n",
       "      <td>102.034265</td>\n",
       "      <td>1.656037</td>\n",
       "      <td>5.975364</td>\n",
       "      <td>2.317542</td>\n",
       "      <td>7.431153</td>\n",
       "      <td>...</td>\n",
       "      <td>7.555979</td>\n",
       "      <td>4.073291</td>\n",
       "      <td>4.650383</td>\n",
       "      <td>5.246695</td>\n",
       "      <td>5.735564</td>\n",
       "      <td>6.103816</td>\n",
       "      <td>6.545754</td>\n",
       "      <td>6.979000</td>\n",
       "      <td>7.346393</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>21.414634</td>\n",
       "      <td>5.975237</td>\n",
       "      <td>3.487805</td>\n",
       "      <td>6.926829</td>\n",
       "      <td>161.339153</td>\n",
       "      <td>84.405257</td>\n",
       "      <td>1.534660</td>\n",
       "      <td>6.062522</td>\n",
       "      <td>3.468157</td>\n",
       "      <td>7.496018</td>\n",
       "      <td>...</td>\n",
       "      <td>7.446209</td>\n",
       "      <td>4.212128</td>\n",
       "      <td>4.888468</td>\n",
       "      <td>5.525951</td>\n",
       "      <td>5.975081</td>\n",
       "      <td>6.299380</td>\n",
       "      <td>6.681394</td>\n",
       "      <td>7.067183</td>\n",
       "      <td>7.171165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.884906</td>\n",
       "      <td>2.759259</td>\n",
       "      <td>5.370370</td>\n",
       "      <td>162.831246</td>\n",
       "      <td>74.502082</td>\n",
       "      <td>1.381487</td>\n",
       "      <td>5.956572</td>\n",
       "      <td>3.660108</td>\n",
       "      <td>7.471168</td>\n",
       "      <td>...</td>\n",
       "      <td>5.514688</td>\n",
       "      <td>3.901973</td>\n",
       "      <td>4.327438</td>\n",
       "      <td>4.732904</td>\n",
       "      <td>5.096431</td>\n",
       "      <td>5.080239</td>\n",
       "      <td>5.365684</td>\n",
       "      <td>5.482980</td>\n",
       "      <td>5.400140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>23.318182</td>\n",
       "      <td>6.330339</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>8.045455</td>\n",
       "      <td>163.149857</td>\n",
       "      <td>92.872249</td>\n",
       "      <td>1.484800</td>\n",
       "      <td>6.384780</td>\n",
       "      <td>4.174874</td>\n",
       "      <td>7.815815</td>\n",
       "      <td>...</td>\n",
       "      <td>6.172744</td>\n",
       "      <td>4.060443</td>\n",
       "      <td>4.569543</td>\n",
       "      <td>5.057837</td>\n",
       "      <td>5.539301</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.728170</td>\n",
       "      <td>5.901779</td>\n",
       "      <td>5.917717</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>20.560000</td>\n",
       "      <td>6.123796</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>165.088853</td>\n",
       "      <td>80.788719</td>\n",
       "      <td>1.335059</td>\n",
       "      <td>6.174052</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>7.684390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.238678</td>\n",
       "      <td>3.669951</td>\n",
       "      <td>4.112921</td>\n",
       "      <td>4.525180</td>\n",
       "      <td>4.251170</td>\n",
       "      <td>4.104707</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>22.380952</td>\n",
       "      <td>6.182614</td>\n",
       "      <td>3.349206</td>\n",
       "      <td>7.396825</td>\n",
       "      <td>164.113227</td>\n",
       "      <td>89.449143</td>\n",
       "      <td>1.521852</td>\n",
       "      <td>6.248717</td>\n",
       "      <td>4.873016</td>\n",
       "      <td>7.649627</td>\n",
       "      <td>...</td>\n",
       "      <td>7.231355</td>\n",
       "      <td>4.545951</td>\n",
       "      <td>5.133590</td>\n",
       "      <td>5.739994</td>\n",
       "      <td>6.264469</td>\n",
       "      <td>6.422816</td>\n",
       "      <td>6.771282</td>\n",
       "      <td>7.089077</td>\n",
       "      <td>7.031227</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>25.161290</td>\n",
       "      <td>5.774461</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>4.143369</td>\n",
       "      <td>166.212698</td>\n",
       "      <td>99.236969</td>\n",
       "      <td>1.531852</td>\n",
       "      <td>5.860845</td>\n",
       "      <td>2.388103</td>\n",
       "      <td>7.386058</td>\n",
       "      <td>...</td>\n",
       "      <td>3.954843</td>\n",
       "      <td>3.409496</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.382808</td>\n",
       "      <td>4.539297</td>\n",
       "      <td>4.549261</td>\n",
       "      <td>4.743845</td>\n",
       "      <td>4.850075</td>\n",
       "      <td>4.323304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>18.074074</td>\n",
       "      <td>5.806478</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>164.385174</td>\n",
       "      <td>70.742453</td>\n",
       "      <td>1.348570</td>\n",
       "      <td>5.877785</td>\n",
       "      <td>3.391975</td>\n",
       "      <td>7.397610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.198673</td>\n",
       "      <td>3.630985</td>\n",
       "      <td>4.009603</td>\n",
       "      <td>4.409003</td>\n",
       "      <td>3.997053</td>\n",
       "      <td>4.066888</td>\n",
       "      <td>3.725693</td>\n",
       "      <td>2.784239</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>21.284553</td>\n",
       "      <td>6.179102</td>\n",
       "      <td>3.219512</td>\n",
       "      <td>6.975610</td>\n",
       "      <td>164.084221</td>\n",
       "      <td>83.736404</td>\n",
       "      <td>1.363989</td>\n",
       "      <td>6.230333</td>\n",
       "      <td>4.664747</td>\n",
       "      <td>7.732390</td>\n",
       "      <td>...</td>\n",
       "      <td>8.626406</td>\n",
       "      <td>4.923624</td>\n",
       "      <td>5.447814</td>\n",
       "      <td>5.972218</td>\n",
       "      <td>6.539676</td>\n",
       "      <td>7.003520</td>\n",
       "      <td>7.460310</td>\n",
       "      <td>7.890512</td>\n",
       "      <td>8.276919</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>19.658537</td>\n",
       "      <td>5.914988</td>\n",
       "      <td>3.048780</td>\n",
       "      <td>5.756098</td>\n",
       "      <td>166.863082</td>\n",
       "      <td>77.226327</td>\n",
       "      <td>1.395619</td>\n",
       "      <td>5.985012</td>\n",
       "      <td>2.447832</td>\n",
       "      <td>7.443992</td>\n",
       "      <td>...</td>\n",
       "      <td>6.598232</td>\n",
       "      <td>3.886705</td>\n",
       "      <td>4.389809</td>\n",
       "      <td>4.926801</td>\n",
       "      <td>5.261718</td>\n",
       "      <td>5.494090</td>\n",
       "      <td>5.865848</td>\n",
       "      <td>6.204873</td>\n",
       "      <td>6.369954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>17.720930</td>\n",
       "      <td>5.750228</td>\n",
       "      <td>2.697674</td>\n",
       "      <td>4.604651</td>\n",
       "      <td>164.527087</td>\n",
       "      <td>69.324920</td>\n",
       "      <td>1.357124</td>\n",
       "      <td>5.825972</td>\n",
       "      <td>3.025840</td>\n",
       "      <td>7.340613</td>\n",
       "      <td>...</td>\n",
       "      <td>5.196423</td>\n",
       "      <td>3.676301</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.647990</td>\n",
       "      <td>5.085665</td>\n",
       "      <td>5.169063</td>\n",
       "      <td>5.358942</td>\n",
       "      <td>5.594479</td>\n",
       "      <td>5.380329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>17.225806</td>\n",
       "      <td>5.579190</td>\n",
       "      <td>2.548387</td>\n",
       "      <td>4.193548</td>\n",
       "      <td>161.392477</td>\n",
       "      <td>67.383958</td>\n",
       "      <td>1.452391</td>\n",
       "      <td>5.679165</td>\n",
       "      <td>2.590502</td>\n",
       "      <td>7.175556</td>\n",
       "      <td>...</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.349904</td>\n",
       "      <td>3.682610</td>\n",
       "      <td>4.071161</td>\n",
       "      <td>4.447785</td>\n",
       "      <td>4.003918</td>\n",
       "      <td>4.150055</td>\n",
       "      <td>4.274928</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>25.841270</td>\n",
       "      <td>6.257190</td>\n",
       "      <td>3.301587</td>\n",
       "      <td>7.499118</td>\n",
       "      <td>165.929853</td>\n",
       "      <td>102.114039</td>\n",
       "      <td>1.475191</td>\n",
       "      <td>6.312625</td>\n",
       "      <td>4.628210</td>\n",
       "      <td>7.790568</td>\n",
       "      <td>...</td>\n",
       "      <td>8.508140</td>\n",
       "      <td>4.551242</td>\n",
       "      <td>5.176856</td>\n",
       "      <td>5.812076</td>\n",
       "      <td>6.371825</td>\n",
       "      <td>6.850507</td>\n",
       "      <td>7.327036</td>\n",
       "      <td>7.760788</td>\n",
       "      <td>8.122028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>19.416667</td>\n",
       "      <td>6.040173</td>\n",
       "      <td>2.826389</td>\n",
       "      <td>5.763889</td>\n",
       "      <td>164.563288</td>\n",
       "      <td>76.121662</td>\n",
       "      <td>1.298125</td>\n",
       "      <td>6.090735</td>\n",
       "      <td>4.419416</td>\n",
       "      <td>7.632631</td>\n",
       "      <td>...</td>\n",
       "      <td>6.257668</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>5.105945</td>\n",
       "      <td>5.313206</td>\n",
       "      <td>5.537334</td>\n",
       "      <td>5.620401</td>\n",
       "      <td>5.805135</td>\n",
       "      <td>5.976351</td>\n",
       "      <td>6.091310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>22.877193</td>\n",
       "      <td>5.824621</td>\n",
       "      <td>2.912281</td>\n",
       "      <td>5.130604</td>\n",
       "      <td>162.065226</td>\n",
       "      <td>90.113408</td>\n",
       "      <td>1.522993</td>\n",
       "      <td>5.913365</td>\n",
       "      <td>2.939753</td>\n",
       "      <td>7.426243</td>\n",
       "      <td>...</td>\n",
       "      <td>5.575239</td>\n",
       "      <td>4.056123</td>\n",
       "      <td>4.501198</td>\n",
       "      <td>4.960657</td>\n",
       "      <td>5.184939</td>\n",
       "      <td>5.169418</td>\n",
       "      <td>5.403803</td>\n",
       "      <td>5.604422</td>\n",
       "      <td>5.480118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>18.912281</td>\n",
       "      <td>5.693595</td>\n",
       "      <td>2.929825</td>\n",
       "      <td>5.228070</td>\n",
       "      <td>158.876430</td>\n",
       "      <td>74.254334</td>\n",
       "      <td>1.530849</td>\n",
       "      <td>5.798249</td>\n",
       "      <td>2.397661</td>\n",
       "      <td>7.270916</td>\n",
       "      <td>...</td>\n",
       "      <td>7.533869</td>\n",
       "      <td>4.255613</td>\n",
       "      <td>4.826312</td>\n",
       "      <td>5.403240</td>\n",
       "      <td>5.979993</td>\n",
       "      <td>6.177425</td>\n",
       "      <td>6.654314</td>\n",
       "      <td>7.047680</td>\n",
       "      <td>7.348266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>25.464286</td>\n",
       "      <td>6.616246</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>163.099820</td>\n",
       "      <td>100.684505</td>\n",
       "      <td>1.421288</td>\n",
       "      <td>6.649411</td>\n",
       "      <td>7.740823</td>\n",
       "      <td>8.113419</td>\n",
       "      <td>...</td>\n",
       "      <td>7.724212</td>\n",
       "      <td>4.424847</td>\n",
       "      <td>5.028803</td>\n",
       "      <td>5.577369</td>\n",
       "      <td>6.031136</td>\n",
       "      <td>6.363244</td>\n",
       "      <td>6.711588</td>\n",
       "      <td>7.060610</td>\n",
       "      <td>7.424184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>19.133333</td>\n",
       "      <td>5.909660</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>165.378392</td>\n",
       "      <td>75.050748</td>\n",
       "      <td>1.361830</td>\n",
       "      <td>5.976143</td>\n",
       "      <td>4.137963</td>\n",
       "      <td>7.473203</td>\n",
       "      <td>...</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.481240</td>\n",
       "      <td>3.907010</td>\n",
       "      <td>4.281861</td>\n",
       "      <td>4.720172</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>4.320816</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.342993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>19.211268</td>\n",
       "      <td>5.922814</td>\n",
       "      <td>2.788732</td>\n",
       "      <td>5.802817</td>\n",
       "      <td>163.585967</td>\n",
       "      <td>75.352194</td>\n",
       "      <td>1.368851</td>\n",
       "      <td>5.990213</td>\n",
       "      <td>3.049394</td>\n",
       "      <td>7.502897</td>\n",
       "      <td>...</td>\n",
       "      <td>5.234112</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>4.791650</td>\n",
       "      <td>5.245707</td>\n",
       "      <td>5.667723</td>\n",
       "      <td>5.793776</td>\n",
       "      <td>5.960844</td>\n",
       "      <td>5.839187</td>\n",
       "      <td>5.411088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>27.312500</td>\n",
       "      <td>6.923266</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>170.933703</td>\n",
       "      <td>108.146947</td>\n",
       "      <td>1.321618</td>\n",
       "      <td>6.921481</td>\n",
       "      <td>8.122396</td>\n",
       "      <td>8.348757</td>\n",
       "      <td>...</td>\n",
       "      <td>6.003578</td>\n",
       "      <td>3.921973</td>\n",
       "      <td>4.409763</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.348595</td>\n",
       "      <td>5.559479</td>\n",
       "      <td>5.495630</td>\n",
       "      <td>5.703366</td>\n",
       "      <td>5.926259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>31.785714</td>\n",
       "      <td>6.290479</td>\n",
       "      <td>3.357143</td>\n",
       "      <td>7.658730</td>\n",
       "      <td>161.230794</td>\n",
       "      <td>126.116779</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>6.371939</td>\n",
       "      <td>5.511035</td>\n",
       "      <td>7.836350</td>\n",
       "      <td>...</td>\n",
       "      <td>5.931955</td>\n",
       "      <td>3.789855</td>\n",
       "      <td>4.367864</td>\n",
       "      <td>4.954506</td>\n",
       "      <td>5.430442</td>\n",
       "      <td>5.715330</td>\n",
       "      <td>6.065365</td>\n",
       "      <td>6.387320</td>\n",
       "      <td>5.916728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>20.761905</td>\n",
       "      <td>6.057493</td>\n",
       "      <td>3.119048</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>164.681027</td>\n",
       "      <td>81.668748</td>\n",
       "      <td>1.401802</td>\n",
       "      <td>6.120762</td>\n",
       "      <td>3.509921</td>\n",
       "      <td>7.596294</td>\n",
       "      <td>...</td>\n",
       "      <td>6.846092</td>\n",
       "      <td>3.974998</td>\n",
       "      <td>4.517704</td>\n",
       "      <td>5.125079</td>\n",
       "      <td>5.599347</td>\n",
       "      <td>5.933364</td>\n",
       "      <td>6.335871</td>\n",
       "      <td>6.708575</td>\n",
       "      <td>6.648544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105 rows  1525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature-0  feature-1  feature-2  feature-3   feature-4   feature-5  \\\n",
       "0     37.977273   6.758452   3.636364  10.792929  160.801682  151.109783   \n",
       "1     19.408163   5.933978   2.816327   5.877551  162.949911   76.153796   \n",
       "2     40.265306   7.425645   3.734694  13.160998  172.099640  161.790879   \n",
       "3     43.976744   7.648293   3.837209  14.392765  168.885456  175.277251   \n",
       "4     24.320988   6.534011   3.567901   8.913580  163.076959   96.019681   \n",
       "5     20.924051   6.134299   3.037975   6.506329  165.707039   82.761541   \n",
       "6     34.150000   6.740695   3.733333  10.214815  164.252922  135.639059   \n",
       "7     23.833333   6.395508   3.141026   8.717949  163.221967   94.106131   \n",
       "8     32.380952   6.152543   2.857143   6.402116  164.380868  128.391104   \n",
       "9     45.228571   6.608449   3.714286   9.180952  159.167580  180.141749   \n",
       "10    24.757143   6.241471   3.071429   6.949206  167.005923   97.692813   \n",
       "11    32.096774   7.206768   3.838710  13.806452  160.469926  127.646528   \n",
       "12    29.833333   6.436864   3.476190   8.296296  162.859109  118.257591   \n",
       "13    36.500000   6.700508   3.653846  10.709402  163.669041  145.244215   \n",
       "14    38.851852   7.402900   3.777778  12.979424  170.018323  154.527750   \n",
       "15    27.534483   6.269883   3.500000   7.973180  161.381053  109.038626   \n",
       "16    23.837500   6.722804   3.125000   7.875000  172.043543   93.939718   \n",
       "17    55.263158   7.389305   3.684211  12.257310  170.139845  220.606751   \n",
       "18    43.057692   7.061350   3.942308  11.871795  163.441127  171.586122   \n",
       "19    42.886792   7.052642   3.943396  12.100629  164.829574  170.922688   \n",
       "20    35.761905   6.858500   2.666667   9.640212  168.739205  141.875852   \n",
       "21    21.528571   6.482779   2.942857   6.400000  171.920444   84.572241   \n",
       "22    23.054795   6.395890   3.164384   8.301370  162.696886   90.885447   \n",
       "23    19.160000   6.067396   2.560000   5.920000  167.413784   75.058797   \n",
       "24    17.131868   5.762305   3.000000   4.109890  165.080662   66.885356   \n",
       "25    28.432432   6.603514   4.324324  11.027027  153.571767  112.900411   \n",
       "26    21.500000   6.207650   2.954545   7.272727  164.257367   85.172313   \n",
       "27    28.428571   6.252383   2.885714   7.384127  164.322747  112.473778   \n",
       "28    38.658537   6.752610   3.682927  10.753388  159.812730  153.837907   \n",
       "29    21.670588   6.292286   2.882353   6.776471  167.982829   85.273347   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "1075  23.849057   6.443296   3.584906   8.490566  165.656310   94.855312   \n",
       "1076  20.528302   6.044432   3.603774   6.075472  162.424590   80.699025   \n",
       "1077  20.440000   5.968498   3.700000   6.080000  161.064334   80.380964   \n",
       "1078  24.333333   5.957881   3.062500   5.509259  164.383290   95.963738   \n",
       "1079  23.375000   6.322417   3.203125   8.375000  162.588954   92.261393   \n",
       "1080  41.800000   9.073735   3.350000  21.400000  180.387654  166.508642   \n",
       "1081  18.390244   5.605241   3.000000   4.731707  158.624476   72.159797   \n",
       "1082  23.458333   6.397181   3.500000   8.375000  166.060607   93.350066   \n",
       "1083  25.809524   5.873886   3.380952   5.962963  162.051123  102.034265   \n",
       "1084  21.414634   5.975237   3.487805   6.926829  161.339153   84.405257   \n",
       "1085  19.000000   5.884906   2.759259   5.370370  162.831246   74.502082   \n",
       "1086  23.318182   6.330339   3.363636   8.045455  163.149857   92.872249   \n",
       "1087  20.560000   6.123796   2.680000   5.920000  165.088853   80.788719   \n",
       "1088  22.380952   6.182614   3.349206   7.396825  164.113227   89.449143   \n",
       "1089  25.161290   5.774461   2.806452   4.143369  166.212698   99.236969   \n",
       "1090  18.074074   5.806478   2.555556   4.518519  164.385174   70.742453   \n",
       "1091  21.284553   6.179102   3.219512   6.975610  164.084221   83.736404   \n",
       "1092  19.658537   5.914988   3.048780   5.756098  166.863082   77.226327   \n",
       "1093  17.720930   5.750228   2.697674   4.604651  164.527087   69.324920   \n",
       "1094  17.225806   5.579190   2.548387   4.193548  161.392477   67.383958   \n",
       "1095  25.841270   6.257190   3.301587   7.499118  165.929853  102.114039   \n",
       "1096  19.416667   6.040173   2.826389   5.763889  164.563288   76.121662   \n",
       "1097  22.877193   5.824621   2.912281   5.130604  162.065226   90.113408   \n",
       "1098  18.912281   5.693595   2.929825   5.228070  158.876430   74.254334   \n",
       "1099  25.464286   6.616246   3.750000   9.285714  163.099820  100.684505   \n",
       "1100  19.133333   5.909660   2.733333   5.466667  165.378392   75.050748   \n",
       "1101  19.211268   5.922814   2.788732   5.802817  163.585967   75.352194   \n",
       "1102  27.312500   6.923266   3.406250  10.750000  170.933703  108.146947   \n",
       "1103  31.785714   6.290479   3.357143   7.658730  161.230794  126.116779   \n",
       "1104  20.761905   6.057493   3.119048   6.476190  164.681027   81.668748   \n",
       "\n",
       "      feature-6  feature-7  feature-8  feature-9 ...   feature-1515  \\\n",
       "0      1.791689   6.818675   8.138413   8.270161 ...       5.658393   \n",
       "1      1.381401   6.002651   5.080499   7.514421 ...       4.830811   \n",
       "2      1.603976   7.410120  10.114794   8.805738 ...       6.397659   \n",
       "3      1.622298   7.629033  12.180817   9.070719 ...       5.879135   \n",
       "4      1.380679   6.566695   4.417010   8.058783 ...       8.148663   \n",
       "5      1.381957   6.187547   4.684599   7.660347 ...       6.087556   \n",
       "6      1.620887   6.781702   8.631090   8.248393 ...       6.198225   \n",
       "7      1.435936   6.443753   5.834402   7.904135 ...       6.582328   \n",
       "8      1.687697   6.232890   4.476844   7.736528 ...       0.000000   \n",
       "9      1.981354   6.690537   8.428546   8.221041 ...       5.214936   \n",
       "10     1.408460   6.289021   5.919754   7.789862 ...       6.175997   \n",
       "11     1.591140   7.228381  11.071685   8.598289 ...       7.242977   \n",
       "12     1.600911   6.497038   6.283812   7.960386 ...       5.658611   \n",
       "13     1.785651   6.764423   7.295706   8.158660 ...       5.768126   \n",
       "14     1.476580   7.381493  11.933931   8.867678 ...       3.218876   \n",
       "15     1.618478   6.345124   4.994148   7.791228 ...       8.934004   \n",
       "16     1.156748   6.710586   6.512587   8.234018 ...       5.638355   \n",
       "17     1.969099   7.412105  10.847813   8.885381 ...       0.000000   \n",
       "18     1.834505   7.104088   9.456211   8.542274 ...       6.706174   \n",
       "19     1.833416   7.095513   8.143800   8.514148 ...       6.976085   \n",
       "20     1.456355   6.864043  11.470610   8.439979 ...       0.000000   \n",
       "21     1.127633   6.481077   5.269841   8.031493 ...       5.036953   \n",
       "22     1.370996   6.435699   6.796613   7.942436 ...       6.182085   \n",
       "23     1.241288   6.107552   6.423333   7.651508 ...       0.000000   \n",
       "24     1.294529   5.828765   2.738324   7.375662 ...       6.448889   \n",
       "25     1.738632   6.683730   6.055743   8.059202 ...       8.293510   \n",
       "26     1.392726   6.258106   5.808081   7.738226 ...       7.443490   \n",
       "27     1.536709   6.312869   6.333953   7.825781 ...       3.791267   \n",
       "28     1.814410   6.815551   8.392444   8.278695 ...       5.800228   \n",
       "29     1.296235   6.326075   4.991830   7.822375 ...       6.577992   \n",
       "...         ...        ...        ...        ... ...            ...   \n",
       "1075   1.429258   6.483247   5.732180   7.917090 ...       8.145414   \n",
       "1076   1.402911   6.109594   3.789570   7.610526 ...       7.481855   \n",
       "1077   1.456437   6.045898   3.411944   7.531384 ...       7.520150   \n",
       "1078   1.496938   6.033144   3.958207   7.545887 ...       7.057353   \n",
       "1079   1.451475   6.377366   5.672743   7.837564 ...       6.824985   \n",
       "1080   1.000430   8.896735  19.768056  10.349211 ...       0.000000   \n",
       "1081   1.549658   5.717783   2.198509   7.184370 ...       7.163209   \n",
       "1082   1.430009   6.439448   5.322917   7.869501 ...       8.177485   \n",
       "1083   1.656037   5.975364   2.317542   7.431153 ...       7.555979   \n",
       "1084   1.534660   6.062522   3.468157   7.496018 ...       7.446209   \n",
       "1085   1.381487   5.956572   3.660108   7.471168 ...       5.514688   \n",
       "1086   1.484800   6.384780   4.174874   7.815815 ...       6.172744   \n",
       "1087   1.335059   6.174052   4.833333   7.684390 ...       0.000000   \n",
       "1088   1.521852   6.248717   4.873016   7.649627 ...       7.231355   \n",
       "1089   1.531852   5.860845   2.388103   7.386058 ...       3.954843   \n",
       "1090   1.348570   5.877785   3.391975   7.397610 ...       0.000000   \n",
       "1091   1.363989   6.230333   4.664747   7.732390 ...       8.626406   \n",
       "1092   1.395619   5.985012   2.447832   7.443992 ...       6.598232   \n",
       "1093   1.357124   5.825972   3.025840   7.340613 ...       5.196423   \n",
       "1094   1.452391   5.679165   2.590502   7.175556 ...       3.840795   \n",
       "1095   1.475191   6.312625   4.628210   7.790568 ...       8.508140   \n",
       "1096   1.298125   6.090735   4.419416   7.632631 ...       6.257668   \n",
       "1097   1.522993   5.913365   2.939753   7.426243 ...       5.575239   \n",
       "1098   1.530849   5.798249   2.397661   7.270916 ...       7.533869   \n",
       "1099   1.421288   6.649411   7.740823   8.113419 ...       7.724212   \n",
       "1100   1.361830   5.976143   4.137963   7.473203 ...       3.446011   \n",
       "1101   1.368851   5.990213   3.049394   7.502897 ...       5.234112   \n",
       "1102   1.321618   6.921481   8.122396   8.348757 ...       6.003578   \n",
       "1103   1.718917   6.371939   5.511035   7.836350 ...       5.931955   \n",
       "1104   1.401802   6.120762   3.509921   7.596294 ...       6.846092   \n",
       "\n",
       "      feature-1516  feature-1517  feature-1518  feature-1519  feature-1520  \\\n",
       "0         4.151040      4.540632      4.953183      5.351562      5.311048   \n",
       "1         3.817712      4.123094      4.426343      4.823804      4.652173   \n",
       "2         4.223177      4.685597      5.116870      5.333926      5.504569   \n",
       "3         4.280132      4.563045      5.007714      5.159773      5.393628   \n",
       "4         4.624973      5.173321      5.720312      6.259342      6.626469   \n",
       "5         4.430817      4.820282      5.183187      5.595176      5.489454   \n",
       "6         4.471639      4.801970      5.237107      5.493833      5.573816   \n",
       "7         4.600158      5.032071      5.499726      5.978728      5.995208   \n",
       "8         3.449988      3.865979      4.506730      4.765906      4.965028   \n",
       "9         3.828641      4.234107      4.682131      4.890349      5.192957   \n",
       "10        4.363099      4.716264      5.155457      5.591686      5.680173   \n",
       "11        4.094345      4.639572      5.151845      5.678037      5.850765   \n",
       "12        4.051785      4.519067      4.935373      5.281616      5.221369   \n",
       "13        3.931826      4.356709      4.844187      5.326662      5.340239   \n",
       "14        3.688879      3.761200      4.110874      4.442651      4.406719   \n",
       "15        4.644391      5.241747      5.930586      6.610360      7.088878   \n",
       "16        4.234107      4.532599      4.804021      5.043425      5.181784   \n",
       "17        3.617652      3.868593      4.357510      4.523146      4.789573   \n",
       "18        4.508108      4.898772      5.374989      5.679959      5.755742   \n",
       "19        4.572130      5.115746      5.566195      5.936711      6.080505   \n",
       "20        3.135494      2.484907      2.397895      2.302585      2.197225   \n",
       "21        3.931826      4.234107      4.442651      4.672829      4.753590   \n",
       "22        4.330733      4.605170      4.912655      5.181784      5.393628   \n",
       "23        2.833213      2.944439      2.944439      3.044522      3.044522   \n",
       "24        4.262680      4.624973      5.010635      5.351858      5.590987   \n",
       "25        4.363099      5.012301      5.675469      6.310259      6.686641   \n",
       "26        4.304065      4.828314      5.422745      6.018593      6.373640   \n",
       "27        3.772761      4.081766      4.514972      4.873765      4.997212   \n",
       "28        4.098503      4.492841      4.938513      5.120237      5.262042   \n",
       "29        4.385147      4.798885      5.216633      5.602695      5.857442   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1075      4.375757      5.032071      5.702949      6.339698      6.834210   \n",
       "1076      4.151040      4.734003      5.287636      5.850225      6.211102   \n",
       "1077      4.166665      4.725173      5.302683      5.881406      6.250458   \n",
       "1078      4.131159      4.601413      5.158696      5.690359      6.054403   \n",
       "1079      4.430817      4.919981      5.421641      5.892335      6.042336   \n",
       "1080      3.178054      2.890372      2.995732      3.135494      2.995732   \n",
       "1081      3.951244      4.559126      5.140200      5.685703      5.885409   \n",
       "1082      4.297285      4.978456      5.658611      6.322790      6.818753   \n",
       "1083      4.073291      4.650383      5.246695      5.735564      6.103816   \n",
       "1084      4.212128      4.888468      5.525951      5.975081      6.299380   \n",
       "1085      3.901973      4.327438      4.732904      5.096431      5.080239   \n",
       "1086      4.060443      4.569543      5.057837      5.539301      5.568821   \n",
       "1087      3.238678      3.669951      4.112921      4.525180      4.251170   \n",
       "1088      4.545951      5.133590      5.739994      6.264469      6.422816   \n",
       "1089      3.409496      3.868593      4.382808      4.539297      4.549261   \n",
       "1090      3.198673      3.630985      4.009603      4.409003      3.997053   \n",
       "1091      4.923624      5.447814      5.972218      6.539676      7.003520   \n",
       "1092      3.886705      4.389809      4.926801      5.261718      5.494090   \n",
       "1093      3.676301      4.123094      4.647990      5.085665      5.169063   \n",
       "1094      3.349904      3.682610      4.071161      4.447785      4.003918   \n",
       "1095      4.551242      5.176856      5.812076      6.371825      6.850507   \n",
       "1096      4.762174      5.105945      5.313206      5.537334      5.620401   \n",
       "1097      4.056123      4.501198      4.960657      5.184939      5.169418   \n",
       "1098      4.255613      4.826312      5.403240      5.979993      6.177425   \n",
       "1099      4.424847      5.028803      5.577369      6.031136      6.363244   \n",
       "1100      3.481240      3.907010      4.281861      4.720172      4.574711   \n",
       "1101      4.248495      4.791650      5.245707      5.667723      5.793776   \n",
       "1102      3.921973      4.409763      4.898772      5.348595      5.559479   \n",
       "1103      3.789855      4.367864      4.954506      5.430442      5.715330   \n",
       "1104      3.974998      4.517704      5.125079      5.599347      5.933364   \n",
       "\n",
       "      feature-1521  feature-1522  feature-1523    y  \n",
       "0         5.560922      5.643015      5.715999  0.0  \n",
       "1         4.795274      4.860781      5.001426  0.0  \n",
       "2         5.797956      6.009581      6.200889  0.0  \n",
       "3         5.640132      5.472271      5.741399  0.0  \n",
       "4         7.062406      7.472998      7.829842  0.0  \n",
       "5         5.604998      5.847522      5.987080  0.0  \n",
       "6         5.764799      5.865760      5.998937  0.0  \n",
       "7         6.179952      6.364051      6.481290  0.0  \n",
       "8         3.840795      3.595598      0.000000  0.0  \n",
       "9         5.342334      5.402677      5.303305  0.0  \n",
       "10        5.977302      6.030986      6.214671  0.0  \n",
       "11        6.134888      6.451753      6.793466  0.0  \n",
       "12        5.465948      5.520210      5.499982  0.0  \n",
       "13        5.395331      5.454787      5.557552  0.0  \n",
       "14        4.543295      4.605170      4.290459  0.0  \n",
       "15        7.641069      8.134765      8.607916  0.0  \n",
       "16        5.337538      5.497168      5.568345  0.0  \n",
       "17        4.523146      3.944006      0.000000  0.0  \n",
       "18        6.060582      6.240763      6.434747  0.0  \n",
       "19        6.198796      6.333335      6.652742  0.0  \n",
       "20        2.772589      0.000000      0.000000  0.0  \n",
       "21        4.762174      4.890349      4.969813  0.0  \n",
       "22        5.652489      5.831882      6.040255  0.0  \n",
       "23        2.708050      0.000000      0.000000  0.0  \n",
       "24        5.834811      6.077642      6.293419  0.0  \n",
       "25        7.157565      7.635515      8.018008  0.0  \n",
       "26        6.751321      7.048332      7.302612  0.0  \n",
       "27        4.848606      4.455074      4.352694  0.0  \n",
       "28        5.561162      5.568821      5.681878  0.0  \n",
       "29        6.151851      6.484856      6.339973  0.0  \n",
       "...            ...           ...           ...  ...  \n",
       "1075      7.294229      7.631557      8.007074  1.0  \n",
       "1076      6.591717      6.991033      7.274674  1.0  \n",
       "1077      6.631384      7.030719      7.325005  1.0  \n",
       "1078      6.455334      6.873095      6.939417  1.0  \n",
       "1079      6.333613      6.572807      6.736893  1.0  \n",
       "1080      3.218876      2.302585      0.000000  1.0  \n",
       "1081      6.272759      6.609097      6.919128  1.0  \n",
       "1082      7.288522      7.655010      8.058613  1.0  \n",
       "1083      6.545754      6.979000      7.346393  1.0  \n",
       "1084      6.681394      7.067183      7.171165  1.0  \n",
       "1085      5.365684      5.482980      5.400140  1.0  \n",
       "1086      5.728170      5.901779      5.917717  1.0  \n",
       "1087      4.104707      3.446011      3.056357  1.0  \n",
       "1088      6.771282      7.089077      7.031227  1.0  \n",
       "1089      4.743845      4.850075      4.323304  1.0  \n",
       "1090      4.066888      3.725693      2.784239  1.0  \n",
       "1091      7.460310      7.890512      8.276919  1.0  \n",
       "1092      5.865848      6.204873      6.369954  1.0  \n",
       "1093      5.358942      5.594479      5.380329  1.0  \n",
       "1094      4.150055      4.274928      4.123094  1.0  \n",
       "1095      7.327036      7.760788      8.122028  1.0  \n",
       "1096      5.805135      5.976351      6.091310  1.0  \n",
       "1097      5.403803      5.604422      5.480118  1.0  \n",
       "1098      6.654314      7.047680      7.348266  1.0  \n",
       "1099      6.711588      7.060610      7.424184  1.0  \n",
       "1100      4.320816      4.406719      4.342993  1.0  \n",
       "1101      5.960844      5.839187      5.411088  1.0  \n",
       "1102      5.495630      5.703366      5.926259  1.0  \n",
       "1103      6.065365      6.387320      5.916728  1.0  \n",
       "1104      6.335871      6.708575      6.648544  1.0  \n",
       "\n",
       "[1105 rows x 1525 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _data_ = pd.read_csv('./data/train.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:07:51.132614Z",
     "start_time": "2018-05-01T21:07:50.887210Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c85678780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.y.unique())\n",
    "plt.hist(data.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:07:52.750759Z",
     "start_time": "2018-05-01T21:07:52.669939Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "790 315 0.7149321266968326\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum().sum())\n",
    "\n",
    "num_1 = data[data.y == 1].shape[0] \n",
    "num_0 = data[data.y == 0].shape[0]\n",
    "\n",
    "print(num_1, num_0, num_1 / (num_0 + num_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:08:04.885754Z",
     "start_time": "2018-05-01T21:07:53.514470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1279 correlated features\n",
      "Dropped 20 correlated features\n"
     ]
    }
   ],
   "source": [
    "data = pp_pipeline(_data_)\n",
    "\n",
    "data_X = data.drop('y', axis=1)\n",
    "data_y = data[['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:08:05.657635Z",
     "start_time": "2018-05-01T21:08:05.637193Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "(train_X, train_y, test_X, test_y) = split_data2(data, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:42:59.222715Z",
     "start_time": "2018-05-01T22:42:57.936834Z"
    }
   },
   "outputs": [],
   "source": [
    "class DFS(BaseEstimator):\n",
    "    def __init__(self, layers_sizes=[128, 64, 2], batch_size=32, lambda1=1e-3, lambda2=1.,\n",
    "                 alpha1=1e-3, alpha2=0., num_epochs=10, verbose=0, N=None, \n",
    "                 dropout_rate=1.):\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.num_layers = len(layers_sizes)\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.num_epochs = num_epochs\n",
    "        self.verbose = verbose\n",
    "        self.N = N\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, test_data=None):\n",
    "        self._build_graph_(X.shape[1])\n",
    "        self.features = X.columns #Persisting for `select_most_important_ftrs`\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(self.num_epochs):\n",
    "            X_cur = X.sample(frac=1, random_state=epoch_i)\n",
    "            y_cur = y.sample(frac=1, random_state=epoch_i)\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in batch_data(X_cur, y_cur, \n",
    "                                               batch_size=self.batch_size):\n",
    "                train_loss, _ = self.sess.run([self.total_loss, self.train_step],\n",
    "                                               feed_dict = {self.x: batch_X,\n",
    "                                                            self.y: batch_y,\n",
    "                                                            self.dropout_rate_ph: self.dropout_rate})\n",
    "                epoch_loss += train_loss\n",
    "            epoch_loss /= X.shape[0] // self.batch_size\n",
    "            \n",
    "            train_predict = self.predict(X_cur)\n",
    "            train_accuracy = mtcs.accuracy_score(y_cur, train_predict)\n",
    "            \n",
    "            if self.verbose:\n",
    "                if test_data is not None:\n",
    "                    test_X, test_y = test_data\n",
    "                    test_predict = self.predict(test_X)\n",
    "                    test_accuracy = mtcs.accuracy_score(test_y, test_predict)\n",
    "                    print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}.\"\n",
    "                          f\"Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "                else:\n",
    "                    print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}. \"\n",
    "                          f\"Train accuracy: {train_accuracy}.\")\n",
    "                \n",
    "        return self\n",
    "       \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions_proba = self.sess.run(self.predictions, \n",
    "                                          feed_dict={self.x: X,\n",
    "                                                     self.dropout_rate_ph: 1.})\n",
    "        \n",
    "        return predictions_proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions_proba = self.predict_proba(X)\n",
    "        \n",
    "        return list(map(np.argmax, predictions_proba))\n",
    "    \n",
    "    \n",
    "    def get_features_weights(self):\n",
    "        weights = self.sess.run(self.features_weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs(self, N):\n",
    "        weights = self.get_features_weights()\n",
    "        feature_weight = sorted(zip(weights, self.features), \n",
    "                                key=lambda x: abs(x[0]))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight[-N:])\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs_thresh(self, thresh=0.1):\n",
    "        weights = self.get_features_weights()\n",
    "        feature_weight = filter(lambda x: x[0] >= thresh,\n",
    "                                zip(weights, self.features))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight)\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None, threshold=None):\n",
    "        if N:\n",
    "            features = list(self.select_most_important_ftrs(N))\n",
    "        elif self.N:\n",
    "            features = list(self.select_most_important_ftrs(self.N))\n",
    "        else:\n",
    "            features = list(self.select_most_important_ftrs_thresh(threshold))\n",
    "        \n",
    "        return X[features]\n",
    "    \n",
    "     \n",
    "    def _build_graph_(self, num_features):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        ###Placeholders \n",
    "        x = tf.placeholder(tf.float32, [None, num_features], 'x_ph')\n",
    "        y = tf.placeholder(tf.int32, [None], 'y_ph')\n",
    "        dropout_rate_ph = tf.placeholder(tf.float32, None, 'dropout_rate')\n",
    "        \n",
    "        ###Weights initialization\n",
    "        w = tf.get_variable(\"dfs_features_weight\", \n",
    "                            initializer = tf.constant(1., shape=[num_features]))\n",
    "        self.layers_sizes = [num_features] + self.layers_sizes\n",
    "        W, b = [], []\n",
    "        for layer_i in range(self.num_layers):\n",
    "            W.append(weight_init(f\"layer_{layer_i}_weights\",\n",
    "                                 shape=[self.layers_sizes[layer_i],\n",
    "                                        self.layers_sizes[layer_i+1]]))\n",
    "            b.append(bias_init(f\"layer_{layer_i}_bias\",\n",
    "                               shape=[self.layers_sizes[layer_i+1]]))\n",
    "        \n",
    "        ###Input transformations\n",
    "        logits = x * w #feature selection\n",
    "        for layer_i in range(self.num_layers):\n",
    "            if layer_i != self.num_layers - 1:\n",
    "                logits = tf.nn.dropout(\n",
    "                            tf.nn.relu(tf.matmul(logits, W[layer_i]) + b[layer_i]),\n",
    "                            keep_prob=dropout_rate_ph)\n",
    "            else:\n",
    "                logits = tf.matmul(logits, W[layer_i]) + b[layer_i]\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "            \n",
    "        ###Loss calculation\n",
    "        logloss = tf.reduce_sum(\n",
    "                        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                                       logits=logits))\n",
    "        w_loss = elastic_net(w, self.lambda1, self.lambda2)\n",
    "        W_loss = tf.reduce_sum([elastic_net(W_i, self.alpha1, self.alpha2) for W_i in W])\n",
    "        \n",
    "        total_loss = tf.reduce_sum(logloss + w_loss + W_loss)\n",
    "        \n",
    "        ###Optimizer\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.dropout_rate_ph = dropout_rate_ph\n",
    "        self.predictions = predictions\n",
    "        self.total_loss = total_loss\n",
    "        self.train_step = train_step\n",
    "        self.features_weights = w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:25:03.099536Z",
     "start_time": "2018-05-01T23:00:24.338036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 25 candidates, totalling 75 fits\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8508474576271187, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.2s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8542372881355932, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   38.2s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8673469387755102, total=  19.6s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   57.8s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8338983050847457, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8809523809523809, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8372881355932204, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  20.9s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8741496598639455, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8372881355932204, total=  18.7s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8372881355932204, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8775510204081632, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8271186440677966, total=  18.7s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8372881355932204, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8741496598639455, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  4.8min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  18.5s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  5.1min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8711864406779661, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  5.4min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8843537414965986, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  5.7min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  6.1min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8203389830508474, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  6.4min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8435374149659864, total=  20.9s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  6.7min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  7.1min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  7.4min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7448979591836735, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  7.7min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  8.0min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8338983050847457, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  8.3min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  8.6min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  18.8s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  9.0min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  9.3min remaining:    0.0s\n",
      "[CV] alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.21, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  9.6min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.823728813559322, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  9.9min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8576271186440678, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 10.2min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8673469387755102, total=  19.9s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed: 10.6min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8271186440677966, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed: 10.9min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  21.3s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed: 11.2min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.7s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 11.6min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed: 11.9min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed: 12.2min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed: 12.5min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 12.8min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed: 13.2min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed: 13.5min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed: 13.8min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed: 14.1min remaining:    0.0s\n",
      "[CV] alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.41, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 14.5min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.823728813559322, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed: 14.8min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8508474576271187, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed: 15.1min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8639455782312925, total=  21.3s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 15.5min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.8s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed: 15.8min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 16.1min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed: 16.4min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed: 16.8min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed: 17.1min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed: 17.4min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed: 17.7min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed: 18.0min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed: 18.4min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed: 18.7min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed: 19.0min remaining:    0.0s\n",
      "[CV] alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.61, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 19.3min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  21.1s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed: 19.7min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8067796610169492, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed: 20.0min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.01, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7414965986394558, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed: 20.3min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed: 20.6min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed: 21.0min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.21, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed: 21.3min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed: 21.6min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed: 21.9min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.41, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed: 22.2min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed: 22.6min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed: 22.9min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.61, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 23.2min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7254237288135593, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed: 23.5min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7186440677966102, total=  18.8s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed: 23.8min remaining:    0.0s\n",
      "[CV] alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.81, dropout_rate=0.25, lambda1=0.81, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.7006802721088435, total=  21.1s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 24.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 24.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DFS(N=None, alpha1=0.001, alpha2=0.0, batch_size=32, dropout_rate=1.0,\n",
       "  lambda1=0.001, lambda2=1.0, layers_sizes=[128, 64, 2], num_epochs=10,\n",
       "  verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'layers_sizes': [[64, 64, 32, 2]], 'lambda1': array([ 0.01,  0.21,  0.41,  0.61,  0.81]), 'alpha1': array([ 0.01,  0.21,  0.41,  0.61,  0.81]), 'num_epochs': [200], 'dropout_rate': [0.25]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=100)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'layers_sizes': [[64, 64, 32, 2]],\n",
    "        'lambda1': np.arange(1e-2, 1, 2e-1),\n",
    "        'alpha1': np.arange(1e-2, 1, 2e-1),\n",
    "        'num_epochs': [200],\n",
    "        'dropout_rate': [0.25],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = ms.GridSearchCV(DFS(), param_grid, scoring='accuracy', verbose=100)\n",
    "\n",
    "grid.fit(train_X, train_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:31:55.160111Z",
     "start_time": "2018-05-01T23:26:40.407956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "[CV] dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.3s remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8677966101694915, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   38.6s remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.001, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8639455782312925, total=  18.8s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   57.4s remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8440677966101695, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8440677966101695, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.021, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8639455782312925, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8508474576271187, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8677966101694915, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.041, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8639455782312925, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.864406779661017, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8677966101694915, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.061, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8605442176870748, total=  20.7s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8677966101694915, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8745762711864407, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  dropout_rate=0.25, lambda1=0.081, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8673469387755102, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  4.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  4.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DFS(N=None, alpha1=0.001, alpha2=0.0, batch_size=32, dropout_rate=1.0,\n",
       "  lambda1=0.001, lambda2=1.0, layers_sizes=[128, 64, 2], num_epochs=10,\n",
       "  verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'layers_sizes': [[64, 64, 32, 2]], 'lambda1': array([ 0.001,  0.021,  0.041,  0.061,  0.081]), 'num_epochs': [200], 'dropout_rate': [0.25]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=100)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'layers_sizes': [[64, 64, 32, 2]],\n",
    "        'lambda1': np.arange(1e-3, 1e-1, 2e-2),\n",
    "#         'alpha1': np.arange(1e-2, 1, 2e-1),\n",
    "        'num_epochs': [200],\n",
    "        'dropout_rate': [0.25],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = ms.GridSearchCV(DFS(), param_grid, scoring='accuracy', verbose=100)\n",
    "\n",
    "grid.fit(train_X, train_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:44:39.224143Z",
     "start_time": "2018-05-01T23:44:39.201182Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-204-c99cb7d35c3f>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-204-c99cb7d35c3f>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    'num_epochs': [200],\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'layers_sizes': [[64, 64, 32, 2]],\n",
    "        'lambda1': np.arange(1e-3, 1e-1, 2e-2),\n",
    "        'alpha1': np.arange(1e-2, 1, 5e-1),\n",
    "        'num_epochs': [200],\n",
    "        'dropout_rate': [0.25],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid2 = ms.GridSearchCV(DFS(), param_grid, scoring='accuracy', verbose=100)\n",
    "\n",
    "grid2.fit(train_X, train_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:55:23.130709Z",
     "start_time": "2018-05-01T23:45:14.127891Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8576271186440678, total=  19.1s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.2s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8610169491525423, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   38.1s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.03, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8707482993197279, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   57.4s remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8542372881355932, total=  18.8s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8610169491525423, total=  21.3s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.035, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8741496598639455, total=  19.8s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  19.6s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8677966101694915, total=  19.6s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.04, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8639455782312925, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.847457627118644, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  3.3min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8711864406779661, total=  19.4s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.045, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8809523809523809, total=  19.6s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8576271186440678, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  4.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8745762711864407, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  4.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.05, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8707482993197279, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  4.9min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8542372881355932, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  5.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.864406779661017, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  5.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.055, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8809523809523809, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  5.8min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8610169491525423, total=  21.4s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  6.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8745762711864407, total=  19.5s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  6.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.06, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8741496598639455, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:  6.8min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8610169491525423, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  7.2min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.864406779661017, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  7.5min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.065, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8707482993197279, total=  19.2s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  7.8min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8610169491525423, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  8.1min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8745762711864407, total=  18.7s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  8.4min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.07, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8877551020408163, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  8.7min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8576271186440678, total=  18.9s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  9.1min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8711864406779661, total=  19.3s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  9.4min remaining:    0.0s\n",
      "[CV] alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200 \n",
      "[CV]  alpha1=0.01, dropout_rate=0.25, lambda1=0.075, layers_sizes=[64, 64, 32, 2], num_epochs=200, score=0.8945578231292517, total=  19.0s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  9.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  9.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=DFS(N=None, alpha1=0.001, alpha2=0.0, batch_size=32, dropout_rate=1.0,\n",
       "  lambda1=0.001, lambda2=1.0, layers_sizes=[128, 64, 2], num_epochs=10,\n",
       "  verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'layers_sizes': [[64, 64, 32, 2]], 'lambda1': array([ 0.03 ,  0.035,  0.04 ,  0.045,  0.05 ,  0.055,  0.06 ,  0.065,\n",
       "        0.07 ,  0.075]), 'alpha1': [0.01], 'num_epochs': [200], 'dropout_rate': [0.25]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=100)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = [\n",
    "    {\n",
    "        'layers_sizes': [[64, 64, 32, 2]],\n",
    "        'lambda1': np.arange(0.030, 0.080, 0.005),\n",
    "        'alpha1': [0.01],\n",
    "        'num_epochs': [200],\n",
    "        'dropout_rate': [0.25],\n",
    "    }\n",
    "]\n",
    "\n",
    "grid3 = ms.GridSearchCV(DFS(), param_grid, scoring='accuracy', verbose=100)\n",
    "\n",
    "grid3.fit(train_X, train_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:45:35.924079Z",
     "start_time": "2018-05-01T22:42:59.274006Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 465.47497784649886.Train accuracy: 0.7149321266968326. Test accuracy: 0.7149321266968326.\n",
      "==> Epoch: 1. Train loss: 452.4825032552083.Train accuracy: 0.7149321266968326. Test accuracy: 0.7149321266968326.\n",
      "==> Epoch: 2. Train loss: 439.6175548412182.Train accuracy: 0.7149321266968326. Test accuracy: 0.7149321266968326.\n",
      "==> Epoch: 3. Train loss: 427.1764142071759.Train accuracy: 0.7149321266968326. Test accuracy: 0.7149321266968326.\n",
      "==> Epoch: 4. Train loss: 414.29669980649595.Train accuracy: 0.7149321266968326. Test accuracy: 0.7149321266968326.\n",
      "==> Epoch: 5. Train loss: 401.23277000144674.Train accuracy: 0.7771493212669683. Test accuracy: 0.8009049773755657.\n",
      "==> Epoch: 6. Train loss: 388.2038415979456.Train accuracy: 0.8099547511312217. Test accuracy: 0.8371040723981901.\n",
      "==> Epoch: 7. Train loss: 374.73662425853587.Train accuracy: 0.8359728506787331. Test accuracy: 0.8552036199095022.\n",
      "==> Epoch: 8. Train loss: 362.15824494538487.Train accuracy: 0.8416289592760181. Test accuracy: 0.8597285067873304.\n",
      "==> Epoch: 9. Train loss: 349.15908700448495.Train accuracy: 0.8518099547511312. Test accuracy: 0.8733031674208145.\n",
      "==> Epoch: 10. Train loss: 336.52459829824943.Train accuracy: 0.832579185520362. Test accuracy: 0.832579185520362.\n",
      "==> Epoch: 11. Train loss: 324.79538189923323.Train accuracy: 0.8585972850678733. Test accuracy: 0.8642533936651584.\n",
      "==> Epoch: 12. Train loss: 312.4567317256221.Train accuracy: 0.8631221719457014. Test accuracy: 0.8552036199095022.\n",
      "==> Epoch: 13. Train loss: 299.5899488661024.Train accuracy: 0.8699095022624435. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 14. Train loss: 286.3451006854022.Train accuracy: 0.8755656108597285. Test accuracy: 0.8823529411764706.\n",
      "==> Epoch: 15. Train loss: 275.3025331850405.Train accuracy: 0.8733031674208145. Test accuracy: 0.8642533936651584.\n",
      "==> Epoch: 16. Train loss: 262.63042930320455.Train accuracy: 0.8789592760180995. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 17. Train loss: 250.92152969925493.Train accuracy: 0.8789592760180995. Test accuracy: 0.8778280542986425.\n",
      "==> Epoch: 18. Train loss: 238.65033298068576.Train accuracy: 0.8902714932126696. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 19. Train loss: 226.06313465259694.Train accuracy: 0.8699095022624435. Test accuracy: 0.8687782805429864.\n",
      "==> Epoch: 20. Train loss: 214.53387677228008.Train accuracy: 0.8947963800904978. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 21. Train loss: 201.33540118182148.Train accuracy: 0.8914027149321267. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 22. Train loss: 189.85233561197916.Train accuracy: 0.8778280542986425. Test accuracy: 0.8778280542986425.\n",
      "==> Epoch: 23. Train loss: 178.95981739185476.Train accuracy: 0.8766968325791855. Test accuracy: 0.8597285067873304.\n",
      "==> Epoch: 24. Train loss: 166.91193248607493.Train accuracy: 0.8812217194570136. Test accuracy: 0.8778280542986425.\n",
      "==> Epoch: 25. Train loss: 155.21443119755497.Train accuracy: 0.8947963800904978. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 26. Train loss: 143.24737040201822.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 27. Train loss: 131.88036939832898.Train accuracy: 0.9004524886877828. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 28. Train loss: 119.97415104618779.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 29. Train loss: 108.23652338098597.Train accuracy: 0.8970588235294118. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 30. Train loss: 97.10528705738209.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 31. Train loss: 85.62572168420863.Train accuracy: 0.8902714932126696. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 32. Train loss: 74.63191816541884.Train accuracy: 0.8823529411764706. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 33. Train loss: 63.83411478113245.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 34. Train loss: 53.23320982191298.Train accuracy: 0.8608597285067874. Test accuracy: 0.8733031674208145.\n",
      "==> Epoch: 35. Train loss: 43.214947594536675.Train accuracy: 0.8563348416289592. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 36. Train loss: 34.222628628766095.Train accuracy: 0.8755656108597285. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 37. Train loss: 25.536371654934353.Train accuracy: 0.8631221719457014. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 38. Train loss: 20.711179062172217.Train accuracy: 0.8687782805429864. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 39. Train loss: 19.732522434658474.Train accuracy: 0.8631221719457014. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 40. Train loss: 17.96185956177888.Train accuracy: 0.8540723981900452. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 41. Train loss: 17.492271811873824.Train accuracy: 0.8631221719457014. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 42. Train loss: 16.41675027211507.Train accuracy: 0.8721719457013575. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 43. Train loss: 16.54265845263446.Train accuracy: 0.8506787330316742. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 44. Train loss: 16.112977663675945.Train accuracy: 0.8631221719457014. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 45. Train loss: 15.631280828405309.Train accuracy: 0.8699095022624435. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 46. Train loss: 15.43088114703143.Train accuracy: 0.8631221719457014. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 47. Train loss: 15.120104471842447.Train accuracy: 0.8721719457013575. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 48. Train loss: 15.320203498557762.Train accuracy: 0.8699095022624435. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 49. Train loss: 14.941332357901114.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 50. Train loss: 14.998923160411694.Train accuracy: 0.8744343891402715. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 51. Train loss: 15.042040012500904.Train accuracy: 0.8733031674208145. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 52. Train loss: 14.895447589732983.Train accuracy: 0.8642533936651584. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 53. Train loss: 14.698154696711788.Train accuracy: 0.8721719457013575. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 54. Train loss: 14.076790633024993.Train accuracy: 0.8710407239819005. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 55. Train loss: 14.489862689265498.Train accuracy: 0.8721719457013575. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 56. Train loss: 14.111901459870515.Train accuracy: 0.8755656108597285. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 57. Train loss: 14.173082422327113.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 58. Train loss: 14.567563939977575.Train accuracy: 0.8744343891402715. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 59. Train loss: 14.016109625498453.Train accuracy: 0.8721719457013575. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 60. Train loss: 13.46735088913529.Train accuracy: 0.8721719457013575. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 61. Train loss: 14.226613186023853.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 62. Train loss: 14.153211311057762.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 63. Train loss: 13.276497699596264.Train accuracy: 0.8653846153846154. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 64. Train loss: 14.214345084296333.Train accuracy: 0.8744343891402715. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 65. Train loss: 14.004704652009186.Train accuracy: 0.8755656108597285. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 66. Train loss: 14.137482643127441.Train accuracy: 0.8755656108597285. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 67. Train loss: 13.889567516468189.Train accuracy: 0.8766968325791855. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 68. Train loss: 13.860593583848742.Train accuracy: 0.8721719457013575. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 69. Train loss: 14.347848115143952.Train accuracy: 0.8653846153846154. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 70. Train loss: 13.260028521219889.Train accuracy: 0.8721719457013575. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 71. Train loss: 14.020042807967574.Train accuracy: 0.8653846153846154. Test accuracy: 0.8959276018099548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 72. Train loss: 13.637063485604745.Train accuracy: 0.8766968325791855. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 73. Train loss: 13.468262743066859.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 74. Train loss: 13.903235965304905.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 75. Train loss: 13.706279578032317.Train accuracy: 0.8766968325791855. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 76. Train loss: 13.296345587129947.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 77. Train loss: 13.936011543980351.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 78. Train loss: 13.58905212967484.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 79. Train loss: 13.834293912958216.Train accuracy: 0.8721719457013575. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 80. Train loss: 13.196787763524938.Train accuracy: 0.8710407239819005. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 81. Train loss: 13.131214318452058.Train accuracy: 0.8721719457013575. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 82. Train loss: 13.42759194197478.Train accuracy: 0.8687782805429864. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 83. Train loss: 13.611988950658727.Train accuracy: 0.8755656108597285. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 84. Train loss: 13.844742033216688.Train accuracy: 0.8789592760180995. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 85. Train loss: 13.095912156281647.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 86. Train loss: 13.52851853547273.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 87. Train loss: 13.233491703316018.Train accuracy: 0.8766968325791855. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 88. Train loss: 13.403746834507695.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 89. Train loss: 13.648049001340512.Train accuracy: 0.8676470588235294. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 90. Train loss: 13.432651555096662.Train accuracy: 0.8778280542986425. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 91. Train loss: 12.9775983315927.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 92. Train loss: 13.436950789557564.Train accuracy: 0.8766968325791855. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 93. Train loss: 13.045061429341635.Train accuracy: 0.8766968325791855. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 94. Train loss: 13.52576556029143.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 95. Train loss: 13.10701377303512.Train accuracy: 0.8778280542986425. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 96. Train loss: 13.359188909883853.Train accuracy: 0.8755656108597285. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 97. Train loss: 13.794510717745181.Train accuracy: 0.8766968325791855. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 98. Train loss: 13.127943162564877.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 99. Train loss: 13.137750554967809.Train accuracy: 0.8800904977375565. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 100. Train loss: 13.269186514395255.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 101. Train loss: 13.28361373477512.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 102. Train loss: 13.17479172459355.Train accuracy: 0.8642533936651584. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 103. Train loss: 13.363724161077428.Train accuracy: 0.8778280542986425. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 104. Train loss: 12.935367089730722.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 105. Train loss: 13.377219694632071.Train accuracy: 0.8755656108597285. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 106. Train loss: 13.194413255762171.Train accuracy: 0.8800904977375565. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 107. Train loss: 13.142648237722891.Train accuracy: 0.8778280542986425. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 108. Train loss: 13.312621999669958.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 109. Train loss: 12.886450732195819.Train accuracy: 0.8676470588235294. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 110. Train loss: 13.524777112183747.Train accuracy: 0.8846153846153846. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 111. Train loss: 12.558571285671658.Train accuracy: 0.8812217194570136. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 112. Train loss: 13.041550318400065.Train accuracy: 0.8744343891402715. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 113. Train loss: 12.933280891842312.Train accuracy: 0.8755656108597285. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 114. Train loss: 13.192697525024414.Train accuracy: 0.8733031674208145. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 115. Train loss: 12.792451364022714.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 116. Train loss: 12.747270089608651.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 117. Train loss: 13.210944458290383.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 118. Train loss: 13.068566622557464.Train accuracy: 0.8823529411764706. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 119. Train loss: 12.978033666257504.Train accuracy: 0.8823529411764706. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 120. Train loss: 13.073521808341697.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 121. Train loss: 12.664237004739267.Train accuracy: 0.8800904977375565. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 122. Train loss: 12.839128494262695.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 123. Train loss: 13.081820329030355.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 124. Train loss: 12.674362924363878.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 125. Train loss: 12.5471398918717.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 126. Train loss: 13.071402231852213.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 127. Train loss: 12.82279888788859.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 128. Train loss: 12.893504548955846.Train accuracy: 0.8721719457013575. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 129. Train loss: 13.07904526039406.Train accuracy: 0.8834841628959276. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 130. Train loss: 12.846945427082202.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 131. Train loss: 12.44488596033167.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 132. Train loss: 12.5919782144052.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 133. Train loss: 12.731463891488534.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 134. Train loss: 12.598492516411675.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 135. Train loss: 12.755929717311153.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 136. Train loss: 12.819809860653347.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 137. Train loss: 12.82967253084536.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 138. Train loss: 12.316685959144875.Train accuracy: 0.8800904977375565. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 139. Train loss: 12.796680874294704.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 140. Train loss: 12.894004044709382.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 141. Train loss: 12.87719076651114.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 142. Train loss: 12.633299562666151.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 143. Train loss: 12.311846256256104.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 144. Train loss: 12.767170040695756.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 145. Train loss: 12.303435184337475.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 146. Train loss: 12.581906901465523.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 147. Train loss: 12.621534117945918.Train accuracy: 0.8800904977375565. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 148. Train loss: 12.326409074995253.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 149. Train loss: 12.751135843771475.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 150. Train loss: 12.338648990348533.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 151. Train loss: 12.252275202009413.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 152. Train loss: 12.778498472990814.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 153. Train loss: 12.589601622687447.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 154. Train loss: 12.710034776616979.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 155. Train loss: 12.298449039459229.Train accuracy: 0.8846153846153846. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 156. Train loss: 12.501305156283909.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 157. Train loss: 12.45495695537991.Train accuracy: 0.8789592760180995. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 158. Train loss: 12.508791287740072.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 159. Train loss: 12.384532380987096.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 160. Train loss: 12.149164976897064.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 161. Train loss: 13.010711051799634.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 162. Train loss: 12.234142020896629.Train accuracy: 0.8800904977375565. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 163. Train loss: 12.506510699236834.Train accuracy: 0.8823529411764706. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 164. Train loss: 12.54505964561745.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 165. Train loss: 12.83048461984705.Train accuracy: 0.8812217194570136. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 166. Train loss: 12.399834632873535.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 167. Train loss: 12.517533125700774.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 168. Train loss: 12.467590579280147.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 169. Train loss: 12.348959251686379.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 170. Train loss: 12.504461994877568.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 171. Train loss: 12.1827073097229.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 172. Train loss: 12.657536930508083.Train accuracy: 0.8789592760180995. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 173. Train loss: 12.258748513680917.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 174. Train loss: 12.19606191140634.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 175. Train loss: 12.637752656583432.Train accuracy: 0.8789592760180995. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 176. Train loss: 12.422891351911757.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 177. Train loss: 12.399665708895084.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 178. Train loss: 12.706344586831552.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 179. Train loss: 12.1832381884257.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 180. Train loss: 12.51788819277728.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 181. Train loss: 12.37473671524613.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 182. Train loss: 12.34486629344799.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 183. Train loss: 12.749827367288095.Train accuracy: 0.8766968325791855. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 184. Train loss: 12.785697795726636.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 185. Train loss: 12.060026928230569.Train accuracy: 0.8778280542986425. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 186. Train loss: 12.66433251345599.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 187. Train loss: 12.230995743362993.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 188. Train loss: 12.219291280817103.Train accuracy: 0.8733031674208145. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 189. Train loss: 12.594710650267425.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 190. Train loss: 12.765290684170193.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 191. Train loss: 12.232494389569318.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 192. Train loss: 12.500618599079273.Train accuracy: 0.8789592760180995. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 193. Train loss: 12.216868065021655.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 194. Train loss: 12.017589339503536.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 195. Train loss: 12.442939369766801.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 196. Train loss: 12.52708657582601.Train accuracy: 0.8778280542986425. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 197. Train loss: 12.341119554307726.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 198. Train loss: 12.106006781260172.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 199. Train loss: 11.935339998315882.Train accuracy: 0.8834841628959276. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 200. Train loss: 12.487266116672092.Train accuracy: 0.8846153846153846. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 201. Train loss: 12.088834197432906.Train accuracy: 0.8834841628959276. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 202. Train loss: 12.667738402331317.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 203. Train loss: 12.160183729948821.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 204. Train loss: 12.200104713439941.Train accuracy: 0.8642533936651584. Test accuracy: 0.8552036199095022.\n",
      "==> Epoch: 205. Train loss: 12.142698853104203.Train accuracy: 0.8812217194570136. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 206. Train loss: 12.121056521380389.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 207. Train loss: 12.753271597403067.Train accuracy: 0.8823529411764706. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 208. Train loss: 11.892186588711208.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 209. Train loss: 12.169293174037227.Train accuracy: 0.8699095022624435. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 210. Train loss: 12.293586978205928.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 211. Train loss: 12.331638971964518.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 212. Train loss: 11.754196714471888.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 213. Train loss: 12.652651239324499.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 214. Train loss: 12.496810188999882.Train accuracy: 0.8778280542986425. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 215. Train loss: 12.582153249669958.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 216. Train loss: 12.776860660976833.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 217. Train loss: 12.197065618303087.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 218. Train loss: 11.9540898181774.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 219. Train loss: 11.727775432445386.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 220. Train loss: 12.252872979199445.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 221. Train loss: 12.390455298953587.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 222. Train loss: 12.26149375350387.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 223. Train loss: 11.882777320014107.Train accuracy: 0.8766968325791855. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 224. Train loss: 12.453462971581352.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 225. Train loss: 12.35474282723886.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 226. Train loss: 12.542609638637966.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 227. Train loss: 12.621600733862984.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 228. Train loss: 12.641959843812165.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 229. Train loss: 11.841911775094491.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 230. Train loss: 12.480053301210758.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 231. Train loss: 12.066032904165763.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 232. Train loss: 11.984014970284921.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 233. Train loss: 12.055881394280327.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 234. Train loss: 12.215824992568404.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 235. Train loss: 12.572821758411548.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 236. Train loss: 12.139939149220785.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 237. Train loss: 12.241045139454029.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 238. Train loss: 12.569768499445033.Train accuracy: 0.8766968325791855. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 239. Train loss: 12.20851594430429.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 240. Train loss: 12.281095204529938.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 241. Train loss: 12.160568272625959.Train accuracy: 0.8846153846153846. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 242. Train loss: 12.271606869167751.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 243. Train loss: 12.03619310590956.Train accuracy: 0.8755656108597285. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 244. Train loss: 12.344538670999032.Train accuracy: 0.8733031674208145. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 245. Train loss: 12.492044060318559.Train accuracy: 0.8834841628959276. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 246. Train loss: 12.118714950702808.Train accuracy: 0.8755656108597285. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 247. Train loss: 12.278523604075113.Train accuracy: 0.8766968325791855. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 248. Train loss: 12.102488976937753.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 249. Train loss: 12.243222501542833.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 250. Train loss: 12.147721378891557.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 251. Train loss: 11.855891174740261.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 252. Train loss: 11.99499891422413.Train accuracy: 0.8755656108597285. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 253. Train loss: 11.87015927279437.Train accuracy: 0.8812217194570136. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 254. Train loss: 12.132292676855016.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 255. Train loss: 12.150025491361264.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 256. Train loss: 11.763953632778591.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 257. Train loss: 12.119677225748697.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 258. Train loss: 12.034411801232231.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 259. Train loss: 12.49102528889974.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 260. Train loss: 12.17786651187473.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 261. Train loss: 11.611676498695656.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 262. Train loss: 12.199685555917245.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 263. Train loss: 12.094779932940448.Train accuracy: 0.8778280542986425. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 264. Train loss: 12.082183608302364.Train accuracy: 0.8766968325791855. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 265. Train loss: 11.78482977549235.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 266. Train loss: 12.137562204290319.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 267. Train loss: 12.416262591326678.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 268. Train loss: 11.853828536139595.Train accuracy: 0.8812217194570136. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 269. Train loss: 11.819372918870714.Train accuracy: 0.8687782805429864. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 270. Train loss: 12.299538241492378.Train accuracy: 0.8778280542986425. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 271. Train loss: 11.688419659932455.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 272. Train loss: 12.167789017712629.Train accuracy: 0.8699095022624435. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 273. Train loss: 12.561979081895617.Train accuracy: 0.8721719457013575. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 274. Train loss: 12.115173516450104.Train accuracy: 0.8766968325791855. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 275. Train loss: 11.796987798478868.Train accuracy: 0.8687782805429864. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 276. Train loss: 12.182066758473715.Train accuracy: 0.8755656108597285. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 277. Train loss: 12.040936876226354.Train accuracy: 0.8846153846153846. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 278. Train loss: 12.4248803280018.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 279. Train loss: 11.97597991095649.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 280. Train loss: 12.123996999528673.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 281. Train loss: 12.26899097583912.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 282. Train loss: 11.947599093119303.Train accuracy: 0.8755656108597285. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 283. Train loss: 11.842286427815756.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 284. Train loss: 12.14256680453265.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 285. Train loss: 11.675251678184226.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 286. Train loss: 11.841932932535807.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 287. Train loss: 11.96263239118788.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 288. Train loss: 12.028474542829725.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 289. Train loss: 12.384463645793774.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 290. Train loss: 12.30230024125841.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 291. Train loss: 11.810009444201434.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 292. Train loss: 11.663734647962782.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 293. Train loss: 11.978746555469654.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 294. Train loss: 12.671214845445421.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 295. Train loss: 11.835890681655318.Train accuracy: 0.8891402714932126. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 296. Train loss: 11.879437658521864.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 297. Train loss: 11.786690341101753.Train accuracy: 0.8846153846153846. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 298. Train loss: 12.344592112082022.Train accuracy: 0.8868778280542986. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 299. Train loss: 12.789963669247097.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 300. Train loss: 12.254589698932788.Train accuracy: 0.8710407239819005. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 301. Train loss: 12.179971553661206.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 302. Train loss: 12.101388472097891.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 303. Train loss: 12.153640976658574.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 304. Train loss: 12.398692501915825.Train accuracy: 0.8744343891402715. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 305. Train loss: 12.207828804298684.Train accuracy: 0.8744343891402715. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 306. Train loss: 12.456482516394722.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 307. Train loss: 12.145228933404994.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 308. Train loss: 12.370185869711417.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 309. Train loss: 11.50240749782986.Train accuracy: 0.8789592760180995. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 310. Train loss: 12.484235622264721.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 311. Train loss: 12.74207932860763.Train accuracy: 0.8812217194570136. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 312. Train loss: 12.018031296906647.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 313. Train loss: 11.902036825815836.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 314. Train loss: 12.344359168299922.Train accuracy: 0.8755656108597285. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 315. Train loss: 11.869183787593135.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 316. Train loss: 11.868195904625786.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 317. Train loss: 11.805885367923313.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 318. Train loss: 11.516142544923005.Train accuracy: 0.8733031674208145. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 319. Train loss: 11.873227154767072.Train accuracy: 0.8823529411764706. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 320. Train loss: 11.922732070640281.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 321. Train loss: 12.257326267383716.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 322. Train loss: 12.367968965459752.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 323. Train loss: 11.96739857285111.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 324. Train loss: 12.311443346518057.Train accuracy: 0.8812217194570136. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 325. Train loss: 12.59898888623273.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 326. Train loss: 11.851552080225062.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 327. Train loss: 12.137611035947446.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 328. Train loss: 11.917080049161557.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 329. Train loss: 12.335517017929643.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 330. Train loss: 11.815592412595395.Train accuracy: 0.8823529411764706. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 331. Train loss: 12.337148878309462.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 332. Train loss: 12.578974123354312.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 333. Train loss: 12.125440526891637.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 334. Train loss: 12.187638212133336.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 335. Train loss: 11.535611859074345.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 336. Train loss: 11.981918264318395.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 337. Train loss: 11.669063956649214.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 338. Train loss: 12.232494583836308.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 339. Train loss: 11.88897438402529.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 340. Train loss: 12.110401330170808.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 341. Train loss: 11.721353142349809.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 342. Train loss: 11.477461214418765.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 343. Train loss: 12.173505376886439.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 344. Train loss: 12.141381334375453.Train accuracy: 0.8744343891402715. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 345. Train loss: 12.164484094690394.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 346. Train loss: 11.52489040516041.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 347. Train loss: 11.865498454482466.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 348. Train loss: 12.433912330203587.Train accuracy: 0.8766968325791855. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 349. Train loss: 12.136558179502133.Train accuracy: 0.8518099547511312. Test accuracy: 0.8416289592760181.\n",
      "==> Epoch: 350. Train loss: 12.290553322544804.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 351. Train loss: 11.962543381585014.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 352. Train loss: 12.374019817069724.Train accuracy: 0.8766968325791855. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 353. Train loss: 11.812588532765707.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 354. Train loss: 11.911437846996167.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 355. Train loss: 11.948204376079419.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 356. Train loss: 11.792427221934.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 357. Train loss: 12.068063983210811.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 358. Train loss: 12.071246465047201.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 359. Train loss: 12.144357999165853.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 360. Train loss: 11.781431886884901.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 361. Train loss: 11.757425184603091.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 362. Train loss: 12.131923657876474.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 363. Train loss: 11.871388523666948.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 364. Train loss: 12.178812132941353.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 365. Train loss: 11.885095861223009.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 366. Train loss: 11.974476708306206.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 367. Train loss: 11.494458428135625.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 368. Train loss: 11.720256063673231.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 369. Train loss: 11.825216434620044.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 370. Train loss: 11.743302998719392.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 371. Train loss: 12.283142195807564.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 372. Train loss: 12.221993976169163.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 373. Train loss: 12.026476771743209.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 374. Train loss: 11.922907069877342.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 375. Train loss: 11.566689915127224.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 376. Train loss: 11.785755351737693.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 377. Train loss: 11.779970063103569.Train accuracy: 0.8721719457013575. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 378. Train loss: 11.709219826592339.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 379. Train loss: 11.865832523063377.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 380. Train loss: 12.258610018977413.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 381. Train loss: 11.519750082934344.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 382. Train loss: 11.594437970055473.Train accuracy: 0.8778280542986425. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 383. Train loss: 12.695018167848941.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 384. Train loss: 11.771196118107548.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 385. Train loss: 11.627716576611554.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 386. Train loss: 12.239466967406097.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 387. Train loss: 11.805566999647352.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 388. Train loss: 12.102691597408718.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 389. Train loss: 11.43942568037245.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 390. Train loss: 11.861984429536042.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 391. Train loss: 11.806663389559146.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 392. Train loss: 11.587482505374485.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 393. Train loss: 12.117097360116464.Train accuracy: 0.8823529411764706. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 394. Train loss: 11.902506245507134.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 395. Train loss: 11.571013556586372.Train accuracy: 0.8823529411764706. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 396. Train loss: 12.566429244147407.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 397. Train loss: 11.97712465568825.Train accuracy: 0.8902714932126696. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 398. Train loss: 12.193894103721336.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 399. Train loss: 11.791139072842068.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 400. Train loss: 12.009004222022163.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 401. Train loss: 12.090757776189733.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 402. Train loss: 11.93927854961819.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 403. Train loss: 11.6823394563463.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 404. Train loss: 11.837695669244837.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 405. Train loss: 11.54411342408922.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 406. Train loss: 11.751785490247938.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 407. Train loss: 11.62507982607241.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 408. Train loss: 11.868831634521484.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 409. Train loss: 11.62360323799981.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 410. Train loss: 11.531920009189182.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 411. Train loss: 12.035709822619403.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 412. Train loss: 11.627043794702601.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 413. Train loss: 11.485928482479519.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 414. Train loss: 11.682121418140552.Train accuracy: 0.8846153846153846. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 415. Train loss: 12.022743242758292.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 416. Train loss: 11.852446750358299.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 417. Train loss: 12.108436125296134.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 418. Train loss: 11.816680166456434.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 419. Train loss: 11.747138800444427.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 420. Train loss: 11.898500318880435.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 421. Train loss: 11.88065622470997.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 422. Train loss: 12.28898705376519.Train accuracy: 0.8902714932126696. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 423. Train loss: 12.34478489557902.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 424. Train loss: 11.898686055783871.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 425. Train loss: 11.53773381974962.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 426. Train loss: 12.3501205974155.Train accuracy: 0.8834841628959276. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 427. Train loss: 11.989278634389242.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 428. Train loss: 11.756342004846644.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 429. Train loss: 12.134484467683015.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 430. Train loss: 11.541455886982105.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 431. Train loss: 11.700553258260092.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 432. Train loss: 11.441205413253218.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 433. Train loss: 12.064456498181379.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 434. Train loss: 11.963536121227124.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 435. Train loss: 11.857757868590179.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 436. Train loss: 11.796033435397678.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 437. Train loss: 11.445717811584473.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 438. Train loss: 11.981436535164162.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 439. Train loss: 11.892177087289316.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 440. Train loss: 11.710584852430555.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 441. Train loss: 11.878511022638392.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 442. Train loss: 11.57619622901634.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 443. Train loss: 11.586823092566597.Train accuracy: 0.8800904977375565. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 444. Train loss: 12.017496091348153.Train accuracy: 0.8755656108597285. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 445. Train loss: 11.765068725303367.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 446. Train loss: 11.972331523895264.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 447. Train loss: 11.781001974035192.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 448. Train loss: 11.888344146587231.Train accuracy: 0.8868778280542986. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 449. Train loss: 11.96334966023763.Train accuracy: 0.8925339366515838. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 450. Train loss: 11.519473588025129.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 451. Train loss: 11.805605817724157.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 452. Train loss: 11.32346103809498.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 453. Train loss: 12.052753448486328.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 454. Train loss: 11.509866537871185.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 455. Train loss: 11.858130243089464.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 456. Train loss: 12.025336954328749.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 457. Train loss: 11.734129340560347.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 458. Train loss: 11.665969371795654.Train accuracy: 0.8812217194570136. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 459. Train loss: 12.026758776770698.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 460. Train loss: 11.520301024119059.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 461. Train loss: 11.693261764667652.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 462. Train loss: 12.078430546654594.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 463. Train loss: 11.80346319410536.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 464. Train loss: 11.546475357479519.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 465. Train loss: 12.22391614207515.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 466. Train loss: 11.804272263138383.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 467. Train loss: 11.742730423256203.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 468. Train loss: 11.780762018980804.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 469. Train loss: 12.409188129283764.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 470. Train loss: 11.506805331618697.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 471. Train loss: 12.283943900355586.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 472. Train loss: 11.876552670090287.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 473. Train loss: 11.92715945067229.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 474. Train loss: 12.051578168515805.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 475. Train loss: 11.7232671490422.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 476. Train loss: 11.73561922709147.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 477. Train loss: 11.829917890054208.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 478. Train loss: 11.94795235881099.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 479. Train loss: 11.835280259450277.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 480. Train loss: 11.738620228237576.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 481. Train loss: 11.519993110939309.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 482. Train loss: 12.043733826390019.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 483. Train loss: 12.145243415126094.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 484. Train loss: 11.552311208513048.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 485. Train loss: 11.997421653182418.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 486. Train loss: 12.360895669018781.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 487. Train loss: 12.133786960884377.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 488. Train loss: 11.455738155930131.Train accuracy: 0.8755656108597285. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 489. Train loss: 12.031854576534695.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 490. Train loss: 12.012697643703884.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 491. Train loss: 11.563573766637731.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 492. Train loss: 11.457513915167915.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 493. Train loss: 11.897605613425926.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 494. Train loss: 11.905122951224998.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 495. Train loss: 12.081431848031503.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 496. Train loss: 12.10163891756976.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 497. Train loss: 11.800184620751274.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 498. Train loss: 12.092087410114429.Train accuracy: 0.8789592760180995. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 499. Train loss: 11.998128290529605.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 500. Train loss: 12.153689631709346.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 501. Train loss: 11.510895782046848.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 502. Train loss: 12.143974710393834.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 503. Train loss: 11.84919963059602.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 504. Train loss: 11.810643990834555.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 505. Train loss: 11.708806408776177.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 506. Train loss: 12.117629298457393.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 507. Train loss: 12.106364515092638.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 508. Train loss: 11.528181111371076.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 509. Train loss: 11.962091710832384.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 510. Train loss: 11.668448289235434.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 511. Train loss: 12.00885804494222.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 512. Train loss: 11.992640106766313.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 513. Train loss: 12.30296246210734.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 514. Train loss: 11.67250426610311.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 515. Train loss: 11.107530117034912.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 516. Train loss: 12.010720977076778.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 517. Train loss: 11.701521308333785.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 518. Train loss: 12.638395185823795.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 519. Train loss: 11.772035740039966.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 520. Train loss: 11.89904910546762.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 521. Train loss: 11.730850025459572.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 522. Train loss: 11.918473296695286.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 523. Train loss: 11.390150388081869.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 524. Train loss: 11.806578706811976.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 525. Train loss: 12.146111753251818.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 526. Train loss: 11.929833323867232.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 527. Train loss: 11.872420310974121.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 528. Train loss: 11.510609909340188.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 529. Train loss: 11.703335373489946.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 530. Train loss: 11.345199584960938.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 531. Train loss: 11.913049503608987.Train accuracy: 0.8914027149321267. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 532. Train loss: 11.663085125110767.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 533. Train loss: 12.206005838182238.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 534. Train loss: 11.994408395555284.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 535. Train loss: 12.11031784834685.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 536. Train loss: 11.508722058048955.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 537. Train loss: 12.172152519226074.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 538. Train loss: 11.43429840935601.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 539. Train loss: 11.691998676017478.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 540. Train loss: 11.694333200101498.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 541. Train loss: 11.680964946746826.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 542. Train loss: 11.792458640204536.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 543. Train loss: 11.636365396005136.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 544. Train loss: 11.357072265059859.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 545. Train loss: 11.905341272000912.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 546. Train loss: 11.503378196998879.Train accuracy: 0.8834841628959276. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 547. Train loss: 11.517686861532706.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 548. Train loss: 11.836419741312662.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 549. Train loss: 11.97549189461602.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 550. Train loss: 11.53483051723904.Train accuracy: 0.8902714932126696. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 551. Train loss: 11.626705169677734.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 552. Train loss: 11.734831668712475.Train accuracy: 0.8925339366515838. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 553. Train loss: 11.930188920762804.Train accuracy: 0.8902714932126696. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 554. Train loss: 11.170715791207773.Train accuracy: 0.8936651583710408. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 555. Train loss: 11.926517362947818.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 556. Train loss: 11.938814145547372.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 557. Train loss: 11.550767563007495.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 558. Train loss: 12.086900428489402.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 559. Train loss: 11.798632974977847.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 560. Train loss: 11.537219983560068.Train accuracy: 0.8936651583710408. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 561. Train loss: 11.933914785031918.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 562. Train loss: 11.625218179490831.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 563. Train loss: 11.742490874396431.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 564. Train loss: 11.569740736926043.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 565. Train loss: 11.728703693107322.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 566. Train loss: 11.814227139508283.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 567. Train loss: 12.028338820845992.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 568. Train loss: 11.93383017292729.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 569. Train loss: 11.80254943282516.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 570. Train loss: 12.130241552988688.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 571. Train loss: 12.182788601628056.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 572. Train loss: 11.229550414615208.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 573. Train loss: 11.568500465816921.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 574. Train loss: 11.71545139948527.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 575. Train loss: 11.555824933228669.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 576. Train loss: 11.789047929975721.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 577. Train loss: 11.827807514755815.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 578. Train loss: 11.62358026151304.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 579. Train loss: 11.892659699475324.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 580. Train loss: 11.555782176830151.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 581. Train loss: 11.366270471502233.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 582. Train loss: 11.735619085806388.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 583. Train loss: 12.203108487305817.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 584. Train loss: 11.852032838044343.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 585. Train loss: 11.561674259327075.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 586. Train loss: 11.96150274629946.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 587. Train loss: 12.176291960257071.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 588. Train loss: 11.854263482270417.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 589. Train loss: 11.525006629802563.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 590. Train loss: 11.404171184257224.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 591. Train loss: 11.443695615839076.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 592. Train loss: 11.598065464584916.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 593. Train loss: 11.107579407868561.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 594. Train loss: 12.550410517939815.Train accuracy: 0.8902714932126696. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 595. Train loss: 11.253649181789822.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 596. Train loss: 11.980317698584663.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 597. Train loss: 11.890283090096933.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 598. Train loss: 11.85853241108082.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 599. Train loss: 11.92071459028456.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 600. Train loss: 11.956002412018952.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 601. Train loss: 11.807171291775173.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 602. Train loss: 11.775759149480749.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 603. Train loss: 11.22062455283271.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 604. Train loss: 11.53893808082298.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 605. Train loss: 11.760895835028755.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 606. Train loss: 11.933366651888248.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 607. Train loss: 11.534376356336805.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 608. Train loss: 11.424756986123544.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 609. Train loss: 12.11109424520422.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 610. Train loss: 12.041826530739113.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 611. Train loss: 11.78033028708564.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 612. Train loss: 11.594165890305131.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 613. Train loss: 11.925471606077972.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 614. Train loss: 11.931918453287196.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 615. Train loss: 11.89838695526123.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 616. Train loss: 11.612573553014684.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 617. Train loss: 11.689957265500668.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 618. Train loss: 12.062103977909794.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 619. Train loss: 11.776547696855333.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 620. Train loss: 11.52366590499878.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 621. Train loss: 11.617359638214111.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 622. Train loss: 11.513591554429796.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 623. Train loss: 11.428152808436641.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 624. Train loss: 12.272394162637216.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 625. Train loss: 12.031087080637613.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 626. Train loss: 11.724868474183259.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 627. Train loss: 11.770452128516304.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 628. Train loss: 11.594788674955014.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 629. Train loss: 11.688928021325005.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 630. Train loss: 11.456262641482883.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 631. Train loss: 11.601991953673187.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 632. Train loss: 12.050731782559994.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 633. Train loss: 11.821555137634277.Train accuracy: 0.8959276018099548. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 634. Train loss: 11.436712953779432.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 635. Train loss: 11.445813214337385.Train accuracy: 0.8959276018099548. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 636. Train loss: 11.436339184089944.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 637. Train loss: 11.757794910007053.Train accuracy: 0.8947963800904978. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 638. Train loss: 11.541397465599907.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 639. Train loss: 11.329053507910835.Train accuracy: 0.8959276018099548. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 640. Train loss: 11.987556899035418.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 641. Train loss: 11.610539189091435.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 642. Train loss: 11.529759230437103.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 643. Train loss: 12.326506508721245.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 644. Train loss: 11.627174977903012.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 645. Train loss: 11.765316009521484.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 646. Train loss: 11.582798622272632.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 647. Train loss: 11.63436621206778.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 648. Train loss: 12.145768271552193.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 649. Train loss: 11.391069977371782.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 650. Train loss: 11.704447163475884.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 651. Train loss: 11.957689196975142.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 652. Train loss: 11.393260037457502.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 653. Train loss: 11.683888894540292.Train accuracy: 0.8947963800904978. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 654. Train loss: 11.917052993067989.Train accuracy: 0.8789592760180995. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 655. Train loss: 11.587423024354157.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 656. Train loss: 11.67538778870194.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 657. Train loss: 11.637058823196977.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 658. Train loss: 11.65954190713388.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 659. Train loss: 11.896785135622379.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 660. Train loss: 11.986877723976418.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 661. Train loss: 11.34837297157005.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 662. Train loss: 11.444763059969302.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 663. Train loss: 12.002307132438377.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 664. Train loss: 11.70426172680325.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 665. Train loss: 11.530658845548276.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 666. Train loss: 11.146296624784116.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 667. Train loss: 11.630761164206046.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 668. Train loss: 11.759029829943621.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 669. Train loss: 11.494377312836823.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 670. Train loss: 12.128755057299578.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 671. Train loss: 11.381165840007641.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 672. Train loss: 11.672234376271566.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 673. Train loss: 11.738667081903529.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 674. Train loss: 11.155269057662398.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 675. Train loss: 11.590989430745443.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 676. Train loss: 11.760063242029261.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 677. Train loss: 11.775438238073278.Train accuracy: 0.8891402714932126. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 678. Train loss: 11.518056622257939.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 679. Train loss: 11.756205594098127.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 680. Train loss: 11.509301980336508.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 681. Train loss: 11.154849193714282.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 682. Train loss: 11.849528189058658.Train accuracy: 0.8891402714932126. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 683. Train loss: 11.577903482649061.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 684. Train loss: 12.276422394646538.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 685. Train loss: 11.465237582171405.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 686. Train loss: 11.19176114047015.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 687. Train loss: 11.578005349194562.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 688. Train loss: 11.533725667882848.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 689. Train loss: 11.333675296218306.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 690. Train loss: 11.557425958138925.Train accuracy: 0.8789592760180995. Test accuracy: 0.8778280542986425.\n",
      "==> Epoch: 691. Train loss: 11.589260684119331.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 692. Train loss: 11.889921170693857.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 693. Train loss: 11.889197914688676.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 694. Train loss: 11.588340158815738.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 695. Train loss: 12.047535613731101.Train accuracy: 0.8857466063348416. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 696. Train loss: 11.985709013762298.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 697. Train loss: 12.007689846886528.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 698. Train loss: 11.850602750424985.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 699. Train loss: 11.759089010733145.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 700. Train loss: 11.875708791944716.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 701. Train loss: 12.11658145763256.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 702. Train loss: 11.62601285510593.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 703. Train loss: 11.549239370557997.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 704. Train loss: 11.479185263315836.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 705. Train loss: 11.573787283014369.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 706. Train loss: 11.469660511723271.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 707. Train loss: 11.612464480929905.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 708. Train loss: 11.72148406064069.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 709. Train loss: 11.81684109016701.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 710. Train loss: 11.679250575878003.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 711. Train loss: 11.606902917226156.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 712. Train loss: 11.760719829135471.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 713. Train loss: 11.528669622209337.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 714. Train loss: 11.974081675211588.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 715. Train loss: 11.747040783917463.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 716. Train loss: 11.580318256660744.Train accuracy: 0.8925339366515838. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 717. Train loss: 11.710719285187897.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 718. Train loss: 11.793195883433023.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 719. Train loss: 11.487273286890101.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 720. Train loss: 11.437084286301225.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 721. Train loss: 11.642259191583705.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 722. Train loss: 11.550704726466426.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 723. Train loss: 11.421374921445492.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 724. Train loss: 11.718890825907389.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 725. Train loss: 11.23168060514662.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 726. Train loss: 11.805206192864311.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 727. Train loss: 11.331190533108181.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 728. Train loss: 11.836202374211064.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 729. Train loss: 11.681641154819065.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 730. Train loss: 11.392017717714664.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 731. Train loss: 10.960315386454264.Train accuracy: 0.8970588235294118. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 732. Train loss: 11.18592013253106.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 733. Train loss: 11.455215012585676.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 734. Train loss: 11.350316612808793.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 735. Train loss: 11.381788306766087.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 736. Train loss: 11.60787093197858.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 737. Train loss: 11.50650656664813.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 738. Train loss: 11.5228143091555.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 739. Train loss: 11.563950538635254.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 740. Train loss: 11.765519230454057.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 741. Train loss: 11.534180711816859.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 742. Train loss: 12.194246044865361.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 743. Train loss: 11.52140310075548.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 744. Train loss: 11.722624372552943.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 745. Train loss: 11.506945186191135.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 746. Train loss: 11.865784645080566.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 747. Train loss: 11.53335119176794.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 748. Train loss: 11.408286271271882.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 749. Train loss: 11.84852389936094.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 750. Train loss: 11.329937051843714.Train accuracy: 0.8981900452488688. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 751. Train loss: 11.799748650303593.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 752. Train loss: 11.65679520147818.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 753. Train loss: 11.863539501472756.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 754. Train loss: 12.26038916905721.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 755. Train loss: 12.00346526393184.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 756. Train loss: 11.771942615509033.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 757. Train loss: 11.641224560914216.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 758. Train loss: 11.8564229188142.Train accuracy: 0.8868778280542986. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 759. Train loss: 11.65867754265114.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 760. Train loss: 11.514937294854057.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 761. Train loss: 11.638113569330287.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 762. Train loss: 11.297697261527732.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 763. Train loss: 11.198496482990405.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 764. Train loss: 11.728945325922083.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 765. Train loss: 11.60755925708347.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 766. Train loss: 11.981823356063277.Train accuracy: 0.8925339366515838. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 767. Train loss: 11.42744239171346.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 768. Train loss: 11.971669779883491.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 769. Train loss: 11.47972438953541.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 770. Train loss: 11.485352516174316.Train accuracy: 0.8936651583710408. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 771. Train loss: 11.78036677395856.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 772. Train loss: 11.672489307544849.Train accuracy: 0.8970588235294118. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 773. Train loss: 11.659675615805167.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 774. Train loss: 11.604163275824654.Train accuracy: 0.8925339366515838. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 775. Train loss: 11.402818520863852.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 776. Train loss: 11.766023635864258.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 777. Train loss: 11.215093294779459.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 778. Train loss: 11.470064904954699.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 779. Train loss: 11.34837657433969.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 780. Train loss: 11.824987287874576.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 781. Train loss: 11.763751453823513.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 782. Train loss: 11.243104210606328.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 783. Train loss: 11.143007684636999.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 784. Train loss: 11.426904766647905.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 785. Train loss: 11.846938645398176.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 786. Train loss: 11.566590397446245.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 787. Train loss: 11.536212320680972.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 788. Train loss: 11.797931229626691.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 789. Train loss: 11.78251478407118.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 790. Train loss: 11.89591356560036.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 791. Train loss: 11.642011289243344.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 792. Train loss: 11.621636337704128.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 793. Train loss: 11.86142960301152.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 794. Train loss: 11.581242031521267.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 795. Train loss: 11.609652801796242.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 796. Train loss: 11.60015222761366.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 797. Train loss: 11.829420919771549.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 798. Train loss: 11.811443116929796.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 799. Train loss: 10.973292615678576.Train accuracy: 0.8993212669683258. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 800. Train loss: 11.627351813846165.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 801. Train loss: 11.855753827978063.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 802. Train loss: 11.639809502495659.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 803. Train loss: 11.407976892259386.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 804. Train loss: 11.729861153496635.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 805. Train loss: 11.234412052013257.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 806. Train loss: 11.584482510884603.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 807. Train loss: 11.517677889929878.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 808. Train loss: 11.32149037608394.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 809. Train loss: 11.78485354670772.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 810. Train loss: 11.82276404345477.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 811. Train loss: 11.624438321148908.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 812. Train loss: 11.266800262309888.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 813. Train loss: 11.204780525631374.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 814. Train loss: 11.346332214496753.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 815. Train loss: 11.51376289791531.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 816. Train loss: 11.597280166767261.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 817. Train loss: 11.926919442635995.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 818. Train loss: 11.66575288772583.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 819. Train loss: 11.268034546463578.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 820. Train loss: 11.795094472390634.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 821. Train loss: 11.855622662438286.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 822. Train loss: 11.881863134878653.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 823. Train loss: 11.65755976570977.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 824. Train loss: 11.55552445517646.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 825. Train loss: 12.224286361976906.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 826. Train loss: 11.094710738570601.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 827. Train loss: 11.823884080957484.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 828. Train loss: 11.629021185415763.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 829. Train loss: 11.712846455750642.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 830. Train loss: 11.585908995734322.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 831. Train loss: 11.500293996598986.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 832. Train loss: 11.494813707139757.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 833. Train loss: 11.396516234786422.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 834. Train loss: 11.34748308746903.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 835. Train loss: 11.272962340602168.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 836. Train loss: 11.838473602577492.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 837. Train loss: 11.749176837779858.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 838. Train loss: 11.460153862282082.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 839. Train loss: 11.935953034294975.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 840. Train loss: 11.997399029908356.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 841. Train loss: 11.854766033313892.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 842. Train loss: 11.740028769881636.Train accuracy: 0.8925339366515838. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 843. Train loss: 11.430717874456334.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 844. Train loss: 11.94353598135489.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 845. Train loss: 11.710748919734248.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 846. Train loss: 11.787099414401585.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 847. Train loss: 11.585492893501565.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 848. Train loss: 11.448167818563956.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 849. Train loss: 11.414110395643446.Train accuracy: 0.8959276018099548. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 850. Train loss: 11.558467917972141.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 851. Train loss: 11.533744123246935.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 852. Train loss: 11.369819817719636.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 853. Train loss: 11.498536904652914.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 854. Train loss: 12.013500107659233.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 855. Train loss: 11.524169745268646.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 856. Train loss: 11.103419816052472.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 857. Train loss: 11.77995491027832.Train accuracy: 0.8880090497737556. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 858. Train loss: 11.924809597156665.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 859. Train loss: 11.646744516160753.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 860. Train loss: 11.78927861319648.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 861. Train loss: 11.072265395411739.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 862. Train loss: 11.658390892876518.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 863. Train loss: 10.987352353555185.Train accuracy: 0.8993212669683258. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 864. Train loss: 11.352983916247332.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 865. Train loss: 11.294146219889322.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 866. Train loss: 11.561237812042236.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 867. Train loss: 11.29397996266683.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 868. Train loss: 11.07984753008242.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 869. Train loss: 12.25847449126067.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 870. Train loss: 11.49447907341851.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 871. Train loss: 11.691953376487449.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 872. Train loss: 11.394151316748726.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 873. Train loss: 11.249558201542607.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 874. Train loss: 11.938037254192212.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 875. Train loss: 11.491018754464609.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 876. Train loss: 11.414643729174578.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 877. Train loss: 11.012441705774378.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 878. Train loss: 11.677360163794624.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 879. Train loss: 12.314413812425402.Train accuracy: 0.9015837104072398. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 880. Train loss: 11.529514877884477.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 881. Train loss: 11.761914871357105.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 882. Train loss: 11.412117887426305.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 883. Train loss: 11.551547138779252.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 884. Train loss: 12.074358887142605.Train accuracy: 0.8891402714932126. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 885. Train loss: 11.615944138279668.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 886. Train loss: 12.122537736539487.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 887. Train loss: 11.958732693283647.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 888. Train loss: 11.169495741526285.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 889. Train loss: 11.267957228201407.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 890. Train loss: 11.139760194001374.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 891. Train loss: 11.237228375894052.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 892. Train loss: 11.58090587898537.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 893. Train loss: 11.401498388361048.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 894. Train loss: 11.776699825569436.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 895. Train loss: 11.856897548392967.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 896. Train loss: 11.564611699846056.Train accuracy: 0.8959276018099548. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 897. Train loss: 11.618895000881619.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 898. Train loss: 11.513004974082664.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 899. Train loss: 11.507610126777932.Train accuracy: 0.8981900452488688. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 900. Train loss: 11.912864473130968.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 901. Train loss: 11.77985050060131.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 902. Train loss: 11.473123868306478.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 903. Train loss: 11.362421106409144.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 904. Train loss: 11.33895100487603.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 905. Train loss: 11.567425904450593.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 906. Train loss: 11.580939098640725.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 907. Train loss: 11.695125456209537.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 908. Train loss: 12.061809292546025.Train accuracy: 0.8857466063348416. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 909. Train loss: 11.58163019462868.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 910. Train loss: 11.30007376494231.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 911. Train loss: 11.595284620920816.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 912. Train loss: 11.282726411466244.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 913. Train loss: 11.57991940886886.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 914. Train loss: 12.069261762830946.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 915. Train loss: 11.544145001305473.Train accuracy: 0.8936651583710408. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 916. Train loss: 12.05752909625018.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 917. Train loss: 11.668072824124936.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 918. Train loss: 11.597568759211788.Train accuracy: 0.9004524886877828. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 919. Train loss: 11.645135526303891.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 920. Train loss: 11.81158662725378.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 921. Train loss: 11.827841016981337.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 922. Train loss: 11.382000322695133.Train accuracy: 0.8981900452488688. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 923. Train loss: 11.853033436669243.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 924. Train loss: 11.34731806649102.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 925. Train loss: 11.188432410911277.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 926. Train loss: 11.536895981541386.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 927. Train loss: 11.912081435874656.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 928. Train loss: 11.573071868331343.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 929. Train loss: 11.537200486218488.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 930. Train loss: 11.53922141039813.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 931. Train loss: 11.447409205966526.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 932. Train loss: 11.408571561177572.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 933. Train loss: 11.6271541206925.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 934. Train loss: 11.416603406270346.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 935. Train loss: 11.556348447446469.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 936. Train loss: 11.561466799841988.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 937. Train loss: 11.546292870133012.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 938. Train loss: 11.931401941511366.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 939. Train loss: 11.483711737173575.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 940. Train loss: 11.455566123679832.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 941. Train loss: 11.673804848282426.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 942. Train loss: 11.387995490321407.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 943. Train loss: 11.835856843877721.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 944. Train loss: 11.897930162924307.Train accuracy: 0.8891402714932126. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 945. Train loss: 11.688554816775852.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 946. Train loss: 11.500671121809217.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 947. Train loss: 11.330594928176314.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 948. Train loss: 11.623155505568892.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 949. Train loss: 11.611547346468326.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 950. Train loss: 11.271778106689453.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 951. Train loss: 11.465946903935185.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 952. Train loss: 11.575659769552725.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 953. Train loss: 11.874872048695883.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 954. Train loss: 11.620804680718315.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 955. Train loss: 11.75232940249973.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 956. Train loss: 11.998484788117585.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 957. Train loss: 11.255768705297399.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 958. Train loss: 11.410826859650788.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 959. Train loss: 12.118830168688739.Train accuracy: 0.8925339366515838. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 960. Train loss: 11.665698881502506.Train accuracy: 0.8778280542986425. Test accuracy: 0.8823529411764706.\n",
      "==> Epoch: 961. Train loss: 11.372516402491817.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 962. Train loss: 11.773465227197718.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 963. Train loss: 11.270009711936668.Train accuracy: 0.9004524886877828. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 964. Train loss: 11.50500691378558.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 965. Train loss: 11.822433012503165.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 966. Train loss: 11.3516392354612.Train accuracy: 0.8936651583710408. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 967. Train loss: 11.455259870599818.Train accuracy: 0.8993212669683258. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 968. Train loss: 11.12799792819553.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 969. Train loss: 11.704794283266422.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 970. Train loss: 11.518510059074119.Train accuracy: 0.8993212669683258. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 971. Train loss: 11.229241300512243.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 972. Train loss: 11.61891727094297.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 973. Train loss: 11.53621569386235.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 974. Train loss: 11.32962096178973.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 975. Train loss: 11.614505626537182.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 976. Train loss: 11.847364531622993.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 977. Train loss: 11.339342293915925.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 978. Train loss: 11.78700480637727.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 979. Train loss: 11.522562397850884.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 980. Train loss: 11.726264900631374.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 981. Train loss: 11.330898231930203.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 982. Train loss: 11.737301243676079.Train accuracy: 0.8993212669683258. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 983. Train loss: 11.351511460763437.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 984. Train loss: 11.384863288314254.Train accuracy: 0.8925339366515838. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 985. Train loss: 11.274971396834761.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 986. Train loss: 11.756816475479692.Train accuracy: 0.8959276018099548. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 987. Train loss: 11.328629546695286.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 988. Train loss: 11.715305451993588.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 989. Train loss: 11.272285832299126.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 990. Train loss: 11.349934224729184.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 991. Train loss: 11.406408115669533.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 992. Train loss: 11.88626425354569.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 993. Train loss: 11.490829768004241.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 994. Train loss: 11.506284183926052.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 995. Train loss: 11.412014113532173.Train accuracy: 0.8947963800904978. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 996. Train loss: 11.920013321770561.Train accuracy: 0.9004524886877828. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 997. Train loss: 11.359904412870053.Train accuracy: 0.8891402714932126. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 998. Train loss: 11.319044289765534.Train accuracy: 0.8970588235294118. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 999. Train loss: 11.873910480075413.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DFS(N=None, alpha1=0.01, alpha2=0.0, batch_size=32, dropout_rate=0.5,\n",
       "  lambda1=2, lambda2=1.0, layers_sizes=[225, 64, 64, 32, 2],\n",
       "  num_epochs=1000, verbose=True)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = DFS([64, 64, 32, 2], lambda1=2, alpha1=1e-2,  num_epochs=1000, verbose=True, dropout_rate=0.5)\n",
    "\n",
    "dfs.fit(train_X, train_y['y'], test_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:22:17.882568Z",
     "start_time": "2018-05-01T22:22:17.620161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.904977375566\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c3fad1550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(mtcs.accuracy_score(test_y, dfs.predict(test_X)))\n",
    "plt.hist(dfs.get_features_weights());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:24:22.699574Z",
     "start_time": "2018-05-01T22:23:24.868797Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 18.39456409878201.Train accuracy: 0.7273755656108597. Test accuracy: 0.7330316742081447.\n",
      "==> Epoch: 1. Train loss: 15.331250967802825.Train accuracy: 0.8167420814479638. Test accuracy: 0.832579185520362.\n",
      "==> Epoch: 2. Train loss: 13.275470945570204.Train accuracy: 0.8484162895927602. Test accuracy: 0.8687782805429864.\n",
      "==> Epoch: 3. Train loss: 12.253489847536441.Train accuracy: 0.8699095022624435. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 4. Train loss: 11.63012460426048.Train accuracy: 0.8755656108597285. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 5. Train loss: 11.020048088497585.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 6. Train loss: 11.709815590469926.Train accuracy: 0.8755656108597285. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 7. Train loss: 11.459298336947406.Train accuracy: 0.8789592760180995. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 8. Train loss: 10.945051705395734.Train accuracy: 0.8800904977375565. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 9. Train loss: 11.027090637772172.Train accuracy: 0.8744343891402715. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 10. Train loss: 10.46155779450028.Train accuracy: 0.8744343891402715. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 11. Train loss: 10.729759604842574.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 12. Train loss: 10.7357284228007.Train accuracy: 0.8778280542986425. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 13. Train loss: 11.18474476425736.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 14. Train loss: 10.369990790331805.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 15. Train loss: 10.187592683015046.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 16. Train loss: 10.299890509358159.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 17. Train loss: 10.63641811300207.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 18. Train loss: 10.395154281898781.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 19. Train loss: 10.230265635031241.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 20. Train loss: 10.037618478139242.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 21. Train loss: 10.248150172056976.Train accuracy: 0.8891402714932126. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 22. Train loss: 10.017257195931894.Train accuracy: 0.8834841628959276. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 23. Train loss: 9.930288120552346.Train accuracy: 0.8823529411764706. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 24. Train loss: 9.930550787183973.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 25. Train loss: 10.184004377435755.Train accuracy: 0.8880090497737556. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 26. Train loss: 10.020546524612993.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 27. Train loss: 10.181576057716653.Train accuracy: 0.8823529411764706. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 28. Train loss: 9.826258376792625.Train accuracy: 0.8857466063348416. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 29. Train loss: 10.317805025312635.Train accuracy: 0.8846153846153846. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 30. Train loss: 9.84193374492504.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 31. Train loss: 10.060777849621243.Train accuracy: 0.8857466063348416. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 32. Train loss: 9.689673176518193.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 33. Train loss: 10.24023785414519.Train accuracy: 0.8846153846153846. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 34. Train loss: 10.236841519673666.Train accuracy: 0.8868778280542986. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 35. Train loss: 9.186190923055014.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 36. Train loss: 9.660997938226771.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 37. Train loss: 9.422027075732196.Train accuracy: 0.8868778280542986. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 38. Train loss: 9.914873388078478.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 39. Train loss: 9.665229523623431.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 40. Train loss: 9.417459770485207.Train accuracy: 0.8868778280542986. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 41. Train loss: 9.51103185724329.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 42. Train loss: 9.220911511668453.Train accuracy: 0.8880090497737556. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 43. Train loss: 9.226691246032715.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 44. Train loss: 9.467614191549796.Train accuracy: 0.8846153846153846. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 45. Train loss: 9.224604580137465.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 46. Train loss: 9.60510829642967.Train accuracy: 0.8902714932126696. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 47. Train loss: 9.323914916427047.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 48. Train loss: 9.557013441015172.Train accuracy: 0.8857466063348416. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 49. Train loss: 9.466661594532154.Train accuracy: 0.8857466063348416. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 50. Train loss: 9.39988085075661.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 51. Train loss: 9.384998904334175.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 52. Train loss: 9.511318692454585.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 53. Train loss: 9.613647584561948.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 54. Train loss: 9.188585767039546.Train accuracy: 0.8947963800904978. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 55. Train loss: 9.61727656258477.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 56. Train loss: 9.095754676394993.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 57. Train loss: 8.989849037594265.Train accuracy: 0.8880090497737556. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 58. Train loss: 9.687736140357124.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 59. Train loss: 9.030576555817216.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 60. Train loss: 9.085744116041395.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 61. Train loss: 9.173215353930438.Train accuracy: 0.8902714932126696. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 62. Train loss: 9.501851382078948.Train accuracy: 0.8891402714932126. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 63. Train loss: 8.894288716492829.Train accuracy: 0.8925339366515838. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 64. Train loss: 8.759626750592831.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 65. Train loss: 9.096529051109597.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 66. Train loss: 8.969999339845446.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 67. Train loss: 8.888898257856015.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 68. Train loss: 9.06474526723226.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 69. Train loss: 8.918622529065168.Train accuracy: 0.8947963800904978. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 70. Train loss: 8.849118532957855.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 71. Train loss: 8.739125975856075.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 72. Train loss: 8.749395970945004.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 73. Train loss: 9.271473955225062.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 74. Train loss: 9.161830213334826.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 75. Train loss: 8.777003173474911.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 76. Train loss: 8.745085257071036.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 77. Train loss: 8.654491230293557.Train accuracy: 0.9015837104072398. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 78. Train loss: 8.947173047948766.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 79. Train loss: 9.078781869676378.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 80. Train loss: 8.643381577950937.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 81. Train loss: 8.525889431988752.Train accuracy: 0.9038461538461539. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 82. Train loss: 8.887078726733172.Train accuracy: 0.9083710407239819. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 83. Train loss: 8.51610502490291.Train accuracy: 0.9015837104072398. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 84. Train loss: 8.492846400649459.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 85. Train loss: 8.623344597993073.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 86. Train loss: 8.593423048655191.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 87. Train loss: 8.654157444282815.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 88. Train loss: 8.54421059290568.Train accuracy: 0.9015837104072398. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 89. Train loss: 8.314978016747368.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 90. Train loss: 8.294793199609828.Train accuracy: 0.9027149321266968. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 91. Train loss: 8.613724849842212.Train accuracy: 0.9049773755656109. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 92. Train loss: 8.012017303042942.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 93. Train loss: 8.007683091693455.Train accuracy: 0.9038461538461539. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 94. Train loss: 8.412059872238725.Train accuracy: 0.9038461538461539. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 95. Train loss: 7.991058685161449.Train accuracy: 0.9038461538461539. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 96. Train loss: 8.381377970730817.Train accuracy: 0.9061085972850679. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 97. Train loss: 8.505371376320168.Train accuracy: 0.9083710407239819. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 98. Train loss: 8.163945966296726.Train accuracy: 0.9095022624434389. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 99. Train loss: 8.249575288207442.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 100. Train loss: 7.939540933679651.Train accuracy: 0.9095022624434389. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 101. Train loss: 8.127961962311357.Train accuracy: 0.9072398190045249. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 102. Train loss: 7.944432797255339.Train accuracy: 0.9072398190045249. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 103. Train loss: 8.16725398876049.Train accuracy: 0.9038461538461539. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 104. Train loss: 8.226487866154423.Train accuracy: 0.9072398190045249. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 105. Train loss: 8.256933706778067.Train accuracy: 0.9106334841628959. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 106. Train loss: 7.78800266760367.Train accuracy: 0.9095022624434389. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 107. Train loss: 8.0742042682789.Train accuracy: 0.9083710407239819. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 108. Train loss: 8.033605178197226.Train accuracy: 0.9095022624434389. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 109. Train loss: 8.459802406805533.Train accuracy: 0.9128959276018099. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 110. Train loss: 8.636608688919633.Train accuracy: 0.9083710407239819. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 111. Train loss: 8.21938905009517.Train accuracy: 0.9106334841628959. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 112. Train loss: 8.033970700369942.Train accuracy: 0.9128959276018099. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 113. Train loss: 8.217592221719247.Train accuracy: 0.9083710407239819. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 114. Train loss: 8.09931500752767.Train accuracy: 0.9117647058823529. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 115. Train loss: 8.159982222097891.Train accuracy: 0.9106334841628959. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 116. Train loss: 7.879482498875371.Train accuracy: 0.9095022624434389. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 117. Train loss: 8.427307747028491.Train accuracy: 0.915158371040724. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 118. Train loss: 8.239892182526765.Train accuracy: 0.916289592760181. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 119. Train loss: 7.604516629819517.Train accuracy: 0.9117647058823529. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 120. Train loss: 7.8186803482196945.Train accuracy: 0.9128959276018099. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 121. Train loss: 7.990106935854311.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 122. Train loss: 7.8182910195103394.Train accuracy: 0.9128959276018099. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 123. Train loss: 8.335773794739335.Train accuracy: 0.9140271493212669. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 124. Train loss: 8.142332951227823.Train accuracy: 0.9140271493212669. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 125. Train loss: 8.001601448765507.Train accuracy: 0.9106334841628959. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 126. Train loss: 7.685593472586738.Train accuracy: 0.915158371040724. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 127. Train loss: 8.060500551153112.Train accuracy: 0.9128959276018099. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 128. Train loss: 7.809038091588904.Train accuracy: 0.9140271493212669. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 129. Train loss: 7.631491034119217.Train accuracy: 0.9128959276018099. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 130. Train loss: 7.894657558865017.Train accuracy: 0.9140271493212669. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 131. Train loss: 8.10858080122206.Train accuracy: 0.918552036199095. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 132. Train loss: 8.09114161244145.Train accuracy: 0.915158371040724. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 133. Train loss: 8.022254572974312.Train accuracy: 0.9140271493212669. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 134. Train loss: 8.084577719370523.Train accuracy: 0.918552036199095. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 135. Train loss: 7.709720284850509.Train accuracy: 0.9140271493212669. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 136. Train loss: 7.872351257889359.Train accuracy: 0.917420814479638. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 137. Train loss: 7.630655200393112.Train accuracy: 0.916289592760181. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 138. Train loss: 7.467659164358069.Train accuracy: 0.9128959276018099. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 139. Train loss: 7.723926014370388.Train accuracy: 0.9128959276018099. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 140. Train loss: 7.859559359373869.Train accuracy: 0.9128959276018099. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 141. Train loss: 8.060829714492515.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 142. Train loss: 7.654551417739303.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 143. Train loss: 7.6393488336492466.Train accuracy: 0.9140271493212669. Test accuracy: 0.918552036199095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 144. Train loss: 7.831716272566053.Train accuracy: 0.918552036199095. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 145. Train loss: 7.7638558811611595.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 146. Train loss: 7.783398893144396.Train accuracy: 0.917420814479638. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 147. Train loss: 7.863646277674922.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 148. Train loss: 7.309012836880154.Train accuracy: 0.916289592760181. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 149. Train loss: 7.641915294859144.Train accuracy: 0.916289592760181. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 150. Train loss: 7.477460817054466.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 151. Train loss: 7.880555444293552.Train accuracy: 0.916289592760181. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 152. Train loss: 7.641387180045799.Train accuracy: 0.917420814479638. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 153. Train loss: 7.750983379505299.Train accuracy: 0.919683257918552. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 154. Train loss: 7.531385412922612.Train accuracy: 0.9128959276018099. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 155. Train loss: 7.424794956489846.Train accuracy: 0.916289592760181. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 156. Train loss: 7.604128413730198.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 157. Train loss: 7.955509256433557.Train accuracy: 0.9140271493212669. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 158. Train loss: 7.294460473237215.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 159. Train loss: 7.142388493926437.Train accuracy: 0.918552036199095. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 160. Train loss: 7.386765718460083.Train accuracy: 0.917420814479638. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 161. Train loss: 7.132132433078907.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 162. Train loss: 7.539129221880877.Train accuracy: 0.917420814479638. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 163. Train loss: 7.445395875860144.Train accuracy: 0.916289592760181. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 164. Train loss: 7.401703189920496.Train accuracy: 0.916289592760181. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 165. Train loss: 6.94459922225387.Train accuracy: 0.9219457013574661. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 166. Train loss: 7.388043518419619.Train accuracy: 0.917420814479638. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 167. Train loss: 7.63198329784252.Train accuracy: 0.916289592760181. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 168. Train loss: 7.388418815754078.Train accuracy: 0.918552036199095. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 169. Train loss: 7.548559559716119.Train accuracy: 0.919683257918552. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 170. Train loss: 7.324813683827718.Train accuracy: 0.917420814479638. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 171. Train loss: 7.777036384299949.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 172. Train loss: 6.8514586554633246.Train accuracy: 0.919683257918552. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 173. Train loss: 7.306642338081643.Train accuracy: 0.920814479638009. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 174. Train loss: 7.613504383299086.Train accuracy: 0.917420814479638. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 175. Train loss: 7.4079878683443425.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 176. Train loss: 7.578936276612459.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 177. Train loss: 7.952702698884187.Train accuracy: 0.915158371040724. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 178. Train loss: 7.532851643032497.Train accuracy: 0.9219457013574661. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 179. Train loss: 7.265549200552481.Train accuracy: 0.920814479638009. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 180. Train loss: 7.244312719062522.Train accuracy: 0.9253393665158371. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 181. Train loss: 7.400644955811678.Train accuracy: 0.9230769230769231. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 182. Train loss: 7.346154866395174.Train accuracy: 0.9219457013574661. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 183. Train loss: 6.968009259965685.Train accuracy: 0.9230769230769231. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 184. Train loss: 7.062672155874747.Train accuracy: 0.920814479638009. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 185. Train loss: 7.2070216867658825.Train accuracy: 0.9230769230769231. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 186. Train loss: 7.459334046752365.Train accuracy: 0.9242081447963801. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 187. Train loss: 7.192910079602842.Train accuracy: 0.919683257918552. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 188. Train loss: 7.295384521837588.Train accuracy: 0.920814479638009. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 189. Train loss: 7.801292750570509.Train accuracy: 0.9219457013574661. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 190. Train loss: 6.997817851878978.Train accuracy: 0.9264705882352942. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 191. Train loss: 6.976453639842846.Train accuracy: 0.9219457013574661. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 192. Train loss: 7.05980760079843.Train accuracy: 0.9276018099547512. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 193. Train loss: 7.262622073844627.Train accuracy: 0.9287330316742082. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 194. Train loss: 7.046889852594446.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 195. Train loss: 7.394065839272958.Train accuracy: 0.918552036199095. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 196. Train loss: 7.188679898226702.Train accuracy: 0.9219457013574661. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 197. Train loss: 6.918091403113471.Train accuracy: 0.9242081447963801. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 198. Train loss: 6.951341885107535.Train accuracy: 0.9264705882352942. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 199. Train loss: 6.802280889617072.Train accuracy: 0.9276018099547512. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 200. Train loss: 6.8125922061778885.Train accuracy: 0.9264705882352942. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 201. Train loss: 7.125792512187251.Train accuracy: 0.9264705882352942. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 202. Train loss: 7.096347234867237.Train accuracy: 0.9276018099547512. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 203. Train loss: 7.015490019762957.Train accuracy: 0.9242081447963801. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 204. Train loss: 6.8856374687618676.Train accuracy: 0.9253393665158371. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 205. Train loss: 7.229618346249616.Train accuracy: 0.9242081447963801. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 206. Train loss: 7.174317218639232.Train accuracy: 0.9253393665158371. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 207. Train loss: 7.045921846672341.Train accuracy: 0.9287330316742082. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 208. Train loss: 6.82579239209493.Train accuracy: 0.9298642533936652. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 209. Train loss: 6.815247897748594.Train accuracy: 0.9298642533936652. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 210. Train loss: 7.122054921256171.Train accuracy: 0.9264705882352942. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 211. Train loss: 6.865189287397596.Train accuracy: 0.9355203619909502. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 212. Train loss: 6.369480477439033.Train accuracy: 0.9276018099547512. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 213. Train loss: 7.24797374230844.Train accuracy: 0.9321266968325792. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 214. Train loss: 6.616218284324363.Train accuracy: 0.9276018099547512. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 215. Train loss: 6.584264287242183.Train accuracy: 0.9321266968325792. Test accuracy: 0.918552036199095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 216. Train loss: 6.838218013445537.Train accuracy: 0.9332579185520362. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 217. Train loss: 7.044879772044994.Train accuracy: 0.9264705882352942. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 218. Train loss: 7.324964196593673.Train accuracy: 0.9298642533936652. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 219. Train loss: 7.14614615616975.Train accuracy: 0.9253393665158371. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 220. Train loss: 6.755206832179317.Train accuracy: 0.9264705882352942. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 221. Train loss: 6.401606431713811.Train accuracy: 0.9276018099547512. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 222. Train loss: 6.354832852328265.Train accuracy: 0.9276018099547512. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 223. Train loss: 6.73168224758572.Train accuracy: 0.9332579185520362. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 224. Train loss: 6.815730977941443.Train accuracy: 0.9276018099547512. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 225. Train loss: 7.248920016818577.Train accuracy: 0.9264705882352942. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 226. Train loss: 6.8051102956136065.Train accuracy: 0.9287330316742082. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 227. Train loss: 7.268344817338167.Train accuracy: 0.9298642533936652. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 228. Train loss: 6.213158148306388.Train accuracy: 0.9332579185520362. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 229. Train loss: 6.847072513015182.Train accuracy: 0.9298642533936652. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 230. Train loss: 7.0059327019585504.Train accuracy: 0.9309954751131222. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 231. Train loss: 7.018629842334324.Train accuracy: 0.9309954751131222. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 232. Train loss: 6.404341759505095.Train accuracy: 0.9287330316742082. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 233. Train loss: 6.703322909496449.Train accuracy: 0.9321266968325792. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 234. Train loss: 6.498190040941592.Train accuracy: 0.9332579185520362. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 235. Train loss: 6.6162079440222845.Train accuracy: 0.9321266968325792. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 236. Train loss: 6.596667024824354.Train accuracy: 0.9276018099547512. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 237. Train loss: 6.8336469508983475.Train accuracy: 0.9298642533936652. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 238. Train loss: 6.590463011353104.Train accuracy: 0.9355203619909502. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 239. Train loss: 6.475485227726124.Train accuracy: 0.9377828054298643. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 240. Train loss: 6.444641960991754.Train accuracy: 0.9298642533936652. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 241. Train loss: 7.00230426258511.Train accuracy: 0.9321266968325792. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 242. Train loss: 6.725013264903316.Train accuracy: 0.9309954751131222. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 243. Train loss: 6.806034220589532.Train accuracy: 0.9377828054298643. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 244. Train loss: 6.4488376688074185.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 245. Train loss: 5.8620429480517355.Train accuracy: 0.9377828054298643. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 246. Train loss: 6.814792328410679.Train accuracy: 0.9389140271493213. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 247. Train loss: 6.318493639981305.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 248. Train loss: 6.749666176460408.Train accuracy: 0.9377828054298643. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 249. Train loss: 6.612510946061876.Train accuracy: 0.9377828054298643. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 250. Train loss: 6.605116137751827.Train accuracy: 0.9332579185520362. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 251. Train loss: 7.193533032028763.Train accuracy: 0.9366515837104072. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 252. Train loss: 6.06791631380717.Train accuracy: 0.9411764705882353. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 253. Train loss: 6.382504834069146.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 254. Train loss: 6.516721575348465.Train accuracy: 0.9400452488687783. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 255. Train loss: 6.335649552168669.Train accuracy: 0.9366515837104072. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 256. Train loss: 6.515644819648178.Train accuracy: 0.9389140271493213. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 257. Train loss: 6.820426596535577.Train accuracy: 0.9377828054298643. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 258. Train loss: 5.982005136984366.Train accuracy: 0.9389140271493213. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 259. Train loss: 6.663709432990463.Train accuracy: 0.9366515837104072. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 260. Train loss: 6.691034334677237.Train accuracy: 0.9321266968325792. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 261. Train loss: 6.518293513192071.Train accuracy: 0.9355203619909502. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 262. Train loss: 6.814966245933816.Train accuracy: 0.9389140271493213. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 263. Train loss: 6.673643933402167.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 264. Train loss: 6.616124753598814.Train accuracy: 0.9377828054298643. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 265. Train loss: 6.263304984128034.Train accuracy: 0.9377828054298643. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 266. Train loss: 6.338934191951045.Train accuracy: 0.9321266968325792. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 267. Train loss: 6.850068004043014.Train accuracy: 0.9355203619909502. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 268. Train loss: 6.525018038573088.Train accuracy: 0.9389140271493213. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 269. Train loss: 6.387808640797933.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 270. Train loss: 6.5680900503087924.Train accuracy: 0.9366515837104072. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 271. Train loss: 6.562303119235569.Train accuracy: 0.9332579185520362. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 272. Train loss: 6.47033445040385.Train accuracy: 0.9343891402714932. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 273. Train loss: 6.127765982239334.Train accuracy: 0.9355203619909502. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 274. Train loss: 6.351281113094753.Train accuracy: 0.9355203619909502. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 275. Train loss: 6.908925692240397.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 276. Train loss: 6.64008046079565.Train accuracy: 0.9355203619909502. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 277. Train loss: 6.19899418618944.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 278. Train loss: 6.346714443630642.Train accuracy: 0.9366515837104072. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 279. Train loss: 6.466888710304543.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 280. Train loss: 5.977536183816415.Train accuracy: 0.9366515837104072. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 281. Train loss: 6.127259223549454.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 282. Train loss: 5.884734869003296.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 283. Train loss: 6.4608955206694425.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 284. Train loss: 6.010130736562941.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 285. Train loss: 5.654060045878093.Train accuracy: 0.9332579185520362. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 286. Train loss: 6.410477779529713.Train accuracy: 0.9377828054298643. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 287. Train loss: 6.755342373141536.Train accuracy: 0.9423076923076923. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 288. Train loss: 6.58192354661447.Train accuracy: 0.9389140271493213. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 289. Train loss: 6.580384510534781.Train accuracy: 0.9377828054298643. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 290. Train loss: 6.256708842736703.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 291. Train loss: 6.515086483072351.Train accuracy: 0.9411764705882353. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 292. Train loss: 6.094768162126894.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 293. Train loss: 6.701187751911305.Train accuracy: 0.9411764705882353. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 294. Train loss: 6.16094085905287.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 295. Train loss: 6.118492223598339.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 296. Train loss: 6.226013324878834.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 297. Train loss: 6.2796872015352605.Train accuracy: 0.9377828054298643. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 298. Train loss: 6.3587081167432995.Train accuracy: 0.9411764705882353. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 299. Train loss: 6.1361300283008156.Train accuracy: 0.9366515837104072. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 300. Train loss: 6.496694767916644.Train accuracy: 0.9377828054298643. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 301. Train loss: 6.667609126479538.Train accuracy: 0.9400452488687783. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 302. Train loss: 6.112252941838017.Train accuracy: 0.9343891402714932. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 303. Train loss: 6.927307535100867.Train accuracy: 0.9423076923076923. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 304. Train loss: 6.332172742596379.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 305. Train loss: 6.254382265938653.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 306. Train loss: 5.9280548095703125.Train accuracy: 0.9434389140271493. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 307. Train loss: 6.484927813212077.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 308. Train loss: 6.453449973353633.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 309. Train loss: 6.276102551707515.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 310. Train loss: 5.84983084819935.Train accuracy: 0.9434389140271493. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 311. Train loss: 6.326075553894043.Train accuracy: 0.9423076923076923. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 312. Train loss: 5.861442640975669.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 313. Train loss: 5.979773706860012.Train accuracy: 0.9411764705882353. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 314. Train loss: 5.838940638082999.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 315. Train loss: 6.448925936663592.Train accuracy: 0.9389140271493213. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 316. Train loss: 5.775041756806551.Train accuracy: 0.9389140271493213. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 317. Train loss: 6.048562191150807.Train accuracy: 0.9389140271493213. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 318. Train loss: 6.367158898600826.Train accuracy: 0.9366515837104072. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 319. Train loss: 5.858997927771674.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 320. Train loss: 5.667290877412866.Train accuracy: 0.9389140271493213. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 321. Train loss: 6.6507983737521705.Train accuracy: 0.9423076923076923. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 322. Train loss: 6.533531943957011.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 323. Train loss: 6.42506758371989.Train accuracy: 0.9366515837104072. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 324. Train loss: 6.5681625207265215.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 325. Train loss: 6.066640474178173.Train accuracy: 0.9400452488687783. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 326. Train loss: 5.641040722529094.Train accuracy: 0.9400452488687783. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 327. Train loss: 5.966439097015946.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 328. Train loss: 6.19552485148112.Train accuracy: 0.9423076923076923. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 329. Train loss: 6.69171600871616.Train accuracy: 0.9377828054298643. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 330. Train loss: 6.055853967313413.Train accuracy: 0.9400452488687783. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 331. Train loss: 5.811947019011886.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 332. Train loss: 6.55968028527719.Train accuracy: 0.9400452488687783. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 333. Train loss: 6.139506366517809.Train accuracy: 0.9400452488687783. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 334. Train loss: 6.390615984245583.Train accuracy: 0.9377828054298643. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 335. Train loss: 6.217023619899043.Train accuracy: 0.9411764705882353. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 336. Train loss: 6.267300950156318.Train accuracy: 0.9366515837104072. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 337. Train loss: 6.406244684148718.Train accuracy: 0.9366515837104072. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 338. Train loss: 6.0182360543145075.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 339. Train loss: 6.37047513767525.Train accuracy: 0.9400452488687783. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 340. Train loss: 5.980051901605394.Train accuracy: 0.9423076923076923. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 341. Train loss: 5.816328918492353.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 342. Train loss: 6.209334444116663.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 343. Train loss: 5.952657955664176.Train accuracy: 0.9423076923076923. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 344. Train loss: 6.237241276988277.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 345. Train loss: 6.041958376213357.Train accuracy: 0.9468325791855203. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 346. Train loss: 5.96078175968594.Train accuracy: 0.9468325791855203. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 347. Train loss: 5.650295509232415.Train accuracy: 0.9423076923076923. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 348. Train loss: 5.623396935286345.Train accuracy: 0.9377828054298643. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 349. Train loss: 5.645812021361457.Train accuracy: 0.9423076923076923. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 350. Train loss: 5.919369671079847.Train accuracy: 0.9411764705882353. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 351. Train loss: 5.9646302020108255.Train accuracy: 0.9445701357466063. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 352. Train loss: 6.0468271794142545.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 353. Train loss: 5.8860259696289345.Train accuracy: 0.9434389140271493. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 354. Train loss: 6.157680829366048.Train accuracy: 0.9445701357466063. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 355. Train loss: 5.953758831377383.Train accuracy: 0.9445701357466063. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 356. Train loss: 5.700542441120854.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 357. Train loss: 5.817472435809948.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 358. Train loss: 5.661349658612852.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 359. Train loss: 5.693859223966245.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 360. Train loss: 5.966482833579734.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 361. Train loss: 6.021650265764307.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 362. Train loss: 5.885782232990971.Train accuracy: 0.9445701357466063. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 363. Train loss: 5.560402516965513.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 364. Train loss: 5.895897335476345.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 365. Train loss: 5.793952774118494.Train accuracy: 0.9400452488687783. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 366. Train loss: 5.758964971259788.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 367. Train loss: 5.750263373057048.Train accuracy: 0.9400452488687783. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 368. Train loss: 5.82279904683431.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 369. Train loss: 5.831482348618684.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 370. Train loss: 5.938708455474289.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 371. Train loss: 5.681001000934177.Train accuracy: 0.9457013574660633. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 372. Train loss: 5.824013754173562.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 373. Train loss: 5.662566739099997.Train accuracy: 0.9423076923076923. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 374. Train loss: 5.280080450905694.Train accuracy: 0.9445701357466063. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 375. Train loss: 5.741017412256311.Train accuracy: 0.9445701357466063. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 376. Train loss: 5.905674298604329.Train accuracy: 0.9457013574660633. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 377. Train loss: 5.980908190762555.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 378. Train loss: 5.554648138858654.Train accuracy: 0.9400452488687783. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 379. Train loss: 5.342034286922878.Train accuracy: 0.9445701357466063. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 380. Train loss: 5.949139321291888.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 381. Train loss: 5.679911781240393.Train accuracy: 0.9490950226244343. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 382. Train loss: 6.134768962860107.Train accuracy: 0.9445701357466063. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 383. Train loss: 5.967276652654012.Train accuracy: 0.9445701357466063. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 384. Train loss: 5.762093508685076.Train accuracy: 0.9434389140271493. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 385. Train loss: 5.597810506820679.Train accuracy: 0.9457013574660633. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 386. Train loss: 5.762500756316715.Train accuracy: 0.9457013574660633. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 387. Train loss: 5.506873704768993.Train accuracy: 0.9445701357466063. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 388. Train loss: 5.548152689580564.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 389. Train loss: 5.160955795535335.Train accuracy: 0.9434389140271493. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 390. Train loss: 5.377651881288599.Train accuracy: 0.9411764705882353. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 391. Train loss: 5.880208324503015.Train accuracy: 0.9445701357466063. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 392. Train loss: 5.601980721509015.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 393. Train loss: 5.743628771216781.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 394. Train loss: 6.287801636589898.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 395. Train loss: 5.402271579813074.Train accuracy: 0.9445701357466063. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 396. Train loss: 5.601959634710242.Train accuracy: 0.9479638009049773. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 397. Train loss: 5.385555271749143.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 398. Train loss: 5.852456225289239.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 399. Train loss: 5.655374385692455.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 400. Train loss: 6.137078762054443.Train accuracy: 0.9411764705882353. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 401. Train loss: 5.412532241256149.Train accuracy: 0.9445701357466063. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 402. Train loss: 5.674486822552151.Train accuracy: 0.9457013574660633. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 403. Train loss: 5.365209915019848.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 404. Train loss: 5.240582227706909.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 405. Train loss: 5.454587927571049.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 406. Train loss: 5.206617046285559.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 407. Train loss: 5.175237841076321.Train accuracy: 0.9468325791855203. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 408. Train loss: 5.017931810131779.Train accuracy: 0.9457013574660633. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 409. Train loss: 5.366841581132677.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 410. Train loss: 5.363245345928051.Train accuracy: 0.9479638009049773. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 411. Train loss: 5.9102245260168.Train accuracy: 0.9490950226244343. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 412. Train loss: 5.335775423932959.Train accuracy: 0.9457013574660633. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 413. Train loss: 5.900436136457655.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 414. Train loss: 5.305810769399007.Train accuracy: 0.9445701357466063. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 415. Train loss: 5.363109473828916.Train accuracy: 0.9434389140271493. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 416. Train loss: 5.719962526250769.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 417. Train loss: 5.556990614643803.Train accuracy: 0.9434389140271493. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 418. Train loss: 5.300833053059048.Train accuracy: 0.9411764705882353. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 419. Train loss: 5.092459316606875.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 420. Train loss: 5.371217467166759.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 421. Train loss: 5.869270695580377.Train accuracy: 0.9457013574660633. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 422. Train loss: 5.758484385631703.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 423. Train loss: 5.5620213791176125.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 424. Train loss: 5.714160504164519.Train accuracy: 0.9457013574660633. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 425. Train loss: 6.056431399451362.Train accuracy: 0.9479638009049773. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 426. Train loss: 5.724020489939937.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 427. Train loss: 5.937556522863883.Train accuracy: 0.9502262443438914. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 428. Train loss: 5.341774693241826.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 429. Train loss: 4.885866836265281.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 430. Train loss: 6.115523629718357.Train accuracy: 0.9468325791855203. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 431. Train loss: 5.39552922602053.Train accuracy: 0.9457013574660633. Test accuracy: 0.9230769230769231.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 432. Train loss: 6.046310998775341.Train accuracy: 0.9468325791855203. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 433. Train loss: 5.308125487080327.Train accuracy: 0.9502262443438914. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 434. Train loss: 5.499607147993864.Train accuracy: 0.9502262443438914. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 435. Train loss: 5.768909101132993.Train accuracy: 0.9490950226244343. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 436. Train loss: 6.027627507845561.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 437. Train loss: 5.2881281110975475.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 438. Train loss: 5.51219493371469.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 439. Train loss: 5.179479775605379.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 440. Train loss: 5.779037850874442.Train accuracy: 0.9468325791855203. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 441. Train loss: 6.1892143090566.Train accuracy: 0.9445701357466063. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 442. Train loss: 5.3261735174391.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 443. Train loss: 6.081828841456661.Train accuracy: 0.9400452488687783. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 444. Train loss: 6.378589665448224.Train accuracy: 0.9366515837104072. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 445. Train loss: 5.323738786909315.Train accuracy: 0.9434389140271493. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 446. Train loss: 5.662083705266316.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 447. Train loss: 6.1699024792070745.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 448. Train loss: 5.15196681022644.Train accuracy: 0.9468325791855203. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 449. Train loss: 5.413188457489014.Train accuracy: 0.9468325791855203. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 450. Train loss: 6.251913247285066.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 451. Train loss: 5.345600582935192.Train accuracy: 0.9490950226244343. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 452. Train loss: 5.7718990908728705.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 453. Train loss: 5.498733511677495.Train accuracy: 0.9490950226244343. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 454. Train loss: 5.256608495005855.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 455. Train loss: 5.664440828341025.Train accuracy: 0.9490950226244343. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 456. Train loss: 5.552499612172444.Train accuracy: 0.9524886877828054. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 457. Train loss: 5.594835784700182.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 458. Train loss: 5.037075329709936.Train accuracy: 0.9490950226244343. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 459. Train loss: 5.47562305132548.Train accuracy: 0.9502262443438914. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 460. Train loss: 5.2690872086419.Train accuracy: 0.9490950226244343. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 461. Train loss: 5.14020742310418.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 462. Train loss: 5.442633372766.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 463. Train loss: 5.6121465276788784.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 464. Train loss: 5.078071214534618.Train accuracy: 0.9502262443438914. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 465. Train loss: 5.599176424520987.Train accuracy: 0.9445701357466063. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 466. Train loss: 5.492613050672743.Train accuracy: 0.9490950226244343. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 467. Train loss: 5.69957388330389.Train accuracy: 0.9468325791855203. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 468. Train loss: 5.991796754024647.Train accuracy: 0.9502262443438914. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 469. Train loss: 4.9819754273803145.Train accuracy: 0.9502262443438914. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 470. Train loss: 5.138595837133902.Train accuracy: 0.9479638009049773. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 471. Train loss: 6.131675843839292.Train accuracy: 0.9490950226244343. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 472. Train loss: 5.36709839326364.Train accuracy: 0.9434389140271493. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 473. Train loss: 5.368262388088085.Train accuracy: 0.9479638009049773. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 474. Train loss: 5.415154563056098.Train accuracy: 0.9468325791855203. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 475. Train loss: 5.697252829869588.Train accuracy: 0.9513574660633484. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 476. Train loss: 5.629505837405169.Train accuracy: 0.9468325791855203. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 477. Train loss: 5.409087207582262.Train accuracy: 0.9468325791855203. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 478. Train loss: 5.510940781346074.Train accuracy: 0.9445701357466063. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 479. Train loss: 5.564602949001171.Train accuracy: 0.9457013574660633. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 480. Train loss: 5.621364072517112.Train accuracy: 0.9457013574660633. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 481. Train loss: 5.334698480588418.Train accuracy: 0.9457013574660633. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 482. Train loss: 5.902589992240623.Train accuracy: 0.9468325791855203. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 483. Train loss: 5.353491438759698.Train accuracy: 0.9411764705882353. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 484. Train loss: 5.614492498062275.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 485. Train loss: 5.601997936213458.Train accuracy: 0.9434389140271493. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 486. Train loss: 5.972298657452619.Train accuracy: 0.9423076923076923. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 487. Train loss: 4.82694794955077.Train accuracy: 0.9457013574660633. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 488. Train loss: 5.49092580654003.Train accuracy: 0.9502262443438914. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 489. Train loss: 5.213827393673085.Train accuracy: 0.9490950226244343. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 490. Train loss: 5.347337210619891.Train accuracy: 0.9479638009049773. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 491. Train loss: 6.654423192695335.Train accuracy: 0.9457013574660633. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 492. Train loss: 5.73995261280625.Train accuracy: 0.9457013574660633. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 493. Train loss: 5.564160691367255.Train accuracy: 0.9457013574660633. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 494. Train loss: 5.051067970417164.Train accuracy: 0.9490950226244343. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 495. Train loss: 5.433371164180614.Train accuracy: 0.9513574660633484. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 496. Train loss: 5.170360622582613.Train accuracy: 0.9457013574660633. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 497. Train loss: 5.94195204310947.Train accuracy: 0.9411764705882353. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 498. Train loss: 5.607361758196795.Train accuracy: 0.9400452488687783. Test accuracy: 0.9276018099547512.\n",
      "==> Epoch: 499. Train loss: 5.788939228764287.Train accuracy: 0.9468325791855203. Test accuracy: 0.918552036199095.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DFS(N=None, alpha1=0.0, alpha2=0.0, batch_size=32, dropout_rate=0.5,\n",
       "  lambda1=0.0, lambda2=1.0, layers_sizes=[10, 128, 64, 2], num_epochs=500,\n",
       "  verbose=True)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_dfs = dfs.transform(train_X, N=10)\n",
    "test_X_dfs = dfs.transform(test_X, N=10)\n",
    "\n",
    "fdf = DFS(num_epochs=500, verbose=True, dropout_rate=0.5, lambda1=0., alpha1=0.)\n",
    "\n",
    "fdf.fit(train_X_dfs, train_y.y, test_data=(test_X_dfs, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:24:30.276944Z",
     "start_time": "2018-05-01T22:24:30.144061Z"
    }
   },
   "outputs": [],
   "source": [
    "class LassoFS(BaseEstimator):\n",
    "    def __init__(self, N=None, C=1.):\n",
    "        self.N = N\n",
    "        self.est = lm.LogisticRegression(penalty='l1', C=C)\n",
    "        self.C = C\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.est.fit(X, y)\n",
    "        self.features = X.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None):\n",
    "        if N:\n",
    "            features = list(self.select_most_important_ftrs(N))\n",
    "        else:\n",
    "            features = list(self.select_most_important_ftrs(self.N))\n",
    "        \n",
    "        return X[features]\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.est.predict(X)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.est.coef_\n",
    "\n",
    "\n",
    "    def select_most_important_ftrs(self, N):\n",
    "        feature_weight = sorted(zip(self.est.coef_[0], self.features),\n",
    "                                key=lambda x: abs(x[0]))\n",
    "\n",
    "        return list(map(lambda x: x[1], feature_weight[-N:]))\n",
    "    \n",
    "    def select_most_important_ftrs_thresh(self, thresh=0.15):\n",
    "        weights = self.est.coef_[0]\n",
    "        feature_weight = filter(lambda x: x[0] >= thresh,\n",
    "                                zip(weights, self.features))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:24:32.147853Z",
     "start_time": "2018-05-01T22:24:31.715435Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.886877828054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c41acf5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lassofs = LassoFS(C=1);\n",
    "lassofs.fit(train_X, train_y.y);\n",
    "print(mtcs.accuracy_score(test_y, lassofs.predict(test_X)))\n",
    "plt.hist(lassofs.coef_[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T22:25:33.620907Z",
     "start_time": "2018-05-01T22:24:35.871875Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 18.312629770349574.Train accuracy: 0.7262443438914027. Test accuracy: 0.7285067873303167.\n",
      "==> Epoch: 1. Train loss: 15.185972460994014.Train accuracy: 0.8382352941176471. Test accuracy: 0.8733031674208145.\n",
      "==> Epoch: 2. Train loss: 12.545572687078405.Train accuracy: 0.8450226244343891. Test accuracy: 0.8823529411764706.\n",
      "==> Epoch: 3. Train loss: 12.54624926602399.Train accuracy: 0.8563348416289592. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 4. Train loss: 11.895811275199607.Train accuracy: 0.8472850678733032. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 5. Train loss: 12.149624400668674.Train accuracy: 0.8472850678733032. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 6. Train loss: 11.75709961078785.Train accuracy: 0.8529411764705882. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 7. Train loss: 11.692636030691641.Train accuracy: 0.8540723981900452. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 8. Train loss: 11.229083520394784.Train accuracy: 0.8563348416289592. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 9. Train loss: 11.60348747394703.Train accuracy: 0.8619909502262444. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 10. Train loss: 11.175409546604863.Train accuracy: 0.8676470588235294. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 11. Train loss: 11.376989664854827.Train accuracy: 0.8653846153846154. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 12. Train loss: 10.772950825867829.Train accuracy: 0.8687782805429864. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 13. Train loss: 10.926330089569092.Train accuracy: 0.8721719457013575. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 14. Train loss: 10.967940012613932.Train accuracy: 0.8755656108597285. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 15. Train loss: 10.573869581575748.Train accuracy: 0.8744343891402715. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 16. Train loss: 10.815824897200972.Train accuracy: 0.8687782805429864. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 17. Train loss: 10.471691255216244.Train accuracy: 0.8733031674208145. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 18. Train loss: 10.785205275924117.Train accuracy: 0.8608597285067874. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 19. Train loss: 10.398224865948713.Train accuracy: 0.8834841628959276. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 20. Train loss: 10.482855161031088.Train accuracy: 0.8733031674208145. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 21. Train loss: 10.548039577625415.Train accuracy: 0.8687782805429864. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 22. Train loss: 10.034275187386406.Train accuracy: 0.8642533936651584. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 23. Train loss: 10.005784334959808.Train accuracy: 0.8766968325791855. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 24. Train loss: 10.206328727580884.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 25. Train loss: 10.293863561418322.Train accuracy: 0.8687782805429864. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 26. Train loss: 10.441736839435718.Train accuracy: 0.8812217194570136. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 27. Train loss: 9.978501125618264.Train accuracy: 0.8778280542986425. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 28. Train loss: 10.254687485871491.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 29. Train loss: 10.211429013146294.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 30. Train loss: 10.231947739919027.Train accuracy: 0.8812217194570136. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 31. Train loss: 9.989061691142895.Train accuracy: 0.8778280542986425. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 32. Train loss: 9.813674988570037.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 33. Train loss: 10.115066210428873.Train accuracy: 0.8800904977375565. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 34. Train loss: 10.130015593987924.Train accuracy: 0.8846153846153846. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 35. Train loss: 9.880897380687573.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 36. Train loss: 10.330695037488583.Train accuracy: 0.8834841628959276. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 37. Train loss: 9.876853024518049.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 38. Train loss: 10.120247505329273.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 39. Train loss: 9.919517870302554.Train accuracy: 0.8812217194570136. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 40. Train loss: 9.692502251377812.Train accuracy: 0.8823529411764706. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 41. Train loss: 10.029124507197627.Train accuracy: 0.8812217194570136. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 42. Train loss: 9.38032157332809.Train accuracy: 0.8834841628959276. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 43. Train loss: 9.837560618365252.Train accuracy: 0.8846153846153846. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 44. Train loss: 10.083296917102954.Train accuracy: 0.8789592760180995. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 45. Train loss: 9.832044257058037.Train accuracy: 0.8812217194570136. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 46. Train loss: 9.608469821788647.Train accuracy: 0.8789592760180995. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 47. Train loss: 9.602015018463135.Train accuracy: 0.8778280542986425. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 48. Train loss: 9.320079379611546.Train accuracy: 0.8789592760180995. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 49. Train loss: 9.527253804383454.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 50. Train loss: 9.286574416690403.Train accuracy: 0.8823529411764706. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 51. Train loss: 9.570531527201334.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 52. Train loss: 9.704725636376274.Train accuracy: 0.8846153846153846. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 53. Train loss: 9.654104427055076.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 54. Train loss: 9.499158294112593.Train accuracy: 0.8891402714932126. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 55. Train loss: 9.796055599495217.Train accuracy: 0.8880090497737556. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 56. Train loss: 9.315607017940945.Train accuracy: 0.8857466063348416. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 57. Train loss: 9.363009028964573.Train accuracy: 0.8880090497737556. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 58. Train loss: 9.742054674360487.Train accuracy: 0.8868778280542986. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 59. Train loss: 9.046118047502306.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 60. Train loss: 9.34270590322989.Train accuracy: 0.8914027149321267. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 61. Train loss: 9.468996083294904.Train accuracy: 0.8789592760180995. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 62. Train loss: 9.355649630228678.Train accuracy: 0.8936651583710408. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 63. Train loss: 9.476225817645037.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 64. Train loss: 9.51031893270987.Train accuracy: 0.8914027149321267. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 65. Train loss: 9.512302937331024.Train accuracy: 0.8902714932126696. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 66. Train loss: 9.407084694615117.Train accuracy: 0.8936651583710408. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 67. Train loss: 9.49680471420288.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 68. Train loss: 9.178518383591264.Train accuracy: 0.8925339366515838. Test accuracy: 0.8823529411764706.\n",
      "==> Epoch: 69. Train loss: 9.713878631591797.Train accuracy: 0.8857466063348416. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 70. Train loss: 9.130079375372993.Train accuracy: 0.8902714932126696. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 71. Train loss: 9.136559610013608.Train accuracy: 0.8914027149321267. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 72. Train loss: 9.104272383230704.Train accuracy: 0.8857466063348416. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 73. Train loss: 9.215653852180198.Train accuracy: 0.8857466063348416. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 74. Train loss: 9.285649140675863.Train accuracy: 0.8936651583710408. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 75. Train loss: 9.26683470054909.Train accuracy: 0.8959276018099548. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 76. Train loss: 8.702714072333443.Train accuracy: 0.8857466063348416. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 77. Train loss: 9.443929424992314.Train accuracy: 0.8936651583710408. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 78. Train loss: 9.161281047043976.Train accuracy: 0.8914027149321267. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 79. Train loss: 9.36915816201104.Train accuracy: 0.8902714932126696. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 80. Train loss: 9.060346002931949.Train accuracy: 0.8880090497737556. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 81. Train loss: 8.597825173978451.Train accuracy: 0.8891402714932126. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 82. Train loss: 9.140422476662529.Train accuracy: 0.8868778280542986. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 83. Train loss: 9.059880804132533.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 84. Train loss: 9.30675294664171.Train accuracy: 0.8902714932126696. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 85. Train loss: 8.453765524758232.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 86. Train loss: 8.99531411241602.Train accuracy: 0.8959276018099548. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 87. Train loss: 9.050581472891349.Train accuracy: 0.8947963800904978. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 88. Train loss: 9.251890941902444.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 89. Train loss: 8.984318750875968.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 90. Train loss: 8.911861649265996.Train accuracy: 0.8947963800904978. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 91. Train loss: 9.3318594649986.Train accuracy: 0.8868778280542986. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 92. Train loss: 9.36952832893089.Train accuracy: 0.8902714932126696. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 93. Train loss: 9.384597919605396.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 94. Train loss: 8.934279088620785.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 95. Train loss: 8.747491041819254.Train accuracy: 0.8947963800904978. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 96. Train loss: 8.83195443506594.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 97. Train loss: 9.39770159897981.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 98. Train loss: 9.019136746724447.Train accuracy: 0.8880090497737556. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 99. Train loss: 8.845245131739864.Train accuracy: 0.8925339366515838. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 100. Train loss: 9.083200754942718.Train accuracy: 0.8947963800904978. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 101. Train loss: 8.80088014072842.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 102. Train loss: 8.819065588491934.Train accuracy: 0.8993212669683258. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 103. Train loss: 8.676027924926192.Train accuracy: 0.8947963800904978. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 104. Train loss: 8.605199072096083.Train accuracy: 0.8970588235294118. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 105. Train loss: 8.94441706162912.Train accuracy: 0.8947963800904978. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 106. Train loss: 9.050512569921988.Train accuracy: 0.8936651583710408. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 107. Train loss: 8.567318077440616.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 108. Train loss: 9.044762328818992.Train accuracy: 0.8970588235294118. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 109. Train loss: 8.890694953777173.Train accuracy: 0.8947963800904978. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 110. Train loss: 8.703006355850786.Train accuracy: 0.8891402714932126. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 111. Train loss: 8.625645019389966.Train accuracy: 0.8993212669683258. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 112. Train loss: 8.695348774945295.Train accuracy: 0.9004524886877828. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 113. Train loss: 8.71198006912514.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 114. Train loss: 9.138574988753707.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 115. Train loss: 8.887938261032104.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 116. Train loss: 8.941716141170925.Train accuracy: 0.8970588235294118. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 117. Train loss: 8.785936761785436.Train accuracy: 0.8981900452488688. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 118. Train loss: 8.916522714826796.Train accuracy: 0.9015837104072398. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 119. Train loss: 8.801312128702799.Train accuracy: 0.8970588235294118. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 120. Train loss: 8.861154662238228.Train accuracy: 0.8947963800904978. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 121. Train loss: 8.71999587394573.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 122. Train loss: 9.23768280170582.Train accuracy: 0.8981900452488688. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 123. Train loss: 8.725090115158647.Train accuracy: 0.8970588235294118. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 124. Train loss: 8.571842458513048.Train accuracy: 0.9015837104072398. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 125. Train loss: 8.841606979016904.Train accuracy: 0.8981900452488688. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 126. Train loss: 8.5460897286733.Train accuracy: 0.8959276018099548. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 127. Train loss: 8.824637077472827.Train accuracy: 0.8914027149321267. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 128. Train loss: 8.82693867330198.Train accuracy: 0.8993212669683258. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 129. Train loss: 8.481391588846842.Train accuracy: 0.8993212669683258. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 130. Train loss: 8.531084484524197.Train accuracy: 0.9015837104072398. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 131. Train loss: 8.577936596340603.Train accuracy: 0.8936651583710408. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 132. Train loss: 8.501605846263745.Train accuracy: 0.9061085972850679. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 133. Train loss: 8.670992904239231.Train accuracy: 0.8936651583710408. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 134. Train loss: 8.62387235959371.Train accuracy: 0.8914027149321267. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 135. Train loss: 8.239799976348877.Train accuracy: 0.9027149321266968. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 136. Train loss: 8.600519374564842.Train accuracy: 0.9015837104072398. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 137. Train loss: 8.823610923908374.Train accuracy: 0.9004524886877828. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 138. Train loss: 8.81894826889038.Train accuracy: 0.9004524886877828. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 139. Train loss: 8.791299298957542.Train accuracy: 0.9038461538461539. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 140. Train loss: 8.953162617153591.Train accuracy: 0.9015837104072398. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 141. Train loss: 8.604063263645878.Train accuracy: 0.8993212669683258. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 142. Train loss: 8.577836434046427.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 143. Train loss: 8.45153887183578.Train accuracy: 0.9015837104072398. Test accuracy: 0.9049773755656109.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 144. Train loss: 8.78042525715298.Train accuracy: 0.9004524886877828. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 145. Train loss: 8.428861017580386.Train accuracy: 0.9027149321266968. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 146. Train loss: 8.909655809402466.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 147. Train loss: 8.506610799718786.Train accuracy: 0.9061085972850679. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 148. Train loss: 8.554041544596354.Train accuracy: 0.8981900452488688. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 149. Train loss: 8.540800712726734.Train accuracy: 0.8981900452488688. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 150. Train loss: 8.865196272178933.Train accuracy: 0.9027149321266968. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 151. Train loss: 8.625647951055456.Train accuracy: 0.8959276018099548. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 152. Train loss: 9.28069512049357.Train accuracy: 0.8891402714932126. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 153. Train loss: 8.626384346573442.Train accuracy: 0.9027149321266968. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 154. Train loss: 8.578314357333714.Train accuracy: 0.9027149321266968. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 155. Train loss: 8.505753852702954.Train accuracy: 0.8993212669683258. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 156. Train loss: 9.062759752626773.Train accuracy: 0.9015837104072398. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 157. Train loss: 8.780660170095938.Train accuracy: 0.9038461538461539. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 158. Train loss: 8.082680525603118.Train accuracy: 0.9038461538461539. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 159. Train loss: 8.612808298181605.Train accuracy: 0.9027149321266968. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 160. Train loss: 8.28303994072808.Train accuracy: 0.9027149321266968. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 161. Train loss: 8.548060178756714.Train accuracy: 0.8970588235294118. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 162. Train loss: 8.150232085475215.Train accuracy: 0.9038461538461539. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 163. Train loss: 8.363286062523171.Train accuracy: 0.9049773755656109. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 164. Train loss: 8.719352748658922.Train accuracy: 0.9027149321266968. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 165. Train loss: 8.429377193804141.Train accuracy: 0.9049773755656109. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 166. Train loss: 8.165514884171662.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 167. Train loss: 8.429931393376103.Train accuracy: 0.9038461538461539. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 168. Train loss: 7.980837362783927.Train accuracy: 0.9027149321266968. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 169. Train loss: 8.619128218403569.Train accuracy: 0.9061085972850679. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 170. Train loss: 8.485739054503265.Train accuracy: 0.9049773755656109. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 171. Train loss: 8.335109728353995.Train accuracy: 0.9049773755656109. Test accuracy: 0.9230769230769231.\n",
      "==> Epoch: 172. Train loss: 7.845037477987784.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 173. Train loss: 8.376863576747754.Train accuracy: 0.9049773755656109. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 174. Train loss: 8.070343953591806.Train accuracy: 0.9061085972850679. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 175. Train loss: 8.53650238778856.Train accuracy: 0.9004524886877828. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 176. Train loss: 8.76686218932823.Train accuracy: 0.9038461538461539. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 177. Train loss: 8.377524799770779.Train accuracy: 0.9038461538461539. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 178. Train loss: 8.515846252441406.Train accuracy: 0.9027149321266968. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 179. Train loss: 8.539710954383567.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 180. Train loss: 8.602999846140543.Train accuracy: 0.9038461538461539. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 181. Train loss: 8.515204049922803.Train accuracy: 0.9038461538461539. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 182. Train loss: 8.213013295774106.Train accuracy: 0.9038461538461539. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 183. Train loss: 8.357125123341879.Train accuracy: 0.9083710407239819. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 184. Train loss: 8.56876242602313.Train accuracy: 0.9083710407239819. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 185. Train loss: 8.413519311834264.Train accuracy: 0.9038461538461539. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 186. Train loss: 8.538718241232413.Train accuracy: 0.9083710407239819. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 187. Train loss: 8.221201243223968.Train accuracy: 0.9027149321266968. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 188. Train loss: 7.967061378337719.Train accuracy: 0.9083710407239819. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 189. Train loss: 8.252212294825801.Train accuracy: 0.9095022624434389. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 190. Train loss: 8.338918853689123.Train accuracy: 0.9038461538461539. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 191. Train loss: 8.023641339054814.Train accuracy: 0.9083710407239819. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 192. Train loss: 8.08340656315839.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 193. Train loss: 7.928376568688287.Train accuracy: 0.9095022624434389. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 194. Train loss: 7.8701206224936024.Train accuracy: 0.9027149321266968. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 195. Train loss: 8.607166316774157.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 196. Train loss: 8.42728309278135.Train accuracy: 0.9061085972850679. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 197. Train loss: 8.675665078339753.Train accuracy: 0.9027149321266968. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 198. Train loss: 7.9278681013319225.Train accuracy: 0.9061085972850679. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 199. Train loss: 8.0686097851506.Train accuracy: 0.9027149321266968. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 200. Train loss: 8.061227939746997.Train accuracy: 0.9038461538461539. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 201. Train loss: 8.393767630612409.Train accuracy: 0.9072398190045249. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 202. Train loss: 8.396985504362318.Train accuracy: 0.9117647058823529. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 203. Train loss: 8.79781139338458.Train accuracy: 0.9027149321266968. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 204. Train loss: 8.266134262084961.Train accuracy: 0.8981900452488688. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 205. Train loss: 7.830430127956249.Train accuracy: 0.9038461538461539. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 206. Train loss: 8.34920831962868.Train accuracy: 0.9061085972850679. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 207. Train loss: 8.458570074152064.Train accuracy: 0.9015837104072398. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 208. Train loss: 8.607780200463754.Train accuracy: 0.9027149321266968. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 209. Train loss: 8.853077482294154.Train accuracy: 0.9049773755656109. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 210. Train loss: 8.047071951406974.Train accuracy: 0.9027149321266968. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 211. Train loss: 8.163884268866646.Train accuracy: 0.9083710407239819. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 212. Train loss: 8.499234005256936.Train accuracy: 0.9061085972850679. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 213. Train loss: 8.825095415115356.Train accuracy: 0.9061085972850679. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 214. Train loss: 8.128418834121138.Train accuracy: 0.9083710407239819. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 215. Train loss: 7.7885337494037765.Train accuracy: 0.9106334841628959. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 216. Train loss: 8.476712456455937.Train accuracy: 0.9049773755656109. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 217. Train loss: 8.335472760377106.Train accuracy: 0.9117647058823529. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 218. Train loss: 8.52096274163988.Train accuracy: 0.9049773755656109. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 219. Train loss: 8.169001579284668.Train accuracy: 0.9083710407239819. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 220. Train loss: 8.006993523350468.Train accuracy: 0.9095022624434389. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 221. Train loss: 7.99824873606364.Train accuracy: 0.9061085972850679. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 222. Train loss: 8.098581631978353.Train accuracy: 0.9038461538461539. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 223. Train loss: 7.8813466584240945.Train accuracy: 0.9049773755656109. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 224. Train loss: 8.331896631805986.Train accuracy: 0.9061085972850679. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 225. Train loss: 8.173341592152914.Train accuracy: 0.9083710407239819. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 226. Train loss: 8.051083229206226.Train accuracy: 0.9072398190045249. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 227. Train loss: 8.32494560877482.Train accuracy: 0.9049773755656109. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 228. Train loss: 8.015201444979068.Train accuracy: 0.9004524886877828. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 229. Train loss: 7.81814483360008.Train accuracy: 0.9106334841628959. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 230. Train loss: 8.099550529762551.Train accuracy: 0.9083710407239819. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 231. Train loss: 8.418405603479457.Train accuracy: 0.9083710407239819. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 232. Train loss: 7.721369822820027.Train accuracy: 0.9106334841628959. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 233. Train loss: 8.011390368143717.Train accuracy: 0.9072398190045249. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 234. Train loss: 8.103384750860709.Train accuracy: 0.9106334841628959. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 235. Train loss: 8.026726475468388.Train accuracy: 0.9106334841628959. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 236. Train loss: 8.03293192828143.Train accuracy: 0.9106334841628959. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 237. Train loss: 8.07257514529758.Train accuracy: 0.9117647058823529. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 238. Train loss: 8.19481611251831.Train accuracy: 0.9049773755656109. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 239. Train loss: 7.97099494934082.Train accuracy: 0.9106334841628959. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 240. Train loss: 7.985683352858932.Train accuracy: 0.9117647058823529. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 241. Train loss: 7.83503559783653.Train accuracy: 0.9015837104072398. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 242. Train loss: 7.8807666743243185.Train accuracy: 0.9038461538461539. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 243. Train loss: 7.712280679632117.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 244. Train loss: 7.979105137012623.Train accuracy: 0.9106334841628959. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 245. Train loss: 7.966712130440606.Train accuracy: 0.9140271493212669. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 246. Train loss: 8.101239946153429.Train accuracy: 0.9106334841628959. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 247. Train loss: 7.752429202750877.Train accuracy: 0.9049773755656109. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 248. Train loss: 8.457942918494895.Train accuracy: 0.9072398190045249. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 249. Train loss: 7.944996021412037.Train accuracy: 0.9083710407239819. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 250. Train loss: 7.801960450631601.Train accuracy: 0.9117647058823529. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 251. Train loss: 8.01006547609965.Train accuracy: 0.9117647058823529. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 252. Train loss: 8.061434127666333.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 253. Train loss: 7.733881862075241.Train accuracy: 0.9083710407239819. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 254. Train loss: 8.530396417335227.Train accuracy: 0.9106334841628959. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 255. Train loss: 7.845719637694182.Train accuracy: 0.9128959276018099. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 256. Train loss: 8.146549048247161.Train accuracy: 0.9061085972850679. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 257. Train loss: 7.670933529182717.Train accuracy: 0.9072398190045249. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 258. Train loss: 8.142083706679168.Train accuracy: 0.9128959276018099. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 259. Train loss: 7.79535111674556.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 260. Train loss: 7.772817346784803.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 261. Train loss: 7.706333054436578.Train accuracy: 0.9117647058823529. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 262. Train loss: 7.6810906021683305.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 263. Train loss: 8.17036881270232.Train accuracy: 0.915158371040724. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 264. Train loss: 7.593614419301351.Train accuracy: 0.9106334841628959. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 265. Train loss: 7.5858583362014205.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 266. Train loss: 7.897027881057174.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 267. Train loss: 7.53690160645379.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 268. Train loss: 8.559093704930058.Train accuracy: 0.9083710407239819. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 269. Train loss: 7.918302536010742.Train accuracy: 0.9061085972850679. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 270. Train loss: 8.34200898806254.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 271. Train loss: 8.107860547524911.Train accuracy: 0.9027149321266968. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 272. Train loss: 7.8674124876658125.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 273. Train loss: 7.803682751125759.Train accuracy: 0.9140271493212669. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 274. Train loss: 8.269620877725107.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 275. Train loss: 7.438172216768618.Train accuracy: 0.9061085972850679. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 276. Train loss: 7.454287723258689.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 277. Train loss: 8.422000734894365.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 278. Train loss: 8.232985311084324.Train accuracy: 0.9117647058823529. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 279. Train loss: 7.7381626588326915.Train accuracy: 0.9140271493212669. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 280. Train loss: 8.033576020488033.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 281. Train loss: 8.127210828993055.Train accuracy: 0.916289592760181. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 282. Train loss: 7.623238581198233.Train accuracy: 0.915158371040724. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 283. Train loss: 7.978783642804181.Train accuracy: 0.9117647058823529. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 284. Train loss: 7.691906416857684.Train accuracy: 0.9061085972850679. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 285. Train loss: 7.525540758062292.Train accuracy: 0.9117647058823529. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 286. Train loss: 8.091423555656716.Train accuracy: 0.9095022624434389. Test accuracy: 0.8823529411764706.\n",
      "==> Epoch: 287. Train loss: 8.063691951610425.Train accuracy: 0.9117647058823529. Test accuracy: 0.8868778280542986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 288. Train loss: 7.952561943619339.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 289. Train loss: 7.977580220611007.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 290. Train loss: 7.714987684179236.Train accuracy: 0.9128959276018099. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 291. Train loss: 7.8206715318891735.Train accuracy: 0.9106334841628959. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 292. Train loss: 7.7673275382430464.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 293. Train loss: 7.9600338229426635.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 294. Train loss: 7.696505113884255.Train accuracy: 0.9117647058823529. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 295. Train loss: 7.801566353550664.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 296. Train loss: 8.495604002917254.Train accuracy: 0.9117647058823529. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 297. Train loss: 7.859639397373906.Train accuracy: 0.9117647058823529. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 298. Train loss: 7.781755800600405.Train accuracy: 0.9106334841628959. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 299. Train loss: 7.962858200073242.Train accuracy: 0.9106334841628959. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 300. Train loss: 7.772373861736721.Train accuracy: 0.9117647058823529. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 301. Train loss: 7.860461800186722.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 302. Train loss: 7.907547897762722.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 303. Train loss: 7.929396134835702.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 304. Train loss: 7.438191881886235.Train accuracy: 0.9117647058823529. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 305. Train loss: 7.533918530852707.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 306. Train loss: 8.290878613789877.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 307. Train loss: 7.6248984425156205.Train accuracy: 0.9072398190045249. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 308. Train loss: 7.798181842874597.Train accuracy: 0.9117647058823529. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 309. Train loss: 7.7053157665111405.Train accuracy: 0.9106334841628959. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 310. Train loss: 7.595265741701479.Train accuracy: 0.9128959276018099. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 311. Train loss: 7.874704590550175.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 312. Train loss: 7.495172182718913.Train accuracy: 0.9117647058823529. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 313. Train loss: 7.768051553655554.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 314. Train loss: 8.147322654724121.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 315. Train loss: 7.675835194411101.Train accuracy: 0.916289592760181. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 316. Train loss: 7.902031916159171.Train accuracy: 0.9140271493212669. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 317. Train loss: 7.741357472207811.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 318. Train loss: 7.310524110440855.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 319. Train loss: 7.398871483626189.Train accuracy: 0.9140271493212669. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 320. Train loss: 7.984095025945593.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 321. Train loss: 7.793119236275002.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 322. Train loss: 8.043457075401589.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 323. Train loss: 7.584546795597783.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 324. Train loss: 8.182268195682102.Train accuracy: 0.9095022624434389. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 325. Train loss: 7.657901781576651.Train accuracy: 0.9117647058823529. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 326. Train loss: 7.436183788158275.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 327. Train loss: 7.65493306407222.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 328. Train loss: 7.647577709621853.Train accuracy: 0.9117647058823529. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 329. Train loss: 7.618276057419954.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 330. Train loss: 7.383522819589685.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 331. Train loss: 7.526687410142687.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 332. Train loss: 7.125159881733082.Train accuracy: 0.916289592760181. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 333. Train loss: 7.273563932489465.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 334. Train loss: 7.981098272182323.Train accuracy: 0.9140271493212669. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 335. Train loss: 7.734633604685466.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 336. Train loss: 7.73299158944024.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 337. Train loss: 7.907213829181813.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 338. Train loss: 7.3320059114032325.Train accuracy: 0.9095022624434389. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 339. Train loss: 7.78127525029359.Train accuracy: 0.9128959276018099. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 340. Train loss: 7.695606461277714.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 341. Train loss: 7.224121861987644.Train accuracy: 0.9117647058823529. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 342. Train loss: 7.292184935675727.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 343. Train loss: 8.076455575448495.Train accuracy: 0.9117647058823529. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 344. Train loss: 7.5658984714084205.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 345. Train loss: 7.818569624865496.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 346. Train loss: 6.777877154173674.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 347. Train loss: 7.491347842746311.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 348. Train loss: 7.692881124990958.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 349. Train loss: 7.0850714930781615.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 350. Train loss: 7.276055371319806.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 351. Train loss: 7.489041593339708.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 352. Train loss: 7.865246834578337.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 353. Train loss: 7.4210085074106855.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 354. Train loss: 7.410798487839876.Train accuracy: 0.917420814479638. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 355. Train loss: 7.117024174442998.Train accuracy: 0.9117647058823529. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 356. Train loss: 7.899073803866351.Train accuracy: 0.9128959276018099. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 357. Train loss: 7.796754298386751.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 358. Train loss: 7.796233592209993.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 359. Train loss: 7.633778739858557.Train accuracy: 0.9140271493212669. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 360. Train loss: 7.6393320913668035.Train accuracy: 0.9117647058823529. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 361. Train loss: 8.109697015197188.Train accuracy: 0.9140271493212669. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 362. Train loss: 7.727642677448414.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 363. Train loss: 7.2749607739625155.Train accuracy: 0.917420814479638. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 364. Train loss: 7.302373426931876.Train accuracy: 0.9128959276018099. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 365. Train loss: 7.941387167683354.Train accuracy: 0.915158371040724. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 366. Train loss: 7.107550126534921.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 367. Train loss: 7.834241496192084.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 368. Train loss: 7.580321903581972.Train accuracy: 0.915158371040724. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 369. Train loss: 7.502304006505896.Train accuracy: 0.915158371040724. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 370. Train loss: 6.973293401576854.Train accuracy: 0.917420814479638. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 371. Train loss: 7.725032762244895.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 372. Train loss: 7.45955068093759.Train accuracy: 0.919683257918552. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 373. Train loss: 7.423409691563359.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 374. Train loss: 7.077235133559616.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 375. Train loss: 7.630565802256267.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 376. Train loss: 7.277175196894893.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 377. Train loss: 7.459575794361256.Train accuracy: 0.9140271493212669. Test accuracy: 0.8868778280542986.\n",
      "==> Epoch: 378. Train loss: 7.14871738575123.Train accuracy: 0.918552036199095. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 379. Train loss: 7.498870955573188.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 380. Train loss: 8.181558891578957.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 381. Train loss: 7.747641377978855.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 382. Train loss: 6.922454056916414.Train accuracy: 0.918552036199095. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 383. Train loss: 7.437403405154193.Train accuracy: 0.918552036199095. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 384. Train loss: 7.347968154483372.Train accuracy: 0.920814479638009. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 385. Train loss: 7.507322479177405.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 386. Train loss: 7.4507439224808305.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 387. Train loss: 7.5963385634952125.Train accuracy: 0.916289592760181. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 388. Train loss: 7.3441413508521185.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 389. Train loss: 7.650572838606657.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 390. Train loss: 7.258915185928345.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 391. Train loss: 7.300101721728289.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 392. Train loss: 7.305739367449725.Train accuracy: 0.918552036199095. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 393. Train loss: 6.950509874909012.Train accuracy: 0.918552036199095. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 394. Train loss: 7.821665684382121.Train accuracy: 0.915158371040724. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 395. Train loss: 7.485468272809629.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 396. Train loss: 7.302954470669782.Train accuracy: 0.919683257918552. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 397. Train loss: 7.289077970716688.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 398. Train loss: 7.130026790830824.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 399. Train loss: 7.019082599216038.Train accuracy: 0.918552036199095. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 400. Train loss: 7.519910671092846.Train accuracy: 0.9140271493212669. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 401. Train loss: 7.713554682555022.Train accuracy: 0.9128959276018099. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 402. Train loss: 7.70535401944761.Train accuracy: 0.917420814479638. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 403. Train loss: 7.641602533834952.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 404. Train loss: 7.66577672958374.Train accuracy: 0.9140271493212669. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 405. Train loss: 7.299653477138943.Train accuracy: 0.915158371040724. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 406. Train loss: 7.543372383824101.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 407. Train loss: 6.8080087326191085.Train accuracy: 0.9140271493212669. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 408. Train loss: 7.407815632996736.Train accuracy: 0.917420814479638. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 409. Train loss: 7.776122269807039.Train accuracy: 0.915158371040724. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 410. Train loss: 7.195449943895693.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 411. Train loss: 7.639437004371926.Train accuracy: 0.915158371040724. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 412. Train loss: 7.524127810089676.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 413. Train loss: 7.263400439862852.Train accuracy: 0.919683257918552. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 414. Train loss: 7.3318741144957364.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 415. Train loss: 7.556653746852168.Train accuracy: 0.919683257918552. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 416. Train loss: 7.156667612217091.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 417. Train loss: 7.135132613005461.Train accuracy: 0.9219457013574661. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 418. Train loss: 7.726266260500307.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 419. Train loss: 7.239458799362183.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 420. Train loss: 7.534389434037386.Train accuracy: 0.920814479638009. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 421. Train loss: 7.47304301791721.Train accuracy: 0.920814479638009. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 422. Train loss: 7.124910328123304.Train accuracy: 0.918552036199095. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 423. Train loss: 7.510900188375403.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 424. Train loss: 7.562800027706005.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 425. Train loss: 7.528700201599686.Train accuracy: 0.916289592760181. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 426. Train loss: 7.622354472125018.Train accuracy: 0.9140271493212669. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 427. Train loss: 7.43981569784659.Train accuracy: 0.919683257918552. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 428. Train loss: 7.441354636792783.Train accuracy: 0.918552036199095. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 429. Train loss: 7.330269778216326.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 430. Train loss: 7.651323901282416.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 431. Train loss: 7.381920108088741.Train accuracy: 0.919683257918552. Test accuracy: 0.9140271493212669.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 432. Train loss: 7.320306045037729.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 433. Train loss: 7.527413094485247.Train accuracy: 0.916289592760181. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 434. Train loss: 7.280081590016683.Train accuracy: 0.9230769230769231. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 435. Train loss: 7.38938848177592.Train accuracy: 0.916289592760181. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 436. Train loss: 7.635342509658249.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 437. Train loss: 7.173439374676457.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 438. Train loss: 7.68285663039596.Train accuracy: 0.920814479638009. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 439. Train loss: 7.084793161462854.Train accuracy: 0.918552036199095. Test accuracy: 0.918552036199095.\n",
      "==> Epoch: 440. Train loss: 7.4403242888274015.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 441. Train loss: 7.223088308616921.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 442. Train loss: 7.128515711537114.Train accuracy: 0.918552036199095. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 443. Train loss: 7.095400580653438.Train accuracy: 0.918552036199095. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 444. Train loss: 7.537178481066668.Train accuracy: 0.917420814479638. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 445. Train loss: 7.414181638647009.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 446. Train loss: 7.536322885089451.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 447. Train loss: 7.9747167251728195.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 448. Train loss: 7.2149529810305.Train accuracy: 0.916289592760181. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 449. Train loss: 7.739777035183376.Train accuracy: 0.915158371040724. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 450. Train loss: 7.274536026848687.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 451. Train loss: 7.585226147263138.Train accuracy: 0.919683257918552. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 452. Train loss: 7.314085708724128.Train accuracy: 0.9230769230769231. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 453. Train loss: 7.322326889744511.Train accuracy: 0.919683257918552. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 454. Train loss: 7.481790171729194.Train accuracy: 0.9219457013574661. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 455. Train loss: 7.279294738063106.Train accuracy: 0.9253393665158371. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 456. Train loss: 7.5472158767558915.Train accuracy: 0.918552036199095. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 457. Train loss: 7.335835227259883.Train accuracy: 0.920814479638009. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 458. Train loss: 7.71284356823674.Train accuracy: 0.920814479638009. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 459. Train loss: 7.280158016416761.Train accuracy: 0.918552036199095. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 460. Train loss: 7.193647216867517.Train accuracy: 0.919683257918552. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 461. Train loss: 6.992672973208958.Train accuracy: 0.9230769230769231. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 462. Train loss: 7.175300386216906.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 463. Train loss: 7.119897268436573.Train accuracy: 0.917420814479638. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 464. Train loss: 6.90581923060947.Train accuracy: 0.916289592760181. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 465. Train loss: 7.292469413192184.Train accuracy: 0.9242081447963801. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 466. Train loss: 7.296298088850798.Train accuracy: 0.9242081447963801. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 467. Train loss: 7.324777382391471.Train accuracy: 0.9219457013574661. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 468. Train loss: 6.978781603000782.Train accuracy: 0.920814479638009. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 469. Train loss: 7.014711706726639.Train accuracy: 0.9253393665158371. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 470. Train loss: 7.248649773774324.Train accuracy: 0.915158371040724. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 471. Train loss: 7.5026951012788.Train accuracy: 0.9230769230769231. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 472. Train loss: 7.179663887730351.Train accuracy: 0.9219457013574661. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 473. Train loss: 7.2990578810373945.Train accuracy: 0.9219457013574661. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 474. Train loss: 7.277377985141896.Train accuracy: 0.9242081447963801. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 475. Train loss: 7.424429822851111.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 476. Train loss: 7.084136927569354.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 477. Train loss: 7.216480255126953.Train accuracy: 0.9230769230769231. Test accuracy: 0.8914027149321267.\n",
      "==> Epoch: 478. Train loss: 7.269664826216521.Train accuracy: 0.9230769230769231. Test accuracy: 0.9140271493212669.\n",
      "==> Epoch: 479. Train loss: 7.13887619972229.Train accuracy: 0.920814479638009. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 480. Train loss: 7.26636877766362.Train accuracy: 0.9230769230769231. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 481. Train loss: 7.356618960698445.Train accuracy: 0.9230769230769231. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 482. Train loss: 7.620900030489321.Train accuracy: 0.9230769230769231. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 483. Train loss: 7.543023321363661.Train accuracy: 0.9253393665158371. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 484. Train loss: 7.152332076319942.Train accuracy: 0.919683257918552. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 485. Train loss: 6.738443710185863.Train accuracy: 0.9242081447963801. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 486. Train loss: 7.484110479001646.Train accuracy: 0.9242081447963801. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 487. Train loss: 6.834694085297762.Train accuracy: 0.9264705882352942. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 488. Train loss: 7.212656025533323.Train accuracy: 0.9242081447963801. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 489. Train loss: 7.0902876147517455.Train accuracy: 0.920814479638009. Test accuracy: 0.9004524886877828.\n",
      "==> Epoch: 490. Train loss: 6.8192821343739825.Train accuracy: 0.9219457013574661. Test accuracy: 0.8959276018099548.\n",
      "==> Epoch: 491. Train loss: 6.958865563074748.Train accuracy: 0.9242081447963801. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 492. Train loss: 7.502407144617151.Train accuracy: 0.9230769230769231. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 493. Train loss: 7.000977878217344.Train accuracy: 0.9230769230769231. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 494. Train loss: 7.739568577872382.Train accuracy: 0.9264705882352942. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 495. Train loss: 7.107787229396679.Train accuracy: 0.920814479638009. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 496. Train loss: 7.308391111868399.Train accuracy: 0.9264705882352942. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 497. Train loss: 6.959602514902751.Train accuracy: 0.9287330316742082. Test accuracy: 0.9049773755656109.\n",
      "==> Epoch: 498. Train loss: 7.05733557983681.Train accuracy: 0.9230769230769231. Test accuracy: 0.9095022624434389.\n",
      "==> Epoch: 499. Train loss: 7.195623556772868.Train accuracy: 0.9264705882352942. Test accuracy: 0.9095022624434389.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DFS(N=None, alpha1=0.0, alpha2=0.0, batch_size=32, dropout_rate=0.5,\n",
       "  lambda1=0.0, lambda2=1.0, layers_sizes=[10, 128, 64, 2], num_epochs=500,\n",
       "  verbose=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X_lasso = lassofs.transform(train_X, N=10)\n",
    "test_X_lasso = lassofs.transform(test_X, N=10)\n",
    "\n",
    "fdf = DFS(num_epochs=500, verbose=True, dropout_rate=0.5, lambda1=0., alpha1=0.)\n",
    "\n",
    "fdf.fit(train_X_lasso, train_y.y, test_data=(test_X_lasso, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T20:33:03.116408Z",
     "start_time": "2018-05-01T20:33:02.973851Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForestFS(BaseEstimator):\n",
    "    def __init__(self, N=None, max_depth=None):\n",
    "        self.N = N\n",
    "        self.max_depth = max_depth\n",
    "        self.est = ens.RandomForestClassifier(max_features=None)\n",
    "\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.est.fit(X, y)\n",
    "        self.features = X.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None):\n",
    "        if N:\n",
    "            features = self.select_most_important_ftrs(N)\n",
    "        else:\n",
    "            features = self.select_most_important_ftrs(self.N)\n",
    "            \n",
    "        return X[features]\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.est.predict(X)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.est.feature_importances_\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs(self, N):\n",
    "        feature_weight = sorted( zip(self.est.feature_importances_,\n",
    "                                     self.features),\n",
    "                                 key=lambda x: x[0])\n",
    "        return list(map(lambda x: x[1], feature_weight[-N:]))\n",
    "\n",
    "    \n",
    "    def select_most_important_ftrs_thresh(self, thresh=0.01):\n",
    "        weights = self.est.feature_importances_\n",
    "        feature_weight = filter(lambda x: x[0] >= thresh,\n",
    "                                zip(weights, self.features))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T21:16:26.446971Z",
     "start_time": "2018-05-01T21:16:25.266131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.900452488688\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0c44eb4cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf = RandomForestFS();\n",
    "rf.fit(train_X, train_y);\n",
    "print(mtcs.accuracy_score(test_y, rf.predict(test_X)))\n",
    "plt.hist(rf.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T11:13:51.343741Z",
     "start_time": "2018-05-01T11:13:51.119748Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = ens.RandomForestClassifier()\n",
    "\n",
    "train_X_dfs = dfs.transform(train_X, 500)\n",
    "test_X_dfs = dfs.transform(test_X, 500)\n",
    "\n",
    "est.fit(train_X_dfs, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T11:13:51.855422Z",
     "start_time": "2018-05-01T11:13:51.842145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85520361990950222"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:30:31.617460Z",
     "start_time": "2018-04-30T16:30:31.607461Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8637867739884362, 0.062351653820433672)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztest(acc_p, acc_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def select_most_important_ftrs(est, features, N):\n",
    "    feature_weight = sorted(zip(est.coef_[0], features),\n",
    "                            key=lambda x: abs(x[0]))\n",
    "    \n",
    "    return list(map(lambda x: x[1], feature_weight[-N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:50:43.173156Z",
     "start_time": "2018-04-30T16:49:42.223366Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=40, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=False, subsample=1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = XGBClassifier(n_estimators=1000, n_jobs=40, silent=False)\n",
    "\n",
    "est.fit(train_X_dfs, train_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:46:40.858161Z",
     "start_time": "2018-04-30T16:46:40.800863Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8876811594202898"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:46:44.087000Z",
     "start_time": "2018-04-30T16:46:44.049767Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:04:20.618628Z",
     "start_time": "2018-05-02T00:04:20.612581Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES = [5, 10, 15, 20, 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:57:49.229411Z",
     "start_time": "2018-05-01T23:57:49.188917Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(main_model, feature_selector, param_grid, \n",
    "             X, y, scoring='accuracy', caching=False):\n",
    "\n",
    "    if main_model is not None:\n",
    "        pipeline_list = [('select_ftrs', feature_selector),\n",
    "                         ('main', main_model)]\n",
    "    else:\n",
    "        pipeline_list = [('select_ftrs', feature_selector)]\n",
    "\n",
    "    if caching:\n",
    "        pipeline = Pipeline(pipeline_list, memory=mkdtemp())\n",
    "    else:\n",
    "        pipeline = Pipeline(pipeline_list)\n",
    "    \n",
    "    grid = ms.GridSearchCV(pipeline, cv=ms.StratifiedKFold(n_splits=5,\n",
    "                                                           shuffle=True),\n",
    "                           n_jobs=1, param_grid=param_grid,\n",
    "                           verbose=100, scoring=scoring)\n",
    "    print('=====', X.shape)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T23:59:06.393586Z",
     "start_time": "2018-05-01T23:59:06.388419Z"
    }
   },
   "outputs": [],
   "source": [
    "all_scores = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T19:22:12.451562Z",
     "start_time": "2018-05-01T19:22:05.781589Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] select_ftrs__N=5, select_ftrs__lambda1=0.0 ......................\n",
      "[CV]  select_ftrs__N=5, select_ftrs__lambda1=0.0, score=0.7642276422764228, total=   2.3s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.3s remaining:    0.0s\n",
      "[CV] select_ftrs__N=5, select_ftrs__lambda1=0.0 ......................\n",
      "[CV]  select_ftrs__N=5, select_ftrs__lambda1=0.0, score=0.8125, total=   2.1s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    4.5s remaining:    0.0s\n",
      "[CV] select_ftrs__N=5, select_ftrs__lambda1=0.0 ......................\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-642-ef4e19b47f6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m all_scores['DFS + RF'] = evaluate(ens.RandomForestClassifier(),\n\u001b[0;32m---> 11\u001b[0;31m                                   DFS(), param_grid, data_X, data_y.y)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-640-986e6c74992c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(main_model, feature_selector, param_grid, X, y, scoring, caching)\u001b[0m\n\u001b[1;32m     16\u001b[0m                            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                            verbose=100, scoring=scoring)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \"\"\"\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[1;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                     **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m     \u001b[0;31m# if we have a weight for this transformer, multiply output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-617-e22e5ad429d5>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, test_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                feed_dict = {self.x: batch_X,\n\u001b[1;32m     33\u001b[0m                                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                                             self.dropout_rate_ph: self.dropout_rate})\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### DFS + RandomForest #####\n",
    "param_grid = [\n",
    "    {\n",
    "     'select_ftrs__N': NUM_FEATURES,\n",
    "     'select_ftrs__lambda1': np.arange(0, 0.05, 0.025)\n",
    "#      'main__n_estimators': [10, 25, 50]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['DFS + RF'] = evaluate(ens.RandomForestClassifier(),\n",
    "                                  DFS(), param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T15:22:29.387512Z",
     "start_time": "2018-05-01T15:18:32.304784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 22 candidates, totalling 66 fits\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0, score=0.8672086720867209, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0, score=0.8505434782608695, total=   2.5s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.0, score=0.8070652173913043, total=   2.5s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    7.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025, score=0.8861788617886179, total=   2.5s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   10.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025, score=0.8478260869565217, total=   2.4s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   12.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=10, select_ftrs__lambda1=0.025, score=0.845108695652174, total=   3.8s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   16.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0, score=0.8807588075880759, total=   2.7s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   19.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0, score=0.8668478260869565, total=   2.5s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   21.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.0, score=0.845108695652174, total=   2.7s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   24.4s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025, score=0.8861788617886179, total=   2.5s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   27.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025, score=0.875, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   29.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=25, select_ftrs__lambda1=0.025, score=0.8858695652173914, total=   2.4s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   32.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0, score=0.8672086720867209, total=   2.8s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   34.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0, score=0.8478260869565217, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   37.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.0, score=0.8641304347826086, total=   4.0s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   41.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025, score=0.8861788617886179, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   44.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025, score=0.875, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   46.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=50, select_ftrs__lambda1=0.025, score=0.8831521739130435, total=   2.8s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   49.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0, score=0.8807588075880759, total=   2.8s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   52.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0, score=0.8695652173913043, total=   2.6s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   55.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.0, score=0.8668478260869565, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   57.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025, score=0.8888888888888888, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025, score=0.8722826086956522, total=   4.1s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=75, select_ftrs__lambda1=0.025, score=0.8559782608695652, total=   2.7s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0, score=0.8915989159891599, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0, score=0.875, total=   2.8s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.0, score=0.8532608695652174, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025, score=0.907859078590786, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025, score=0.875, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=100, select_ftrs__lambda1=0.025, score=0.8831521739130435, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0, score=0.8943089430894309, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0, score=0.8586956521739131, total=   4.5s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.0, score=0.8804347826086957, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025, score=0.9051490514905149, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025, score=0.8641304347826086, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=125, select_ftrs__lambda1=0.025, score=0.8804347826086957, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0, score=0.8970189701897019, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0, score=0.8695652173913043, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.0, score=0.8858695652173914, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:  1.9min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025, score=0.8915989159891599, total=   4.5s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025, score=0.8722826086956522, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=150, select_ftrs__lambda1=0.025, score=0.8940217391304348, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0, score=0.8807588075880759, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0, score=0.8777173913043478, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.0, score=0.8831521739130435, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025, score=0.9105691056910569, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025, score=0.8668478260869565, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=175, select_ftrs__lambda1=0.025, score=0.8994565217391305, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0, score=0.8943089430894309, total=   4.7s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0, score=0.875, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.0, score=0.8858695652173914, total=   3.0s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025, score=0.8943089430894309, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025, score=0.8614130434782609, total=   3.3s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=200, select_ftrs__lambda1=0.025, score=0.8641304347826086, total=   3.3s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:  2.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0, score=0.8997289972899729, total=   3.1s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0, score=0.8586956521739131, total=   2.9s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.0, score=0.8831521739130435, total=   4.8s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:  2.9min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025, score=0.9051490514905149, total=   3.2s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025, score=0.8614130434782609, total=   3.2s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=225, select_ftrs__lambda1=0.025, score=0.875, total=   3.2s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  3.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0, score=0.8997289972899729, total=   7.6s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0 \n",
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0, score=0.8668478260869565, total=   7.5s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:  3.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.0, score=0.8722826086956522, total=   7.3s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025, score=0.9024390243902439, total=   8.1s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025, score=0.8641304347826086, total=   7.8s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025 \n",
      "[CV]  main__random_state=10, select_ftrs__N=1524, select_ftrs__lambda1=0.025, score=0.8668478260869565, total=  10.0s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  3.9min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  3.9min finished\n"
     ]
    }
   ],
   "source": [
    "##### DFS + Gradient Boosting #####\n",
    "param_grid = [\n",
    "    {\n",
    "     'select_ftrs__N': NUM_FEATURES,\n",
    "     'select_ftrs__lambda1': np.arange(0, 0.05, 0.025),\n",
    "     'main__random_state': [10]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['DFS + GB'] = evaluate(XGBClassifier(),\n",
    "                                  DFS(), param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T15:23:03.526194Z",
     "start_time": "2018-05-01T15:22:29.390802Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=10, score=0.8590785907859079, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=10, score=0.8695652173913043, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=10, score=0.8070652173913043, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=25 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=25, score=0.8753387533875339, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=25 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=25, score=0.875, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=25 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=25, score=0.8369565217391305, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=50, score=0.8401084010840109, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    2.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=50, score=0.845108695652174, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    2.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=50, score=0.8641304347826086, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    2.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=75, score=0.8536585365853658, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=75, score=0.842391304347826, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    3.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=75, score=0.8559782608695652, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    3.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=100, score=0.8617886178861789, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    4.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=100, score=0.8396739130434783, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    4.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=100, score=0.8559782608695652, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    4.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=125, score=0.8915989159891599, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    5.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=125, score=0.8614130434782609, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    5.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=125, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    5.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=150, score=0.8292682926829268, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    6.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=150, score=0.8586956521739131, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    6.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=150, score=0.8478260869565217, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    6.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=175, score=0.8644986449864499, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    7.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=175, score=0.8369565217391305, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    7.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=175, score=0.8532608695652174, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    7.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=200, score=0.8753387533875339, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    7.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=200, score=0.8722826086956522, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    8.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=200, score=0.8586956521739131, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    8.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=225, score=0.8699186991869918, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    9.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=225, score=0.8559782608695652, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    9.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=225, score=0.8668478260869565, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   10.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=1524, score=0.8807588075880759, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:   10.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=1524, score=0.8532608695652174, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   11.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.25, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.25, select_ftrs__N=1524, score=0.8152173913043478, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   11.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=10 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=10, score=0.8428184281842819, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   11.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=10 ...........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=10, score=0.875, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:   12.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=10 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=10, score=0.75, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   12.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=25 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=25, score=0.8509485094850948, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:   12.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=25 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=25, score=0.8586956521739131, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   13.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=25 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=25, score=0.8260869565217391, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:   13.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=50 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=50, score=0.8563685636856369, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   13.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=50 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=50, score=0.8668478260869565, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:   13.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=50 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=50, score=0.8885869565217391, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   14.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=75 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=75, score=0.8536585365853658, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:   14.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=75 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=75, score=0.8641304347826086, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:   14.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=75 ...........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=75, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   15.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=100 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=100, score=0.8536585365853658, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:   15.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=100 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=100, score=0.8668478260869565, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:   15.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=100 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=100, score=0.8777173913043478, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   16.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=125 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=125, score=0.8672086720867209, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:   16.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=125 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=125, score=0.8315217391304348, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   16.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=125 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=125, score=0.842391304347826, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:   17.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=150 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=150, score=0.8672086720867209, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:   17.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=150 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=150, score=0.8559782608695652, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:   17.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=150 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=150, score=0.845108695652174, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:   18.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=175 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=175, score=0.8563685636856369, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:   18.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=175 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=175, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:   18.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=175 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=175, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:   19.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=200 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=200, score=0.8482384823848238, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:   19.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=200 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=200, score=0.8505434782608695, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:   19.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=200 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=200, score=0.8586956521739131, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   20.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=225 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=225, score=0.8726287262872628, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:   20.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=225 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=225, score=0.8505434782608695, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:   21.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=225 ..........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=225, score=0.8614130434782609, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:   21.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=1524 .........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=1524, score=0.8536585365853658, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:   22.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=1524 .........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=1524, score=0.8369565217391305, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:   22.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.5, select_ftrs__N=1524 .........................\n",
      "[CV]  select_ftrs__C=0.5, select_ftrs__N=1524, score=0.8396739130434783, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:   22.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=10, score=0.8563685636856369, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:   23.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=10, score=0.8695652173913043, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:   23.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=10 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=10, score=0.7608695652173914, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:   23.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=25 ..........................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=25, score=0.8753387533875339, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:   24.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=25 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=25, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:   24.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=25 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=25, score=0.8125, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:   24.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=50, score=0.8455284552845529, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:   24.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=50, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:   25.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=50 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=50, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:   25.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=75, score=0.8780487804878049, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:   25.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=75, score=0.8288043478260869, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:   26.2s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=75 ..........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=75, score=0.8614130434782609, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:   26.5s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=100, score=0.8672086720867209, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:   26.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=100, score=0.8614130434782609, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   27.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=100 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=100, score=0.842391304347826, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:   27.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=125, score=0.8482384823848238, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:   27.8s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=125, score=0.875, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:   28.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=125 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=125, score=0.8586956521739131, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:   28.4s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=150, score=0.8780487804878049, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:   28.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=150, score=0.8505434782608695, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:   29.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=150 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=150, score=0.8641304347826086, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:   29.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=175, score=0.8672086720867209, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:   29.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=175, score=0.8559782608695652, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:   30.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=175 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=175, score=0.8777173913043478, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:   30.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=200, score=0.8590785907859079, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:   30.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=200, score=0.8641304347826086, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:   31.0s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=200 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=200, score=0.8668478260869565, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:   31.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=225, score=0.8861788617886179, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:   31.6s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=225, score=0.842391304347826, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:   31.9s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=225 .........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=225, score=0.8641304347826086, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:   32.3s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=1524, score=0.8699186991869918, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:   32.7s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=1524, score=0.8586956521739131, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:   33.1s remaining:    0.0s\n",
      "[CV] select_ftrs__C=0.75, select_ftrs__N=1524 ........................\n",
      "[CV]  select_ftrs__C=0.75, select_ftrs__N=1524, score=0.845108695652174, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:   33.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:   33.5s finished\n"
     ]
    }
   ],
   "source": [
    "##### LassoFS + RandomForest #####\n",
    "param_grid = [\n",
    "    {\n",
    "     'select_ftrs__N':  NUM_FEATURES,\n",
    "     'select_ftrs__C': np.arange(0.25, 1., 0.25) \n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['Lasso + RF'] =  evaluate(ens.RandomForestClassifier(),\n",
    "                                     LassoFS(), param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T15:24:52.441797Z",
     "start_time": "2018-05-01T15:23:03.528908Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 33 candidates, totalling 99 fits\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10, score=0.8319783197831978, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10, score=0.8858695652173914, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=10, score=0.8478260869565217, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25, score=0.8428184281842819, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25, score=0.8885869565217391, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=25, score=0.8777173913043478, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    2.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50, score=0.8644986449864499, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    2.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50, score=0.904891304347826, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    3.4s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=50, score=0.8641304347826086, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    3.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75, score=0.8617886178861789, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    4.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75, score=0.8994565217391305, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    4.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=75, score=0.8614130434782609, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    5.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100, score=0.8563685636856369, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    5.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100, score=0.907608695652174, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    6.4s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=100, score=0.8695652173913043, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    6.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125, score=0.8563685636856369, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    7.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125, score=0.8940217391304348, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    8.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=125, score=0.8641304347826086, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    8.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150, score=0.8509485094850948, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    9.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150, score=0.904891304347826, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   10.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=150, score=0.8777173913043478, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   10.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175, score=0.8509485094850948, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   11.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175, score=0.907608695652174, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   12.4s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=175, score=0.875, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   13.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200, score=0.8536585365853658, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   13.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200, score=0.9021739130434783, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:   15.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=200, score=0.8804347826086957, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:   16.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225, score=0.8509485094850948, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:   17.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225, score=0.907608695652174, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:   18.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=225, score=0.8695652173913043, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   19.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524 .\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524, score=0.8590785907859079, total=   5.5s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:   24.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524, score=0.9021739130434783, total=   5.6s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   30.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524 .\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.25, select_ftrs__N=1524, score=0.8695652173913043, total=   5.6s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   36.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10, score=0.8401084010840109, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   36.4s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10, score=0.8858695652173914, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:   36.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=10, score=0.8478260869565217, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:   37.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25, score=0.8428184281842819, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:   37.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25, score=0.8885869565217391, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:   37.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=25, score=0.875, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:   38.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50, score=0.8482384823848238, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   38.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50, score=0.904891304347826, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:   39.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=50, score=0.8722826086956522, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:   39.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75, score=0.8617886178861789, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:   40.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75, score=0.8994565217391305, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:   40.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75 ....\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=75, score=0.8559782608695652, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:   41.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100, score=0.8563685636856369, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:   41.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100, score=0.8994565217391305, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:   42.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=100, score=0.8668478260869565, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:   42.7s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125, score=0.8536585365853658, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:   43.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125, score=0.8967391304347826, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:   43.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=125, score=0.8641304347826086, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed:   44.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150, score=0.8644986449864499, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed:   45.8s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150, score=0.904891304347826, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed:   46.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=150, score=0.8804347826086957, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed:   47.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175, score=0.8563685636856369, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed:   48.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175, score=0.904891304347826, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed:   49.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=175, score=0.875, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed:   49.9s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200, score=0.8617886178861789, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed:   50.6s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200, score=0.9021739130434783, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed:   51.5s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=200, score=0.8777173913043478, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:   52.3s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225, score=0.8563685636856369, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed:   53.2s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225, score=0.907608695652174, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed:   54.1s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=225, score=0.875, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed:   55.0s remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524 ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524, score=0.8590785907859079, total=   5.6s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed:  1.0min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524, score=0.9021739130434783, total=   5.2s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.5, select_ftrs__N=1524, score=0.8695652173913043, total=   5.4s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10, score=0.8401084010840109, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10, score=0.8858695652173914, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=10, score=0.8559782608695652, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25, score=0.8292682926829268, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25, score=0.8940217391304348, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=25, score=0.875, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50, score=0.8672086720867209, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50, score=0.9021739130434783, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=50, score=0.8586956521739131, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75, score=0.8590785907859079, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75, score=0.904891304347826, total=   0.5s\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75 ...\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=75, score=0.8695652173913043, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100, score=0.8563685636856369, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100, score=0.9103260869565217, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=100, score=0.875, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125, score=0.8563685636856369, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125, score=0.9103260869565217, total=   0.7s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=125, score=0.8777173913043478, total=   0.6s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150, score=0.8590785907859079, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150, score=0.8967391304347826, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=150, score=0.8777173913043478, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175, score=0.8617886178861789, total=   1.0s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175, score=0.904891304347826, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=175, score=0.8722826086956522, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200, score=0.8509485094850948, total=   1.1s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200, score=0.8994565217391305, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=200, score=0.875, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225, score=0.8536585365853658, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225 ..\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225, score=0.907608695652174, total=   0.9s\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225 ..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=225, score=0.8614130434782609, total=   0.8s\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524 .\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524, score=0.8590785907859079, total=   5.5s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524 .\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524, score=0.9021739130434783, total=   5.4s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524 .\n",
      "[CV]  main__random_state=10, select_ftrs__C=0.75, select_ftrs__N=1524, score=0.8695652173913043, total=   5.1s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:  1.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed:  1.8min finished\n"
     ]
    }
   ],
   "source": [
    "##### LassoFS + GradientBoosting #####\n",
    "param_grid = [\n",
    "    {\n",
    "     'select_ftrs__N':  NUM_FEATURES,\n",
    "     'select_ftrs__C': np.arange(0.25, 1., 0.25),\n",
    "     'main__random_state': [10]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['Lasso + GB'] = evaluate(XGBClassifier(),\n",
    "                                    LassoFS(), param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T16:34:24.228020Z",
     "start_time": "2018-05-01T16:34:20.217519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] select_ftrs__N=1, select_ftrs__dropout_rate=0.1, select_ftrs__lambda1=0.05, select_ftrs__layers_sizes=[32, 16, 2], select_ftrs__num_epochs=150, select_ftrs__test_data=[      feature-0  feature-1  feature-2  feature-3  feature-4  feature-5  \\\n",
      "670   -0.478645  -1.561921  -0.696277  -1.296979  -1.318790  -0.385580   \n",
      "174   -0.290262  -0.040104  -0.726574  -0.251104   0.185794  -0.248343   \n",
      "137   -0.474567  -0.808953  -1.233911  -0.978462   0.955691  -0.384228   \n",
      "153    0.147392   0.569413   0.099982   0.533354   0.325037   0.071959   \n",
      "523   -0.450602  -0.921771  -1.369119  -1.009942   0.419665  -0.366032   \n",
      "782   -0.209702   0.071686   1.122537   0.292190  -0.379046  -0.188343   \n",
      "894   -0.398907  -0.478920  -0.909908  -0.664656   0.129416  -0.328596   \n",
      "646   -0.163512  -1.210361  -1.191512  -1.582176   0.622506  -0.156864   \n",
      "740   -0.107282  -0.338189  -0.099083  -0.403904  -0.033842  -0.108371   \n",
      "628   -0.236061  -0.203786  -0.126596  -0.064927  -0.050158  -0.207248   \n",
      "276   -0.196069   0.241403  -0.106131   0.248606  -0.053784  -0.178539   \n",
      "823   -0.514485  -1.527201  -0.879389  -1.417111  -0.542516  -0.412407   \n",
      "845   -0.370143  -0.333181  -1.550478  -0.558200   0.219518  -0.307419   \n",
      "356    0.367351   0.168518  -0.521489  -0.150878  -0.280677   0.276740   \n",
      "466   -0.109032  -0.407019  -0.273872  -0.315900  -0.694810  -0.115553   \n",
      "1070  -0.379459  -0.048482  -2.170328  -0.694642   3.986229  -0.314435   \n",
      "1024  -0.263937  -0.045138  -0.011689   0.164861  -0.454419  -0.228712   \n",
      "1102  -0.021078   1.406830   0.414358   1.338244   1.686560  -0.050262   \n",
      "70    -0.378738  -0.709752  -0.759681  -0.503795  -0.445635  -0.312913   \n",
      "532   -0.279909  -0.234814  -0.197402  -0.381496   1.413765  -0.239746   \n",
      "114    0.907038  -0.172747   0.105419  -0.102705  -0.014867   0.886696   \n",
      "963   -0.494162  -1.605178  -0.996269  -1.206687  -1.252624  -0.397082   \n",
      "822   -0.192199  -0.181171   0.917691   0.319195  -1.221650  -0.174653   \n",
      "855   -0.277551  -1.008212  -1.201222  -1.253923   0.320969  -0.226354   \n",
      "1075  -0.190819   0.340476   0.851536   0.410330   0.329020  -0.168542   \n",
      "307   -0.154965   0.214183   0.209616   0.526811  -0.957532  -0.137100   \n",
      "220   -0.217814  -0.074964   0.622303   0.223272   0.238950  -0.188046   \n",
      "787   -0.236383  -0.350440  -1.171777  -0.993750   0.852798  -0.210549   \n",
      "2      0.613731   2.522972   1.218073   2.328402   1.986481   0.427107   \n",
      "697    1.185341   4.142747   2.566442   3.302030   3.601129   0.866666   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "170   -0.559367  -1.575552  -1.601933  -1.506653  -0.579804  -0.445950   \n",
      "22    -0.229746   0.235154  -0.177499   0.332630  -0.432253  -0.203869   \n",
      "202   -0.105955   0.215968   1.077920   0.977239  -1.719121  -0.111205   \n",
      "358   -0.300714  -0.404256   0.630021  -0.021859  -1.087681  -0.255396   \n",
      "1017  -0.309273  -0.300184  -0.066175  -0.176471  -0.093871  -0.261909   \n",
      "147   -0.334712   0.242497  -0.828002  -0.588698   1.889942  -0.282154   \n",
      "78    -0.137754   1.177888  -0.635366   0.432872   1.843470  -0.137106   \n",
      "156   -0.414466  -0.273597  -2.793740  -0.534280   1.347409  -0.340608   \n",
      "486   -0.433250  -0.947589  -0.997539  -0.973106  -0.379122  -0.353108   \n",
      "228    0.093927   0.378768   0.699383   0.642362  -0.623177   0.033348   \n",
      "273    0.358174   1.019496   1.203684   1.189554  -0.263057   0.227281   \n",
      "208    0.314374   1.159036   1.238049   1.375727   0.259958   0.195337   \n",
      "979   -0.136163  -1.095742   0.381585  -0.659405  -1.372242  -0.119173   \n",
      "996   -0.346786  -0.945919  -1.162380  -0.495167  -1.829456  -0.288546   \n",
      "480   -0.505972  -1.363363  -0.181396  -1.433874  -0.466633  -0.406480   \n",
      "691   -0.231070  -0.131278   0.235928  -0.042111   0.413535  -0.203615   \n",
      "826   -0.253794  -0.107806   0.612396  -0.085988   0.961785  -0.220618   \n",
      "518   -0.397640  -0.681310  -1.171777  -0.639001  -0.492843  -0.327188   \n",
      "64     1.276050   2.610418   1.435458   1.863827   1.849996   0.958122   \n",
      "1084  -0.310129  -0.699418   0.613926  -0.231872  -0.781512  -0.261535   \n",
      "850   -0.333612  -0.138101  -0.658688  -0.612506   2.039542  -0.280189   \n",
      "630   -0.284602   0.013154   0.880578  -0.042847   0.001591  -0.238935   \n",
      "528   -0.377499  -0.589756  -0.514497  -0.601554   0.185754  -0.307771   \n",
      "605   -0.178884  -0.912512  -0.081281  -0.852494  -0.921322  -0.166692   \n",
      "929   -0.445622  -1.223996  -1.130336  -1.002655  -0.897912  -0.361696   \n",
      "639   -0.493018  -1.177081  -1.057223  -1.253573   0.147348  -0.397172   \n",
      "387   -0.480745  -0.853494  -1.612946  -1.160084   0.459549  -0.388840   \n",
      "940   -0.087185  -0.355845  -0.179327  -0.146234  -0.803511  -0.099407   \n",
      "658    0.084440  -0.642007  -0.157848  -0.745554  -0.438965   0.055286   \n",
      "1035   0.634659   1.966786   2.337873   2.458849   1.174157   0.431241   \n",
      "\n",
      "      feature-6  feature-7  feature-8  feature-9      ...       feature-1514  \\\n",
      "670    0.086885  -1.520576  -1.316477  -1.608615      ...          -0.340971   \n",
      "174   -0.630414  -0.094923   0.325191  -0.024637      ...          -0.098511   \n",
      "137   -1.005469  -0.879310  -0.442604  -0.769754      ...           0.958521   \n",
      "153    0.125037   0.539450   1.055259   0.612567      ...          -0.705006   \n",
      "523   -0.656831  -0.960033  -0.664264  -0.930568      ...          -1.502921   \n",
      "782   -0.169149   0.052988  -0.387588   0.009128      ...           1.224003   \n",
      "894   -0.828072  -0.539407  -0.336460  -0.398975      ...           1.740733   \n",
      "646   -0.012240  -1.215455  -1.256433  -1.136410      ...          -1.570762   \n",
      "740    0.204300  -0.340656  -0.538455  -0.394275      ...           0.453681   \n",
      "628   -0.034753  -0.207028  -0.759041  -0.346920      ...          -1.016894   \n",
      "276   -0.312673   0.206358   0.374396   0.187628      ...           0.801116   \n",
      "823   -0.265864  -1.517468  -1.341793  -1.552299      ...          -0.257557   \n",
      "845   -0.818586  -0.396495  -0.366867  -0.265352      ...           1.740733   \n",
      "356    0.753119   0.331996   0.277966   0.446289      ...          -0.427213   \n",
      "466    0.188049  -0.399028  -0.486484  -0.372227      ...           0.390865   \n",
      "1070  -1.541897  -0.189708  -0.088362  -0.180598      ...          -2.893276   \n",
      "1024  -0.391087  -0.078484  -0.333624  -0.031371      ...           0.326459   \n",
      "1102  -0.698178   1.308818   1.188030   1.283587      ...          -0.516482   \n",
      "70    -0.355681  -0.725604  -0.345602  -0.712454      ...           0.633325   \n",
      "532   -0.416602  -0.273152  -0.500837  -0.425482      ...          -0.804779   \n",
      "114    0.351780  -0.136322  -0.206891  -0.192422      ...           1.921227   \n",
      "963    0.028502  -1.567740  -1.224988  -1.641593      ...          -0.137350   \n",
      "822    0.333955  -0.151292  -0.064351  -0.305799      ...           1.422330   \n",
      "855   -0.492839  -0.973070  -0.734189  -0.885905      ...           0.051174   \n",
      "1075  -0.279279   0.296292   0.198174   0.221493      ...           0.484516   \n",
      "307    0.336249   0.220601   0.678684   0.040644      ...          -0.608998   \n",
      "220    0.048077  -0.083171  -0.354085  -0.286938      ...           0.907127   \n",
      "787   -0.791669  -0.425734  -0.222096  -0.204188      ...          -0.340971   \n",
      "2      0.400668   2.437803   2.013139   2.407961      ...           0.484516   \n",
      "697    1.083146   4.017984   4.444951   3.902737      ...          -0.427213   \n",
      "...         ...        ...        ...        ...      ...                ...   \n",
      "170   -0.511641  -1.584014  -0.942327  -1.520790      ...           0.514983   \n",
      "22    -0.506017   0.186433   0.638986   0.283857      ...           0.907127   \n",
      "202    0.474132   0.250026   0.845672   0.106909      ...          -0.383726   \n",
      "358   -0.146428  -0.407886  -0.128696  -0.397037      ...           1.546447   \n",
      "1017  -0.415260  -0.331172  -0.470139  -0.321410      ...           1.200981   \n",
      "147   -1.425782   0.112049  -0.174897   0.313265      ...          -0.022568   \n",
      "78    -1.229312   1.042401   1.052634   1.199550      ...          -0.516482   \n",
      "156   -1.289123  -0.379728   0.881525  -0.187310      ...          -3.135736   \n",
      "486   -0.440573  -0.965129  -0.790416  -0.933183      ...          -0.705006   \n",
      "228    0.341408   0.377538   0.692200   0.411246      ...          -0.022568   \n",
      "273    0.736084   1.025500   0.945973   1.003363      ...           1.082639   \n",
      "208    0.504059   1.145594   1.063267   1.104189      ...           1.485145   \n",
      "979    0.583511  -0.959878  -1.210167  -1.094097      ...           0.690494   \n",
      "996    0.269987  -0.901805  -0.271570  -0.987330      ...          -1.785868   \n",
      "480   -0.417106  -1.369497  -1.145379  -1.349162      ...          -0.705006   \n",
      "691   -0.128118  -0.145283  -0.462931  -0.297434      ...          -0.257557   \n",
      "826   -0.358133  -0.142604  -0.639221  -0.264627      ...          -0.257557   \n",
      "518   -0.517308  -0.710376  -0.146975  -0.628522      ...           0.192538   \n",
      "64     2.092762   2.649109   2.247880   2.787344      ...           1.200981   \n",
      "1084   0.130910  -0.675780  -0.739422  -0.814530      ...           0.014570   \n",
      "850   -0.957169  -0.223679  -0.527706  -0.231690      ...          -1.372435   \n",
      "630   -0.513277  -0.039818   0.175542   0.003050      ...           0.390865   \n",
      "528   -0.440472  -0.623121  -0.629107  -0.645456      ...           0.907127   \n",
      "605    0.318657  -0.882626  -0.624241  -0.889461      ...           0.453681   \n",
      "929   -0.140049  -1.209681  -1.170560  -1.245775      ...          -0.608998   \n",
      "639   -0.609291  -1.204883  -0.904049  -1.171554      ...          -0.908627   \n",
      "387   -0.952414  -0.916766  -0.717543  -0.774956      ...          -1.016894   \n",
      "940    0.266217  -0.342640  -0.446568  -0.329401      ...           0.390865   \n",
      "658    0.265428  -0.483994  -0.955834  -0.465002      ...           0.453681   \n",
      "1035   1.100482   1.971637   0.999354   1.726933      ...          -0.298922   \n",
      "\n",
      "      feature-1515  feature-1516  feature-1517  feature-1518  feature-1519  \\\n",
      "670       0.551680     -0.277110      0.032307      0.183895      0.303517   \n",
      "174      -0.530624     -0.343830     -0.542318     -0.713340     -0.717651   \n",
      "137      -0.427632      0.587912      0.116512     -0.160725     -0.634297   \n",
      "153      -0.777795     -0.875153     -0.787457     -0.691724     -0.806001   \n",
      "523      -1.249069     -1.348454     -1.160282     -1.075229     -0.845780   \n",
      "782       1.627941      1.409911      1.510008      1.475109      1.522841   \n",
      "894       0.053851      1.258613      0.770626      0.317250      0.064373   \n",
      "646      -1.002572     -1.511990     -1.230539     -0.927147     -1.061079   \n",
      "740       0.241822      0.456741      0.482043      0.488722      0.474240   \n",
      "628       0.088732     -0.818868     -0.459031     -0.233112     -0.042121   \n",
      "276       0.141988      0.617709      0.380042      0.170951      0.212231   \n",
      "823       0.504265     -0.233691     -0.086360      0.039564      0.173437   \n",
      "845       0.119741      1.120423      0.589276      0.391807      0.317574   \n",
      "356      -0.593313     -0.531932     -0.664283     -0.647530     -0.526615   \n",
      "466       0.514495      0.300611      0.240246      0.238700      0.334423   \n",
      "1070     -2.918446     -3.110891     -3.614643     -3.711234     -4.148053   \n",
      "1024      0.719248      0.384116      0.491378      0.502182      0.574471   \n",
      "1102     -0.010088     -0.343830     -0.240848     -0.170264     -0.097752   \n",
      "70        0.151349      0.464670      0.372108      0.373049      0.355396   \n",
      "532      -0.054093     -0.683950     -0.490617     -0.280974     -0.165436   \n",
      "114       0.381708      1.638542      1.278262      0.938599      0.791862   \n",
      "963       0.206581     -0.087924      0.070211      0.148021      0.262675   \n",
      "822       1.049242      1.451356      1.570869      1.532746      1.477381   \n",
      "855      -0.343785     -0.170075     -0.297297     -0.384817     -0.422747   \n",
      "1075      1.027496      0.690541      0.897227      1.009407      1.081982   \n",
      "307       0.279236     -0.459563     -0.246415     -0.102146      0.051336   \n",
      "220       0.885127      1.029597      1.044756      1.020680      0.973027   \n",
      "787      -0.874359     -0.710303     -0.911491     -1.082849     -1.190206   \n",
      "2         0.180819      0.342746      0.263597      0.149671     -0.115213   \n",
      "697      -0.064146      0.105364     -0.050946      0.078154      0.241051   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "170       0.205661      0.283536      0.183837      0.087359      0.130711   \n",
      "22        0.076387      0.587912      0.116512     -0.149899     -0.296312   \n",
      "202       0.254005     -0.244469      0.025111      0.141956      0.046646   \n",
      "358       1.282628      1.581075      1.395533      1.242509      1.240912   \n",
      "1017      0.487001      1.023411      0.912083      0.681765      0.561734   \n",
      "147      -0.510883     -0.412562     -0.643383     -0.910070     -0.947484   \n",
      "78       -1.013723     -0.763939     -1.114900     -1.166697     -0.970825   \n",
      "156      -2.918446     -3.817870     -4.502539     -4.501903     -4.331542   \n",
      "486      -0.511093     -0.846837     -0.595523     -0.479596     -0.408884   \n",
      "228      -0.442921     -0.087924     -0.229764     -0.289847     -0.320377   \n",
      "273       0.445792      1.017208      0.828838      0.615671      0.413196   \n",
      "208       0.419653      1.319254      1.082886      0.764656      0.470735   \n",
      "979       0.854439      0.774974      0.811725      0.819630      0.832701   \n",
      "996      -2.918446     -1.813676     -1.570666     -1.371852     -1.016627   \n",
      "480       0.164799     -0.581487     -0.332022     -0.173000      0.013606   \n",
      "691       0.269272     -0.108186      0.070211      0.074688      0.195207   \n",
      "826       0.602090     -0.057865      0.212259      0.437230      0.470515   \n",
      "518      -0.083421     -0.233691     -0.535775     -0.645638     -0.736255   \n",
      "64        0.753965      1.241790      0.874714      0.763210      0.625449   \n",
      "1084      0.688776      0.317558      0.634606      0.749763      0.647970   \n",
      "850      -0.764127     -1.212328     -0.935714     -0.786397     -0.588017   \n",
      "630       0.406637      0.432789      0.419204      0.285532      0.208408   \n",
      "528       0.254888      0.863047      0.784958      0.784756      0.686476   \n",
      "605       0.604474      0.639804      0.643196      0.626035      0.592038   \n",
      "929      -0.041543     -0.710303     -0.575390     -0.418431     -0.247988   \n",
      "639      -1.057821     -0.962276     -0.706816     -0.636217     -0.617209   \n",
      "387      -0.790359     -0.903826     -0.794968     -0.753477     -0.582689   \n",
      "940       0.514495      0.300611      0.240246      0.238700      0.334423   \n",
      "658      -0.054216      0.367658      0.307365      0.238700      0.269710   \n",
      "1035      0.576648      0.030004      0.348097      0.486539      0.330480   \n",
      "\n",
      "      feature-1520  feature-1521  feature-1522  feature-1523  \n",
      "670       0.292304      0.377642      0.436993      0.504130  \n",
      "174      -0.951157     -0.953918     -0.786829     -0.619251  \n",
      "137      -0.624318     -0.574417     -0.611686     -0.590944  \n",
      "153      -0.531880     -0.545035     -0.651855     -0.810741  \n",
      "523      -0.961579     -1.141992     -1.032076     -0.953388  \n",
      "782       1.601106      1.584138      1.630810      1.642173  \n",
      "894       0.003478     -0.029811     -0.079485     -0.030709  \n",
      "646      -0.985926     -0.812654     -0.736341     -0.964528  \n",
      "740       0.381691      0.357258      0.407356      0.180971  \n",
      "628      -0.112774     -0.033928      0.005175      0.054187  \n",
      "276       0.106113      0.082412     -0.107746      0.053722  \n",
      "823       0.162184      0.259482      0.356260      0.453013  \n",
      "845       0.171280      0.084288      0.062991      0.121222  \n",
      "356      -0.758022     -0.775638     -0.872538     -0.832774  \n",
      "466       0.290308      0.332905      0.395400      0.470719  \n",
      "1070     -5.337989     -4.505849     -3.971525     -3.410555  \n",
      "1024      0.680304      0.672620      0.663213      0.700245  \n",
      "1102     -0.019498     -0.227371     -0.167164     -0.057612  \n",
      "70        0.272489      0.203552      0.158077      0.076285  \n",
      "532      -0.062588      0.026089      0.005014     -0.052072  \n",
      "114       0.533943      0.412169      0.392792      0.399261  \n",
      "963       0.243899      0.279890      0.352252      0.414700  \n",
      "822       1.365364      1.274329      1.212867      1.210232  \n",
      "855      -0.462109     -0.360693     -0.367828     -0.398658  \n",
      "1075      1.199977      1.172880      1.119013      1.119666  \n",
      "307       0.050624      0.184666      0.234696      0.202341  \n",
      "220       0.868904      0.834006      0.884612      0.843138  \n",
      "787      -1.054732     -1.047140     -1.082761     -0.983111  \n",
      "2        -0.072028      0.007996      0.037094      0.097767  \n",
      "697       0.438442      0.376540      0.102555      0.051392  \n",
      "...            ...           ...           ...           ...  \n",
      "170       0.161041      0.158100      0.148446      0.246714  \n",
      "22       -0.178160     -0.105253     -0.081438      0.006884  \n",
      "202       0.123520      0.195705      0.140692      0.207945  \n",
      "358       1.333942      1.273429      1.296535      1.291719  \n",
      "1017      0.495213      0.421685      0.416100      0.443443  \n",
      "147      -0.832604     -0.832386     -0.756214     -0.622827  \n",
      "78       -0.718969     -0.604951     -0.761615     -0.952929  \n",
      "156      -3.798316     -3.426586     -3.971525     -3.410555  \n",
      "486      -0.491444     -0.367238     -0.378334     -0.467608  \n",
      "228      -0.463206     -0.475270     -0.449123     -0.461025  \n",
      "273       0.384298      0.378193      0.351358      0.425250  \n",
      "208       0.327066      0.342138      0.329007      0.343588  \n",
      "979       0.739376      0.728813      0.780705      0.812371  \n",
      "996      -1.183252     -0.984473     -0.954415     -0.851462  \n",
      "480       0.060628      0.115700      0.154396      0.191844  \n",
      "691       0.117553      0.093799      0.063877      0.212968  \n",
      "826       0.650514      0.670046      0.725065      0.635364  \n",
      "518      -0.453372     -0.354201     -0.304705     -0.174133  \n",
      "64        0.775979      0.775853      0.819276      0.747271  \n",
      "1084      0.688330      0.695774      0.742555      0.646727  \n",
      "850      -0.543356     -0.669513     -0.795687     -0.762167  \n",
      "630       0.279589      0.257234      0.284259      0.359294  \n",
      "528       0.598313      0.555732      0.536995      0.315001  \n",
      "605       0.568399      0.614791      0.636312      0.549384  \n",
      "929      -0.452648     -0.245557     -0.142999     -0.093046  \n",
      "639      -0.703313     -0.585411     -0.607907     -0.851462  \n",
      "387      -0.588182     -0.867487     -0.963298     -0.808780  \n",
      "940       0.290308      0.332905      0.395400      0.470719  \n",
      "658       0.127675      0.028896      0.036582      0.011788  \n",
      "1035      0.430600      0.459752      0.565864      0.494764  \n",
      "\n",
      "[221 rows x 1524 columns],         y\n",
      "670   1.0\n",
      "174   0.0\n",
      "137   0.0\n",
      "153   0.0\n",
      "523   1.0\n",
      "782   1.0\n",
      "894   1.0\n",
      "646   1.0\n",
      "740   1.0\n",
      "628   1.0\n",
      "276   0.0\n",
      "823   1.0\n",
      "845   1.0\n",
      "356   1.0\n",
      "466   1.0\n",
      "1070  1.0\n",
      "1024  1.0\n",
      "1102  1.0\n",
      "70    0.0\n",
      "532   1.0\n",
      "114   0.0\n",
      "963   1.0\n",
      "822   1.0\n",
      "855   1.0\n",
      "1075  1.0\n",
      "307   0.0\n",
      "220   0.0\n",
      "787   1.0\n",
      "2     0.0\n",
      "697   1.0\n",
      "...   ...\n",
      "170   0.0\n",
      "22    0.0\n",
      "202   0.0\n",
      "358   1.0\n",
      "1017  1.0\n",
      "147   0.0\n",
      "78    0.0\n",
      "156   0.0\n",
      "486   1.0\n",
      "228   0.0\n",
      "273   0.0\n",
      "208   0.0\n",
      "979   1.0\n",
      "996   1.0\n",
      "480   1.0\n",
      "691   1.0\n",
      "826   1.0\n",
      "518   1.0\n",
      "64    0.0\n",
      "1084  1.0\n",
      "850   1.0\n",
      "630   1.0\n",
      "528   1.0\n",
      "605   1.0\n",
      "929   1.0\n",
      "639   1.0\n",
      "387   1.0\n",
      "940   1.0\n",
      "658   1.0\n",
      "1035  1.0\n",
      "\n",
      "[221 rows x 1 columns]], select_ftrs__verbose=True \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 100.59648497208305.Train accuracy: 0.6915760869565217. Test accuracy: 0.7104072398190046.\n",
      "==> Epoch: 1. Train loss: 99.0601056969684.Train accuracy: 0.7282608695652174. Test accuracy: 0.7420814479638009.\n",
      "==> Epoch: 2. Train loss: 96.26917266845703.Train accuracy: 0.7391304347826086. Test accuracy: 0.751131221719457.\n",
      "==> Epoch: 3. Train loss: 96.36596845543903.Train accuracy: 0.7730978260869565. Test accuracy: 0.7647058823529411.\n",
      "==> Epoch: 4. Train loss: 94.9281702456267.Train accuracy: 0.78125. Test accuracy: 0.7737556561085973.\n",
      "==> Epoch: 5. Train loss: 92.84950256347656.Train accuracy: 0.7771739130434783. Test accuracy: 0.751131221719457.\n",
      "==> Epoch: 6. Train loss: 91.57225633704144.Train accuracy: 0.7744565217391305. Test accuracy: 0.755656108597285.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-314-9beb89fe121d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m all_scores['DFS'] = evaluate(None, DFS(),\n\u001b[0;32m---> 15\u001b[0;31m                              param_grid, data_X, data_y.y)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-262-986e6c74992c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(main_model, feature_selector, param_grid, X, y, scoring, caching)\u001b[0m\n\u001b[1;32m     16\u001b[0m                            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                            verbose=100, scoring=scoring)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-302-a6dd4327a16a>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, test_data)\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                feed_dict = {self.x: batch_X,\n\u001b[1;32m     34\u001b[0m                                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                                                             self.dropout_rate_ph: self.dropout_rate})\n\u001b[0m\u001b[1;32m     36\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### Pure DFS #####\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_ftrs__layers_sizes': [[32, 16, 2]],\n",
    "        'select_ftrs__N': NUM_FEATURES,\n",
    "        'select_ftrs__lambda1': [5e-2],\n",
    "        'select_ftrs__num_epochs': [150],\n",
    "        'select_ftrs__verbose': [True],\n",
    "        'select_ftrs__test_data': [[test_X, test_y]],\n",
    "        'select_ftrs__dropout_rate': [0.1]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['DFS'] = evaluate(None, DFS(),\n",
    "                             param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T16:34:04.061484Z",
     "start_time": "2018-05-01T16:34:03.853611Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] select_ftrs__N=1, select_ftrs__dropout_rate=0.1, select_ftrs__lambda1=0.05, select_ftrs__layers_sizes=[32, 16, 2], select_ftrs__num_epochs=150, select_ftrs__test_data=[      feature-0  feature-1  feature-2  feature-3  feature-4  feature-5  \\\n",
      "670   -0.478645  -1.561921  -0.696277  -1.296979  -1.318790  -0.385580   \n",
      "174   -0.290262  -0.040104  -0.726574  -0.251104   0.185794  -0.248343   \n",
      "137   -0.474567  -0.808953  -1.233911  -0.978462   0.955691  -0.384228   \n",
      "153    0.147392   0.569413   0.099982   0.533354   0.325037   0.071959   \n",
      "523   -0.450602  -0.921771  -1.369119  -1.009942   0.419665  -0.366032   \n",
      "782   -0.209702   0.071686   1.122537   0.292190  -0.379046  -0.188343   \n",
      "894   -0.398907  -0.478920  -0.909908  -0.664656   0.129416  -0.328596   \n",
      "646   -0.163512  -1.210361  -1.191512  -1.582176   0.622506  -0.156864   \n",
      "740   -0.107282  -0.338189  -0.099083  -0.403904  -0.033842  -0.108371   \n",
      "628   -0.236061  -0.203786  -0.126596  -0.064927  -0.050158  -0.207248   \n",
      "276   -0.196069   0.241403  -0.106131   0.248606  -0.053784  -0.178539   \n",
      "823   -0.514485  -1.527201  -0.879389  -1.417111  -0.542516  -0.412407   \n",
      "845   -0.370143  -0.333181  -1.550478  -0.558200   0.219518  -0.307419   \n",
      "356    0.367351   0.168518  -0.521489  -0.150878  -0.280677   0.276740   \n",
      "466   -0.109032  -0.407019  -0.273872  -0.315900  -0.694810  -0.115553   \n",
      "1070  -0.379459  -0.048482  -2.170328  -0.694642   3.986229  -0.314435   \n",
      "1024  -0.263937  -0.045138  -0.011689   0.164861  -0.454419  -0.228712   \n",
      "1102  -0.021078   1.406830   0.414358   1.338244   1.686560  -0.050262   \n",
      "70    -0.378738  -0.709752  -0.759681  -0.503795  -0.445635  -0.312913   \n",
      "532   -0.279909  -0.234814  -0.197402  -0.381496   1.413765  -0.239746   \n",
      "114    0.907038  -0.172747   0.105419  -0.102705  -0.014867   0.886696   \n",
      "963   -0.494162  -1.605178  -0.996269  -1.206687  -1.252624  -0.397082   \n",
      "822   -0.192199  -0.181171   0.917691   0.319195  -1.221650  -0.174653   \n",
      "855   -0.277551  -1.008212  -1.201222  -1.253923   0.320969  -0.226354   \n",
      "1075  -0.190819   0.340476   0.851536   0.410330   0.329020  -0.168542   \n",
      "307   -0.154965   0.214183   0.209616   0.526811  -0.957532  -0.137100   \n",
      "220   -0.217814  -0.074964   0.622303   0.223272   0.238950  -0.188046   \n",
      "787   -0.236383  -0.350440  -1.171777  -0.993750   0.852798  -0.210549   \n",
      "2      0.613731   2.522972   1.218073   2.328402   1.986481   0.427107   \n",
      "697    1.185341   4.142747   2.566442   3.302030   3.601129   0.866666   \n",
      "...         ...        ...        ...        ...        ...        ...   \n",
      "170   -0.559367  -1.575552  -1.601933  -1.506653  -0.579804  -0.445950   \n",
      "22    -0.229746   0.235154  -0.177499   0.332630  -0.432253  -0.203869   \n",
      "202   -0.105955   0.215968   1.077920   0.977239  -1.719121  -0.111205   \n",
      "358   -0.300714  -0.404256   0.630021  -0.021859  -1.087681  -0.255396   \n",
      "1017  -0.309273  -0.300184  -0.066175  -0.176471  -0.093871  -0.261909   \n",
      "147   -0.334712   0.242497  -0.828002  -0.588698   1.889942  -0.282154   \n",
      "78    -0.137754   1.177888  -0.635366   0.432872   1.843470  -0.137106   \n",
      "156   -0.414466  -0.273597  -2.793740  -0.534280   1.347409  -0.340608   \n",
      "486   -0.433250  -0.947589  -0.997539  -0.973106  -0.379122  -0.353108   \n",
      "228    0.093927   0.378768   0.699383   0.642362  -0.623177   0.033348   \n",
      "273    0.358174   1.019496   1.203684   1.189554  -0.263057   0.227281   \n",
      "208    0.314374   1.159036   1.238049   1.375727   0.259958   0.195337   \n",
      "979   -0.136163  -1.095742   0.381585  -0.659405  -1.372242  -0.119173   \n",
      "996   -0.346786  -0.945919  -1.162380  -0.495167  -1.829456  -0.288546   \n",
      "480   -0.505972  -1.363363  -0.181396  -1.433874  -0.466633  -0.406480   \n",
      "691   -0.231070  -0.131278   0.235928  -0.042111   0.413535  -0.203615   \n",
      "826   -0.253794  -0.107806   0.612396  -0.085988   0.961785  -0.220618   \n",
      "518   -0.397640  -0.681310  -1.171777  -0.639001  -0.492843  -0.327188   \n",
      "64     1.276050   2.610418   1.435458   1.863827   1.849996   0.958122   \n",
      "1084  -0.310129  -0.699418   0.613926  -0.231872  -0.781512  -0.261535   \n",
      "850   -0.333612  -0.138101  -0.658688  -0.612506   2.039542  -0.280189   \n",
      "630   -0.284602   0.013154   0.880578  -0.042847   0.001591  -0.238935   \n",
      "528   -0.377499  -0.589756  -0.514497  -0.601554   0.185754  -0.307771   \n",
      "605   -0.178884  -0.912512  -0.081281  -0.852494  -0.921322  -0.166692   \n",
      "929   -0.445622  -1.223996  -1.130336  -1.002655  -0.897912  -0.361696   \n",
      "639   -0.493018  -1.177081  -1.057223  -1.253573   0.147348  -0.397172   \n",
      "387   -0.480745  -0.853494  -1.612946  -1.160084   0.459549  -0.388840   \n",
      "940   -0.087185  -0.355845  -0.179327  -0.146234  -0.803511  -0.099407   \n",
      "658    0.084440  -0.642007  -0.157848  -0.745554  -0.438965   0.055286   \n",
      "1035   0.634659   1.966786   2.337873   2.458849   1.174157   0.431241   \n",
      "\n",
      "      feature-6  feature-7  feature-8  feature-9      ...       feature-1514  \\\n",
      "670    0.086885  -1.520576  -1.316477  -1.608615      ...          -0.340971   \n",
      "174   -0.630414  -0.094923   0.325191  -0.024637      ...          -0.098511   \n",
      "137   -1.005469  -0.879310  -0.442604  -0.769754      ...           0.958521   \n",
      "153    0.125037   0.539450   1.055259   0.612567      ...          -0.705006   \n",
      "523   -0.656831  -0.960033  -0.664264  -0.930568      ...          -1.502921   \n",
      "782   -0.169149   0.052988  -0.387588   0.009128      ...           1.224003   \n",
      "894   -0.828072  -0.539407  -0.336460  -0.398975      ...           1.740733   \n",
      "646   -0.012240  -1.215455  -1.256433  -1.136410      ...          -1.570762   \n",
      "740    0.204300  -0.340656  -0.538455  -0.394275      ...           0.453681   \n",
      "628   -0.034753  -0.207028  -0.759041  -0.346920      ...          -1.016894   \n",
      "276   -0.312673   0.206358   0.374396   0.187628      ...           0.801116   \n",
      "823   -0.265864  -1.517468  -1.341793  -1.552299      ...          -0.257557   \n",
      "845   -0.818586  -0.396495  -0.366867  -0.265352      ...           1.740733   \n",
      "356    0.753119   0.331996   0.277966   0.446289      ...          -0.427213   \n",
      "466    0.188049  -0.399028  -0.486484  -0.372227      ...           0.390865   \n",
      "1070  -1.541897  -0.189708  -0.088362  -0.180598      ...          -2.893276   \n",
      "1024  -0.391087  -0.078484  -0.333624  -0.031371      ...           0.326459   \n",
      "1102  -0.698178   1.308818   1.188030   1.283587      ...          -0.516482   \n",
      "70    -0.355681  -0.725604  -0.345602  -0.712454      ...           0.633325   \n",
      "532   -0.416602  -0.273152  -0.500837  -0.425482      ...          -0.804779   \n",
      "114    0.351780  -0.136322  -0.206891  -0.192422      ...           1.921227   \n",
      "963    0.028502  -1.567740  -1.224988  -1.641593      ...          -0.137350   \n",
      "822    0.333955  -0.151292  -0.064351  -0.305799      ...           1.422330   \n",
      "855   -0.492839  -0.973070  -0.734189  -0.885905      ...           0.051174   \n",
      "1075  -0.279279   0.296292   0.198174   0.221493      ...           0.484516   \n",
      "307    0.336249   0.220601   0.678684   0.040644      ...          -0.608998   \n",
      "220    0.048077  -0.083171  -0.354085  -0.286938      ...           0.907127   \n",
      "787   -0.791669  -0.425734  -0.222096  -0.204188      ...          -0.340971   \n",
      "2      0.400668   2.437803   2.013139   2.407961      ...           0.484516   \n",
      "697    1.083146   4.017984   4.444951   3.902737      ...          -0.427213   \n",
      "...         ...        ...        ...        ...      ...                ...   \n",
      "170   -0.511641  -1.584014  -0.942327  -1.520790      ...           0.514983   \n",
      "22    -0.506017   0.186433   0.638986   0.283857      ...           0.907127   \n",
      "202    0.474132   0.250026   0.845672   0.106909      ...          -0.383726   \n",
      "358   -0.146428  -0.407886  -0.128696  -0.397037      ...           1.546447   \n",
      "1017  -0.415260  -0.331172  -0.470139  -0.321410      ...           1.200981   \n",
      "147   -1.425782   0.112049  -0.174897   0.313265      ...          -0.022568   \n",
      "78    -1.229312   1.042401   1.052634   1.199550      ...          -0.516482   \n",
      "156   -1.289123  -0.379728   0.881525  -0.187310      ...          -3.135736   \n",
      "486   -0.440573  -0.965129  -0.790416  -0.933183      ...          -0.705006   \n",
      "228    0.341408   0.377538   0.692200   0.411246      ...          -0.022568   \n",
      "273    0.736084   1.025500   0.945973   1.003363      ...           1.082639   \n",
      "208    0.504059   1.145594   1.063267   1.104189      ...           1.485145   \n",
      "979    0.583511  -0.959878  -1.210167  -1.094097      ...           0.690494   \n",
      "996    0.269987  -0.901805  -0.271570  -0.987330      ...          -1.785868   \n",
      "480   -0.417106  -1.369497  -1.145379  -1.349162      ...          -0.705006   \n",
      "691   -0.128118  -0.145283  -0.462931  -0.297434      ...          -0.257557   \n",
      "826   -0.358133  -0.142604  -0.639221  -0.264627      ...          -0.257557   \n",
      "518   -0.517308  -0.710376  -0.146975  -0.628522      ...           0.192538   \n",
      "64     2.092762   2.649109   2.247880   2.787344      ...           1.200981   \n",
      "1084   0.130910  -0.675780  -0.739422  -0.814530      ...           0.014570   \n",
      "850   -0.957169  -0.223679  -0.527706  -0.231690      ...          -1.372435   \n",
      "630   -0.513277  -0.039818   0.175542   0.003050      ...           0.390865   \n",
      "528   -0.440472  -0.623121  -0.629107  -0.645456      ...           0.907127   \n",
      "605    0.318657  -0.882626  -0.624241  -0.889461      ...           0.453681   \n",
      "929   -0.140049  -1.209681  -1.170560  -1.245775      ...          -0.608998   \n",
      "639   -0.609291  -1.204883  -0.904049  -1.171554      ...          -0.908627   \n",
      "387   -0.952414  -0.916766  -0.717543  -0.774956      ...          -1.016894   \n",
      "940    0.266217  -0.342640  -0.446568  -0.329401      ...           0.390865   \n",
      "658    0.265428  -0.483994  -0.955834  -0.465002      ...           0.453681   \n",
      "1035   1.100482   1.971637   0.999354   1.726933      ...          -0.298922   \n",
      "\n",
      "      feature-1515  feature-1516  feature-1517  feature-1518  feature-1519  \\\n",
      "670       0.551680     -0.277110      0.032307      0.183895      0.303517   \n",
      "174      -0.530624     -0.343830     -0.542318     -0.713340     -0.717651   \n",
      "137      -0.427632      0.587912      0.116512     -0.160725     -0.634297   \n",
      "153      -0.777795     -0.875153     -0.787457     -0.691724     -0.806001   \n",
      "523      -1.249069     -1.348454     -1.160282     -1.075229     -0.845780   \n",
      "782       1.627941      1.409911      1.510008      1.475109      1.522841   \n",
      "894       0.053851      1.258613      0.770626      0.317250      0.064373   \n",
      "646      -1.002572     -1.511990     -1.230539     -0.927147     -1.061079   \n",
      "740       0.241822      0.456741      0.482043      0.488722      0.474240   \n",
      "628       0.088732     -0.818868     -0.459031     -0.233112     -0.042121   \n",
      "276       0.141988      0.617709      0.380042      0.170951      0.212231   \n",
      "823       0.504265     -0.233691     -0.086360      0.039564      0.173437   \n",
      "845       0.119741      1.120423      0.589276      0.391807      0.317574   \n",
      "356      -0.593313     -0.531932     -0.664283     -0.647530     -0.526615   \n",
      "466       0.514495      0.300611      0.240246      0.238700      0.334423   \n",
      "1070     -2.918446     -3.110891     -3.614643     -3.711234     -4.148053   \n",
      "1024      0.719248      0.384116      0.491378      0.502182      0.574471   \n",
      "1102     -0.010088     -0.343830     -0.240848     -0.170264     -0.097752   \n",
      "70        0.151349      0.464670      0.372108      0.373049      0.355396   \n",
      "532      -0.054093     -0.683950     -0.490617     -0.280974     -0.165436   \n",
      "114       0.381708      1.638542      1.278262      0.938599      0.791862   \n",
      "963       0.206581     -0.087924      0.070211      0.148021      0.262675   \n",
      "822       1.049242      1.451356      1.570869      1.532746      1.477381   \n",
      "855      -0.343785     -0.170075     -0.297297     -0.384817     -0.422747   \n",
      "1075      1.027496      0.690541      0.897227      1.009407      1.081982   \n",
      "307       0.279236     -0.459563     -0.246415     -0.102146      0.051336   \n",
      "220       0.885127      1.029597      1.044756      1.020680      0.973027   \n",
      "787      -0.874359     -0.710303     -0.911491     -1.082849     -1.190206   \n",
      "2         0.180819      0.342746      0.263597      0.149671     -0.115213   \n",
      "697      -0.064146      0.105364     -0.050946      0.078154      0.241051   \n",
      "...            ...           ...           ...           ...           ...   \n",
      "170       0.205661      0.283536      0.183837      0.087359      0.130711   \n",
      "22        0.076387      0.587912      0.116512     -0.149899     -0.296312   \n",
      "202       0.254005     -0.244469      0.025111      0.141956      0.046646   \n",
      "358       1.282628      1.581075      1.395533      1.242509      1.240912   \n",
      "1017      0.487001      1.023411      0.912083      0.681765      0.561734   \n",
      "147      -0.510883     -0.412562     -0.643383     -0.910070     -0.947484   \n",
      "78       -1.013723     -0.763939     -1.114900     -1.166697     -0.970825   \n",
      "156      -2.918446     -3.817870     -4.502539     -4.501903     -4.331542   \n",
      "486      -0.511093     -0.846837     -0.595523     -0.479596     -0.408884   \n",
      "228      -0.442921     -0.087924     -0.229764     -0.289847     -0.320377   \n",
      "273       0.445792      1.017208      0.828838      0.615671      0.413196   \n",
      "208       0.419653      1.319254      1.082886      0.764656      0.470735   \n",
      "979       0.854439      0.774974      0.811725      0.819630      0.832701   \n",
      "996      -2.918446     -1.813676     -1.570666     -1.371852     -1.016627   \n",
      "480       0.164799     -0.581487     -0.332022     -0.173000      0.013606   \n",
      "691       0.269272     -0.108186      0.070211      0.074688      0.195207   \n",
      "826       0.602090     -0.057865      0.212259      0.437230      0.470515   \n",
      "518      -0.083421     -0.233691     -0.535775     -0.645638     -0.736255   \n",
      "64        0.753965      1.241790      0.874714      0.763210      0.625449   \n",
      "1084      0.688776      0.317558      0.634606      0.749763      0.647970   \n",
      "850      -0.764127     -1.212328     -0.935714     -0.786397     -0.588017   \n",
      "630       0.406637      0.432789      0.419204      0.285532      0.208408   \n",
      "528       0.254888      0.863047      0.784958      0.784756      0.686476   \n",
      "605       0.604474      0.639804      0.643196      0.626035      0.592038   \n",
      "929      -0.041543     -0.710303     -0.575390     -0.418431     -0.247988   \n",
      "639      -1.057821     -0.962276     -0.706816     -0.636217     -0.617209   \n",
      "387      -0.790359     -0.903826     -0.794968     -0.753477     -0.582689   \n",
      "940       0.514495      0.300611      0.240246      0.238700      0.334423   \n",
      "658      -0.054216      0.367658      0.307365      0.238700      0.269710   \n",
      "1035      0.576648      0.030004      0.348097      0.486539      0.330480   \n",
      "\n",
      "      feature-1520  feature-1521  feature-1522  feature-1523  \n",
      "670       0.292304      0.377642      0.436993      0.504130  \n",
      "174      -0.951157     -0.953918     -0.786829     -0.619251  \n",
      "137      -0.624318     -0.574417     -0.611686     -0.590944  \n",
      "153      -0.531880     -0.545035     -0.651855     -0.810741  \n",
      "523      -0.961579     -1.141992     -1.032076     -0.953388  \n",
      "782       1.601106      1.584138      1.630810      1.642173  \n",
      "894       0.003478     -0.029811     -0.079485     -0.030709  \n",
      "646      -0.985926     -0.812654     -0.736341     -0.964528  \n",
      "740       0.381691      0.357258      0.407356      0.180971  \n",
      "628      -0.112774     -0.033928      0.005175      0.054187  \n",
      "276       0.106113      0.082412     -0.107746      0.053722  \n",
      "823       0.162184      0.259482      0.356260      0.453013  \n",
      "845       0.171280      0.084288      0.062991      0.121222  \n",
      "356      -0.758022     -0.775638     -0.872538     -0.832774  \n",
      "466       0.290308      0.332905      0.395400      0.470719  \n",
      "1070     -5.337989     -4.505849     -3.971525     -3.410555  \n",
      "1024      0.680304      0.672620      0.663213      0.700245  \n",
      "1102     -0.019498     -0.227371     -0.167164     -0.057612  \n",
      "70        0.272489      0.203552      0.158077      0.076285  \n",
      "532      -0.062588      0.026089      0.005014     -0.052072  \n",
      "114       0.533943      0.412169      0.392792      0.399261  \n",
      "963       0.243899      0.279890      0.352252      0.414700  \n",
      "822       1.365364      1.274329      1.212867      1.210232  \n",
      "855      -0.462109     -0.360693     -0.367828     -0.398658  \n",
      "1075      1.199977      1.172880      1.119013      1.119666  \n",
      "307       0.050624      0.184666      0.234696      0.202341  \n",
      "220       0.868904      0.834006      0.884612      0.843138  \n",
      "787      -1.054732     -1.047140     -1.082761     -0.983111  \n",
      "2        -0.072028      0.007996      0.037094      0.097767  \n",
      "697       0.438442      0.376540      0.102555      0.051392  \n",
      "...            ...           ...           ...           ...  \n",
      "170       0.161041      0.158100      0.148446      0.246714  \n",
      "22       -0.178160     -0.105253     -0.081438      0.006884  \n",
      "202       0.123520      0.195705      0.140692      0.207945  \n",
      "358       1.333942      1.273429      1.296535      1.291719  \n",
      "1017      0.495213      0.421685      0.416100      0.443443  \n",
      "147      -0.832604     -0.832386     -0.756214     -0.622827  \n",
      "78       -0.718969     -0.604951     -0.761615     -0.952929  \n",
      "156      -3.798316     -3.426586     -3.971525     -3.410555  \n",
      "486      -0.491444     -0.367238     -0.378334     -0.467608  \n",
      "228      -0.463206     -0.475270     -0.449123     -0.461025  \n",
      "273       0.384298      0.378193      0.351358      0.425250  \n",
      "208       0.327066      0.342138      0.329007      0.343588  \n",
      "979       0.739376      0.728813      0.780705      0.812371  \n",
      "996      -1.183252     -0.984473     -0.954415     -0.851462  \n",
      "480       0.060628      0.115700      0.154396      0.191844  \n",
      "691       0.117553      0.093799      0.063877      0.212968  \n",
      "826       0.650514      0.670046      0.725065      0.635364  \n",
      "518      -0.453372     -0.354201     -0.304705     -0.174133  \n",
      "64        0.775979      0.775853      0.819276      0.747271  \n",
      "1084      0.688330      0.695774      0.742555      0.646727  \n",
      "850      -0.543356     -0.669513     -0.795687     -0.762167  \n",
      "630       0.279589      0.257234      0.284259      0.359294  \n",
      "528       0.598313      0.555732      0.536995      0.315001  \n",
      "605       0.568399      0.614791      0.636312      0.549384  \n",
      "929      -0.452648     -0.245557     -0.142999     -0.093046  \n",
      "639      -0.703313     -0.585411     -0.607907     -0.851462  \n",
      "387      -0.588182     -0.867487     -0.963298     -0.808780  \n",
      "940       0.290308      0.332905      0.395400      0.470719  \n",
      "658       0.127675      0.028896      0.036582      0.011788  \n",
      "1035      0.430600      0.459752      0.565864      0.494764  \n",
      "\n",
      "[221 rows x 1524 columns],         y\n",
      "670   1.0\n",
      "174   0.0\n",
      "137   0.0\n",
      "153   0.0\n",
      "523   1.0\n",
      "782   1.0\n",
      "894   1.0\n",
      "646   1.0\n",
      "740   1.0\n",
      "628   1.0\n",
      "276   0.0\n",
      "823   1.0\n",
      "845   1.0\n",
      "356   1.0\n",
      "466   1.0\n",
      "1070  1.0\n",
      "1024  1.0\n",
      "1102  1.0\n",
      "70    0.0\n",
      "532   1.0\n",
      "114   0.0\n",
      "963   1.0\n",
      "822   1.0\n",
      "855   1.0\n",
      "1075  1.0\n",
      "307   0.0\n",
      "220   0.0\n",
      "787   1.0\n",
      "2     0.0\n",
      "697   1.0\n",
      "...   ...\n",
      "170   0.0\n",
      "22    0.0\n",
      "202   0.0\n",
      "358   1.0\n",
      "1017  1.0\n",
      "147   0.0\n",
      "78    0.0\n",
      "156   0.0\n",
      "486   1.0\n",
      "228   0.0\n",
      "273   0.0\n",
      "208   0.0\n",
      "979   1.0\n",
      "996   1.0\n",
      "480   1.0\n",
      "691   1.0\n",
      "826   1.0\n",
      "518   1.0\n",
      "64    0.0\n",
      "1084  1.0\n",
      "850   1.0\n",
      "630   1.0\n",
      "528   1.0\n",
      "605   1.0\n",
      "929   1.0\n",
      "639   1.0\n",
      "387   1.0\n",
      "940   1.0\n",
      "658   1.0\n",
      "1035  1.0\n",
      "\n",
      "[221 rows x 1 columns]], select_ftrs__verbose=True \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter dropout_rate for estimator LassoFS(C=1.0, N=1). Check the list of available parameters with `estimator.get_params().keys()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-a464fe6c3017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m all_scores['Lasso'] = evaluate( None, LassoFS(),\n\u001b[0;32m---> 15\u001b[0;31m                              param_grid, data_X, data_y.y)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-262-986e6c74992c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(main_model, feature_selector, param_grid, X, y, scoring, caching)\u001b[0m\n\u001b[1;32m     16\u001b[0m                            \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                            verbose=100, scoring=scoring)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mtrain_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m_set_params\u001b[0;34m(self, attr, **params)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 3. Step parameters and other initilisation arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BaseComposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_params\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnested_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0mvalid_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msub_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                  \u001b[0;34m'Check the list of available parameters '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                  \u001b[0;34m'with `estimator.get_params().keys()`.'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                                  (key, self))\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid parameter dropout_rate for estimator LassoFS(C=1.0, N=1). Check the list of available parameters with `estimator.get_params().keys()`."
     ]
    }
   ],
   "source": [
    "##### Pure Lasso #####\n",
    "param_grid = [\n",
    "    {\n",
    "     'select_ftrs__N':  NUM_FEATURES,\n",
    "     'select_ftrs__C': np.arange(0.25, 1., 0.25) \n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['Lasso'] = evaluate( None, LassoFS(),\n",
    "                             param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdf = DFS(num_epochs=100, verbose=True, dropout_rate=0.5, lambda1=0., alpha1=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:13:57.589938Z",
     "start_time": "2018-05-02T00:04:39.517901Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== (1105, 225)\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.9004524886877828, total=  26.2s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   26.2s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.9004524886877828, total=  25.6s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.9s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8823529411764706, total=  25.7s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8461538461538461, total=  25.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=10, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8914027149321267, total=  25.5s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.1min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8914027149321267, total=  27.8s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.9095022624434389, total=  26.0s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8733031674208145, total=  26.1s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  3.5min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8416289592760181, total=  25.7s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  3.9min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=15, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8959276018099548, total=  25.8s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.3min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8959276018099548, total=  25.8s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  4.8min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.9049773755656109, total=  26.0s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  5.2min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8778280542986425, total=  29.1s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  5.7min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8642533936651584, total=  26.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  6.1min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=20, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8687782805429864, total=  25.3s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  6.5min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8868778280542986, total=  26.1s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  7.0min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.9049773755656109, total=  26.0s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  7.4min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8914027149321267, total=  26.3s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  7.9min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8552036199095022, total=  26.1s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  8.3min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__N=25, select_ftrs__dropout_rate=0.25, select_ftrs__lambda1=0.07, select_ftrs__layers_sizes=[64, 64, 32, 2], select_ftrs__num_epochs=100, score=0.8823529411764706, total=  28.1s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  8.8min finished\n"
     ]
    }
   ],
   "source": [
    "##### DFS + MLP #####\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_ftrs__layers_sizes': [[64, 64, 32, 2]],\n",
    "        'select_ftrs__N': NUM_FEATURES,\n",
    "        'select_ftrs__lambda1': [0.07],\n",
    "        'select_ftrs__num_epochs': [100],\n",
    "        'select_ftrs__dropout_rate': [0.25],\n",
    "        \n",
    "        'main__num_epochs': [100],\n",
    "        'main__dropout_rate': [0.5],\n",
    "        'main__lambda1': [0.],\n",
    "        'main__alpha1': [0.]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['DFS + MLP'] = evaluate(DFS(), DFS(),\n",
    "                                   param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:18:14.108488Z",
     "start_time": "2018-05-02T00:13:57.599086Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== (1105, 225)\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10, score=0.8461538461538461, total=  12.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   12.0s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10, score=0.8778280542986425, total=  11.9s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   24.0s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10, score=0.9095022624434389, total=  11.9s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   35.9s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10, score=0.8733031674208145, total=  11.7s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   47.6s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=10, score=0.8687782805429864, total=  11.8s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   59.5s remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15, score=0.8552036199095022, total=  12.1s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  1.2min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15, score=0.8687782805429864, total=  11.7s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  1.4min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15, score=0.8868778280542986, total=  11.6s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  1.6min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15, score=0.8778280542986425, total=  11.8s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  1.8min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=15, score=0.8642533936651584, total=  11.8s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.0min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20, score=0.8642533936651584, total=  12.0s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  2.2min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20, score=0.8642533936651584, total=  14.5s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  2.4min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20, score=0.8642533936651584, total=  11.8s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  2.6min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20, score=0.8597285067873304, total=  12.0s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:  2.8min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=20, score=0.8733031674208145, total=  11.9s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25, score=0.8597285067873304, total=  12.4s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25, score=0.8733031674208145, total=  12.2s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25, score=0.8642533936651584, total=  12.1s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25, score=0.8597285067873304, total=  12.2s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25 \n",
      "[CV]  main__alpha1=0.0, main__dropout_rate=0.5, main__lambda1=0.0, main__num_epochs=100, select_ftrs__C=1.0, select_ftrs__N=25, score=0.8506787330316742, total=  12.1s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  4.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:  4.0min finished\n"
     ]
    }
   ],
   "source": [
    "##### Lasso + MLP #####\n",
    "param_grid = [\n",
    "    {\n",
    "        'select_ftrs__N': NUM_FEATURES,\n",
    "        'select_ftrs__C': [1.],\n",
    "        \n",
    "        'main__num_epochs': [100],\n",
    "        'main__dropout_rate': [0.5],\n",
    "        'main__lambda1': [0.],\n",
    "        'main__alpha1': [0.]\n",
    "    }\n",
    "]\n",
    "\n",
    "all_scores['Lasso + MLP'] = evaluate(DFS(), LassoFS(),\n",
    "                                   param_grid, data_X, data_y.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:18:19.615070Z",
     "start_time": "2018-05-02T00:18:19.607173Z"
    }
   },
   "outputs": [],
   "source": [
    "colors = {'DFS + RF': 'blue',\n",
    "          'DFS + GB': 'green',\n",
    "          'Lasso + RF': 'red',\n",
    "          'Lasso + GB': 'pink',\n",
    "          'DFS': 'brown',\n",
    "          'Lasso': 'orange',\n",
    "          'DFS + MLP': 'greenyellow',\n",
    "          'Lasso + MLP': 'cyan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:18:20.717710Z",
     "start_time": "2018-05-02T00:18:20.609214Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_test_scores(all_scores, experiment):\n",
    "    param_id = dict()\n",
    "    cur_results = all_scores[experiment].cv_results_\n",
    "\n",
    "    for i, params in enumerate(cur_results['params']):\n",
    "        key = params['select_ftrs__N']\n",
    "        if key in param_id:\n",
    "            param_id[key].append(i)\n",
    "        else:\n",
    "            param_id[key] = [i]\n",
    "\n",
    "    max_test_score = {}\n",
    "    all_test_scores = {}\n",
    "    for ts in param_id.keys():\n",
    "        all_test_scores[ts] = cur_results['mean_test_score'][param_id[ts]]\n",
    "        max_test_score[ts] = max(all_test_scores[ts])\n",
    "    \n",
    "    return max_test_score, all_test_scores\n",
    "\n",
    "def plot(all_scores):\n",
    "    p = figure(plot_width=1000)\n",
    "    for i, score_title in enumerate(all_scores):\n",
    "        max_scores, _ = get_test_scores(all_scores, score_title)\n",
    "        \n",
    "        p.line(x = list(max_scores.keys()), \n",
    "               y = list(max_scores.values()),\n",
    "               legend=score_title,\n",
    "               line_color=colors[score_title])\n",
    "        p.circle(x = list(max_scores.keys()), \n",
    "               y = list(max_scores.values()),\n",
    "               legend=score_title,\n",
    "               color=colors[score_title],\n",
    "               alpha=1.)\n",
    "        \n",
    "    \n",
    "    p.legend.location = 'bottom_right'\n",
    "    p.legend.click_policy = 'hide'\n",
    "\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T00:18:21.651902Z",
     "start_time": "2018-05-02T00:18:21.439567Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"9d921229-5ae4-4f31-944a-5e3bf22b2433\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"8db0bed2-4f15-4305-98e4-e656b4d3ea6e\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"b8f910d4-024c-407d-bd76-341d919fcbe7\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_color\":{\"value\":\"cyan\"},\"line_color\":{\"value\":\"cyan\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"6c337f26-45c4-48d0-9998-7de1509a0dc6\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"fa9970b8-9273-44e3-8c45-8dfb08ff3fc7\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"e836146c-bebe-43bd-a0e2-44a32134acdb\",\"type\":\"ColumnDataSource\"}},\"id\":\"840f57b5-9923-4757-95e9-9b4c0dfae2be\",\"type\":\"CDSView\"},{\"attributes\":{\"plot\":{\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2ed4784d-b18b-4473-8a76-78941512c97b\",\"type\":\"BasicTicker\"}},\"id\":\"bd62edb3-2837-41ef-b0c8-c8b6c4cbf0e1\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"c37c3761-546f-49f8-8620-777b32168bac\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2ed4784d-b18b-4473-8a76-78941512c97b\",\"type\":\"BasicTicker\"}},\"id\":\"26e5506c-1d6a-4c25-a51b-b4cde99c31fb\",\"type\":\"LinearAxis\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + MLP\"},\"renderers\":[{\"id\":\"5ff0dc6a-b2ea-4978-a062-50bad0452f9a\",\"type\":\"GlyphRenderer\"},{\"id\":\"f4e8842e-2e60-4128-b1c4-1066518f5477\",\"type\":\"GlyphRenderer\"}]},\"id\":\"66e39bd8-d821-48a2-8330-ae99e8502aca\",\"type\":\"LegendItem\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"e64ddf33-e742-4b10-965d-11f3c40aa2a0\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"2ed4784d-b18b-4473-8a76-78941512c97b\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,15,20,25],\"y\":[0.8751131221719457,0.8705882352941177,0.865158371040724,0.8615384615384616]}},\"id\":\"e836146c-bebe-43bd-a0e2-44a32134acdb\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"5dfedeaa-562d-4630-948d-9e88ac00988a\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"06559cd2-76fb-4d73-84b2-cff803674559\",\"type\":\"BasicTicker\"}},\"id\":\"67040703-4a77-4aa6-89b5-a9a2fa75d875\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"9a5f5788-5439-4fe5-bb77-90e72691acc6\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"06559cd2-76fb-4d73-84b2-cff803674559\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"ebd138fc-0467-4b3d-bb5c-952d18bd28bd\",\"type\":\"ColumnDataSource\"}},\"id\":\"8473f516-f950-4fb4-974b-2dfa5ff2ef9e\",\"type\":\"CDSView\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"06559cd2-76fb-4d73-84b2-cff803674559\",\"type\":\"BasicTicker\"}},\"id\":\"9f4a85e3-7807-46d7-92ce-7d46ff819af0\",\"type\":\"Grid\"},{\"attributes\":{\"label\":{\"value\":\"DFS + MLP\"},\"renderers\":[{\"id\":\"2f0eea42-d30b-40c1-bef5-5c9382eb7199\",\"type\":\"GlyphRenderer\"},{\"id\":\"af2e5737-5977-426d-87da-68fe80e9ab38\",\"type\":\"GlyphRenderer\"}]},\"id\":\"98ae9300-0eb2-408c-ba9a-836b0b99af95\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"ebd138fc-0467-4b3d-bb5c-952d18bd28bd\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"6c337f26-45c4-48d0-9998-7de1509a0dc6\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"9a5f5788-5439-4fe5-bb77-90e72691acc6\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"8473f516-f950-4fb4-974b-2dfa5ff2ef9e\",\"type\":\"CDSView\"}},\"id\":\"f4e8842e-2e60-4128-b1c4-1066518f5477\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,15,20,25],\"y\":[0.8751131221719457,0.8705882352941177,0.865158371040724,0.8615384615384616]}},\"id\":\"ebd138fc-0467-4b3d-bb5c-952d18bd28bd\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"896fc267-6385-4569-9938-02c765217393\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"4f2f8f94-6235-4b28-855e-b478fbb14d82\",\"type\":\"SaveTool\"},{\"attributes\":{\"source\":{\"id\":\"17078dc1-5552-4569-b347-9fe144392dd9\",\"type\":\"ColumnDataSource\"}},\"id\":\"4dc54f4d-6871-4502-8042-dc3ba06f1c6d\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"c37c3761-546f-49f8-8620-777b32168bac\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"3a234c4d-7ebb-4050-afc6-2036e427aa82\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"data_source\":{\"id\":\"78e5e24a-f18d-4fac-818a-ab2a6c9faf31\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"030ee9b4-2888-464f-8238-ada89a28ea16\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"ce57f8ef-20c4-4327-9ad0-d902ab946c2c\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"9acda4c8-c95c-4e0f-9999-fdffac53765d\",\"type\":\"CDSView\"}},\"id\":\"2f0eea42-d30b-40c1-bef5-5c9382eb7199\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,15,20,25],\"y\":[0.8841628959276018,0.8823529411764706,0.8823529411764706,0.8841628959276018]}},\"id\":\"17078dc1-5552-4569-b347-9fe144392dd9\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"016bc084-4279-4508-be23-20cfea9277ce\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"line_color\":\"greenyellow\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"030ee9b4-2888-464f-8238-ada89a28ea16\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"ce57f8ef-20c4-4327-9ad0-d902ab946c2c\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"a517972a-77fb-4e28-bf4b-eb11f55036ef\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"5dfedeaa-562d-4630-948d-9e88ac00988a\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_color\":\"cyan\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"b4f002d3-a2ff-419f-91c4-f33580fd8c8b\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"78e5e24a-f18d-4fac-818a-ab2a6c9faf31\",\"type\":\"ColumnDataSource\"}},\"id\":\"9acda4c8-c95c-4e0f-9999-fdffac53765d\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"cb716dc6-f2e6-423f-ad21-a15602d435d6\",\"type\":\"PanTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"97c86505-762c-4ef5-9c3a-11769f9c1057\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"17078dc1-5552-4569-b347-9fe144392dd9\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"0d614f1d-4062-4bcf-86aa-45a8ef91c0bb\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"6da6ed28-a6d6-4a07-99c3-e90e971128a7\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"4dc54f4d-6871-4502-8042-dc3ba06f1c6d\",\"type\":\"CDSView\"}},\"id\":\"af2e5737-5977-426d-87da-68fe80e9ab38\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"overlay\":{\"id\":\"016bc084-4279-4508-be23-20cfea9277ce\",\"type\":\"BoxAnnotation\"}},\"id\":\"708428ea-24ae-4a9d-9256-ae55ee7e024f\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"6da6ed28-a6d6-4a07-99c3-e90e971128a7\",\"type\":\"Circle\"},{\"attributes\":{\"below\":[{\"id\":\"26e5506c-1d6a-4c25-a51b-b4cde99c31fb\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"67040703-4a77-4aa6-89b5-a9a2fa75d875\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"26e5506c-1d6a-4c25-a51b-b4cde99c31fb\",\"type\":\"LinearAxis\"},{\"id\":\"bd62edb3-2837-41ef-b0c8-c8b6c4cbf0e1\",\"type\":\"Grid\"},{\"id\":\"67040703-4a77-4aa6-89b5-a9a2fa75d875\",\"type\":\"LinearAxis\"},{\"id\":\"9f4a85e3-7807-46d7-92ce-7d46ff819af0\",\"type\":\"Grid\"},{\"id\":\"016bc084-4279-4508-be23-20cfea9277ce\",\"type\":\"BoxAnnotation\"},{\"id\":\"e11f803a-2075-434e-92df-de488a62b6d2\",\"type\":\"Legend\"},{\"id\":\"2f0eea42-d30b-40c1-bef5-5c9382eb7199\",\"type\":\"GlyphRenderer\"},{\"id\":\"af2e5737-5977-426d-87da-68fe80e9ab38\",\"type\":\"GlyphRenderer\"},{\"id\":\"5ff0dc6a-b2ea-4978-a062-50bad0452f9a\",\"type\":\"GlyphRenderer\"},{\"id\":\"f4e8842e-2e60-4128-b1c4-1066518f5477\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"e64ddf33-e742-4b10-965d-11f3c40aa2a0\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"94d1232d-8916-4a37-a370-83157806371b\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"cf4e6654-7469-4a56-a1b9-aac454f809ab\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"b8f910d4-024c-407d-bd76-341d919fcbe7\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"cedfb295-53ef-48d5-8021-7cdab14d8fc1\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"fa9970b8-9273-44e3-8c45-8dfb08ff3fc7\",\"type\":\"LinearScale\"}},\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"click_policy\":\"hide\",\"items\":[{\"id\":\"98ae9300-0eb2-408c-ba9a-836b0b99af95\",\"type\":\"LegendItem\"},{\"id\":\"66e39bd8-d821-48a2-8330-ae99e8502aca\",\"type\":\"LegendItem\"}],\"location\":\"bottom_right\",\"plot\":{\"id\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"e11f803a-2075-434e-92df-de488a62b6d2\",\"type\":\"Legend\"},{\"attributes\":{\"data_source\":{\"id\":\"e836146c-bebe-43bd-a0e2-44a32134acdb\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"b4f002d3-a2ff-419f-91c4-f33580fd8c8b\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"97c86505-762c-4ef5-9c3a-11769f9c1057\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"840f57b5-9923-4757-95e9-9b4c0dfae2be\",\"type\":\"CDSView\"}},\"id\":\"5ff0dc6a-b2ea-4978-a062-50bad0452f9a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cb716dc6-f2e6-423f-ad21-a15602d435d6\",\"type\":\"PanTool\"},{\"id\":\"3a234c4d-7ebb-4050-afc6-2036e427aa82\",\"type\":\"WheelZoomTool\"},{\"id\":\"708428ea-24ae-4a9d-9256-ae55ee7e024f\",\"type\":\"BoxZoomTool\"},{\"id\":\"4f2f8f94-6235-4b28-855e-b478fbb14d82\",\"type\":\"SaveTool\"},{\"id\":\"896fc267-6385-4569-9938-02c765217393\",\"type\":\"ResetTool\"},{\"id\":\"a517972a-77fb-4e28-bf4b-eb11f55036ef\",\"type\":\"HelpTool\"}]},\"id\":\"94d1232d-8916-4a37-a370-83157806371b\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null},\"id\":\"cf4e6654-7469-4a56-a1b9-aac454f809ab\",\"type\":\"DataRange1d\"},{\"attributes\":{\"fill_color\":{\"value\":\"greenyellow\"},\"line_color\":{\"value\":\"greenyellow\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"0d614f1d-4062-4bcf-86aa-45a8ef91c0bb\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null},\"id\":\"cedfb295-53ef-48d5-8021-7cdab14d8fc1\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,15,20,25],\"y\":[0.8841628959276018,0.8823529411764706,0.8823529411764706,0.8841628959276018]}},\"id\":\"78e5e24a-f18d-4fac-818a-ab2a6c9faf31\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"4659ecc6-f980-432c-b157-477ba8dda2d4\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.14\"}};\n",
       "  var render_items = [{\"docid\":\"8db0bed2-4f15-4305-98e4-e656b4d3ea6e\",\"elementid\":\"9d921229-5ae4-4f31-944a-5e3bf22b2433\",\"modelid\":\"4659ecc6-f980-432c-b157-477ba8dda2d4\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "4659ecc6-f980-432c-b157-477ba8dda2d4"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T15:14:13.469534Z",
     "start_time": "2018-05-01T15:14:12.639167Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"e8aebb8e-4768-445f-a383-d4c89e6f992e\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"9666f35b-7ca2-46a1-8be2-d912222f2a12\":{\"roots\":{\"references\":[{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"f687dc45-7ccf-4438-864f-e935bf4a9273\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"d6c45417-e219-411c-95e4-a741b4b3e05b\",\"type\":\"ColumnDataSource\"}},\"id\":\"5564b93a-60a1-46d1-be1c-b21f2edc3ed3\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"d6c45417-e219-411c-95e4-a741b4b3e05b\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"dfca7523-74e8-4f2c-9807-9f5e1994b603\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"c1956427-7895-4d03-ad70-dd18a923b193\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"5564b93a-60a1-46d1-be1c-b21f2edc3ed3\",\"type\":\"CDSView\"}},\"id\":\"5db103a0-6acf-40f2-a479-50893cdbbd30\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_color\":\"blue\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"de5d4f27-2f0f-4c6a-a0ff-5e7c7aedfd10\",\"type\":\"Line\"},{\"attributes\":{\"fill_color\":{\"value\":\"green\"},\"line_color\":{\"value\":\"green\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"ee22c741-1e6c-465d-bc5f-3e26d36e9e42\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"2f233339-ff52-4190-a304-9affa54571c9\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8669683257918552,0.8751131221719457,0.8506787330316742,0.869683257918552,0.8542986425339366,0.853393665158371]}},\"id\":\"0c2f1791-aad8-48d7-b83f-333f75e27060\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199]}},\"id\":\"d6c45417-e219-411c-95e4-a741b4b3e05b\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8678733031674208,0.8769230769230769,0.8705882352941177,0.8760180995475113,0.8723981900452489,0.8778280542986425]}},\"id\":\"5bc8d3e8-e7d1-4ffd-8026-7a60e235bfe2\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"b113458e-c1e1-457e-b187-2d3baa4a7652\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5871cdf8-849f-4613-9563-a6d97506ae29\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"7a78fc10-29ee-4463-babc-83be04f2e0cf\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"ef3a70ee-9e5d-40e6-8de7-618b477f6bf1\",\"type\":\"CDSView\"}},\"id\":\"3b1bd088-b48a-461b-ad32-e8742da26c3d\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"e70aee1c-bfda-43c0-93d2-5f26a1cb1ea7\",\"type\":\"PanTool\"},{\"id\":\"8dcceb44-c8b8-4ce6-90e6-6bb0a67db2ec\",\"type\":\"WheelZoomTool\"},{\"id\":\"c5b2496c-fcde-4588-b70e-3e5f31fc9b3c\",\"type\":\"BoxZoomTool\"},{\"id\":\"083bae5a-ff39-43cb-9ce9-424be0df4d36\",\"type\":\"SaveTool\"},{\"id\":\"8795cafb-b9d3-429a-981d-c27bcff4ebb3\",\"type\":\"ResetTool\"},{\"id\":\"ab3621d5-ad97-4b51-883d-40e1140d1841\",\"type\":\"HelpTool\"}]},\"id\":\"3d42bfa9-20e1-4a8f-acec-ac2c92625ab0\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":{\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c4010190-9857-4c5f-bd76-54dc4f78c82e\",\"type\":\"BasicTicker\"}},\"id\":\"23d82baa-8ae1-4f15-9e4a-545ec26f2237\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"3a7f00ef-0cb7-4e0f-9576-5d694531cef3\",\"type\":\"ColumnDataSource\"}},\"id\":\"91c0c668-9149-4bba-a6fd-37f093a046be\",\"type\":\"CDSView\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"0c2f60db-c49b-47a3-bc89-19ba640ec430\",\"type\":\"BasicTicker\"}},\"id\":\"c224bcb7-8a58-4aaf-a211-045aa6240e01\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.853393665158371,0.865158371040724,0.8705882352941177,0.8678733031674208,0.8660633484162896,0.8642533936651584]}},\"id\":\"5e07e9ba-f0b4-4165-83be-fd1c01156d8a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"f6113afd-12e7-42f9-93e8-2bd5fe607d4a\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"0c2f60db-c49b-47a3-bc89-19ba640ec430\",\"type\":\"BasicTicker\"}},\"id\":\"6f44cb65-b6fa-476f-9286-e8d4140b86d8\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_color\":{\"value\":\"orange\"},\"line_color\":{\"value\":\"orange\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"dfca7523-74e8-4f2c-9807-9f5e1994b603\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":\"brown\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3e5f009f-cbe6-4384-b104-60e3acb48d69\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"c4010190-9857-4c5f-bd76-54dc4f78c82e\",\"type\":\"BasicTicker\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8678733031674208,0.8769230769230769,0.8705882352941177,0.8760180995475113,0.8723981900452489,0.8778280542986425]}},\"id\":\"eee329db-242d-43c5-99a2-efe02bf95fd2\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"4864000a-abcd-41c4-a8da-cc421ecb07a8\",\"type\":\"Line\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3fd7d0be-08ad-40f8-912f-3eec8acdb0b5\",\"type\":\"Circle\"},{\"attributes\":{\"source\":{\"id\":\"b113458e-c1e1-457e-b187-2d3baa4a7652\",\"type\":\"ColumnDataSource\"}},\"id\":\"ef3a70ee-9e5d-40e6-8de7-618b477f6bf1\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null},\"id\":\"4793c36f-3e3d-4778-92cc-483d465be386\",\"type\":\"DataRange1d\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"b8c7cffe-8721-4675-bd63-312bd89c3634\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"red\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"661e0ec4-0df1-4bee-b1ab-eadbc002bc7a\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"e70aee1c-bfda-43c0-93d2-5f26a1cb1ea7\",\"type\":\"PanTool\"},{\"attributes\":{\"source\":{\"id\":\"f88c70b9-8502-4a74-aeda-b88a25aeb03c\",\"type\":\"ColumnDataSource\"}},\"id\":\"5bb21c55-ffb0-47d2-9e4f-fb4f38cd0172\",\"type\":\"CDSView\"},{\"attributes\":{\"below\":[{\"id\":\"01e39dfb-c79a-479c-b319-a7f2a1f171cd\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"6f44cb65-b6fa-476f-9286-e8d4140b86d8\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"01e39dfb-c79a-479c-b319-a7f2a1f171cd\",\"type\":\"LinearAxis\"},{\"id\":\"23d82baa-8ae1-4f15-9e4a-545ec26f2237\",\"type\":\"Grid\"},{\"id\":\"6f44cb65-b6fa-476f-9286-e8d4140b86d8\",\"type\":\"LinearAxis\"},{\"id\":\"c224bcb7-8a58-4aaf-a211-045aa6240e01\",\"type\":\"Grid\"},{\"id\":\"a585864f-a390-430f-8c5b-e9890f228a30\",\"type\":\"BoxAnnotation\"},{\"id\":\"b8b59d1c-81e8-4e9c-9d6e-4d3c9dc7004e\",\"type\":\"Legend\"},{\"id\":\"8d899cec-48ab-4892-8adb-0ca9cdda2199\",\"type\":\"GlyphRenderer\"},{\"id\":\"3b1bd088-b48a-461b-ad32-e8742da26c3d\",\"type\":\"GlyphRenderer\"},{\"id\":\"e39dc771-4ec8-4cd9-b2bb-e462ac8be89a\",\"type\":\"GlyphRenderer\"},{\"id\":\"a68611d8-dade-47ea-abb0-1ac088ef63c2\",\"type\":\"GlyphRenderer\"},{\"id\":\"4773ae9d-b866-4b8f-8788-f04f4a4b353f\",\"type\":\"GlyphRenderer\"},{\"id\":\"093d6b0f-b7d9-4576-b538-0d45d99e99b0\",\"type\":\"GlyphRenderer\"},{\"id\":\"78b86275-6dc1-4717-9a45-ed9c0ab9b52c\",\"type\":\"GlyphRenderer\"},{\"id\":\"2881ddb4-951a-4228-9a88-17e52fbb7a1f\",\"type\":\"GlyphRenderer\"},{\"id\":\"a9624280-cfeb-42a1-ae99-c7e702021177\",\"type\":\"GlyphRenderer\"},{\"id\":\"c9accb8a-2790-443d-affd-768d7a90e3bf\",\"type\":\"GlyphRenderer\"},{\"id\":\"c0dd4566-04ed-47c3-baaa-66a698e3426b\",\"type\":\"GlyphRenderer\"},{\"id\":\"5db103a0-6acf-40f2-a479-50893cdbbd30\",\"type\":\"GlyphRenderer\"},{\"id\":\"b2139dce-de3b-4725-9e61-039a34c36773\",\"type\":\"GlyphRenderer\"},{\"id\":\"afcd7b4f-0145-49da-9045-713ca4136be7\",\"type\":\"GlyphRenderer\"},{\"id\":\"c64ebe7c-e4c8-4176-b4da-07a7d82be974\",\"type\":\"GlyphRenderer\"},{\"id\":\"1951b5b8-1d33-4246-82ef-639ff71d07d6\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"e109bafd-0180-4705-a981-324bec21f282\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"3d42bfa9-20e1-4a8f-acec-ac2c92625ab0\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"6546b4f7-f579-4858-93b9-808221492c08\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"80767432-ca7d-41d3-a8df-56cbcfab8e74\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"4793c36f-3e3d-4778-92cc-483d465be386\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"cb32c0ab-a53f-4da3-b2c1-046ff4d5c7be\",\"type\":\"LinearScale\"}},\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"source\":{\"id\":\"edb50718-27e9-420c-bb34-72ff15a6ac78\",\"type\":\"ColumnDataSource\"}},\"id\":\"4f8c161b-9ee1-4a4f-bab5-58c9386fec24\",\"type\":\"CDSView\"},{\"attributes\":{\"label\":{\"value\":\"Lasso\"},\"renderers\":[{\"id\":\"c0dd4566-04ed-47c3-baaa-66a698e3426b\",\"type\":\"GlyphRenderer\"},{\"id\":\"5db103a0-6acf-40f2-a479-50893cdbbd30\",\"type\":\"GlyphRenderer\"}]},\"id\":\"665c3b4a-5259-41a2-917a-263d1d1ff430\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"6496817c-2ce8-4cf2-bbbe-4345d684e115\",\"type\":\"ColumnDataSource\"}},\"id\":\"42c7387f-be42-4110-8dc1-a0082b1271f9\",\"type\":\"CDSView\"},{\"attributes\":{\"data_source\":{\"id\":\"c707385f-acaa-443b-a636-720879252f0a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"df0240ef-96a1-44b5-913d-d039c038f4cd\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"343656f9-5cdd-4b45-8bdd-99e116c75276\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"ec2237ea-f0a7-4df4-85a1-7ae50aa357c9\",\"type\":\"CDSView\"}},\"id\":\"afcd7b4f-0145-49da-9045-713ca4136be7\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"click_policy\":\"hide\",\"items\":[{\"id\":\"b9b6e23a-91cb-42e9-bbd8-be2e294d710b\",\"type\":\"LegendItem\"},{\"id\":\"e5600a0b-e749-49dd-b20a-801a4710b485\",\"type\":\"LegendItem\"},{\"id\":\"c2330b33-e285-4f49-90fa-53268408e09a\",\"type\":\"LegendItem\"},{\"id\":\"c49c2d24-c7f2-475d-a336-9995c246a3e7\",\"type\":\"LegendItem\"},{\"id\":\"e57a1274-9b03-42e8-93c3-619b43beeb60\",\"type\":\"LegendItem\"},{\"id\":\"665c3b4a-5259-41a2-917a-263d1d1ff430\",\"type\":\"LegendItem\"},{\"id\":\"f81d4389-60b3-435d-9d71-5d2f7e70f766\",\"type\":\"LegendItem\"},{\"id\":\"31d95ce2-43fc-45b2-8f84-0629683f2921\",\"type\":\"LegendItem\"}],\"location\":\"bottom_right\",\"plot\":{\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"b8b59d1c-81e8-4e9c-9d6e-4d3c9dc7004e\",\"type\":\"Legend\"},{\"attributes\":{\"data_source\":{\"id\":\"5bc8d3e8-e7d1-4ffd-8026-7a60e235bfe2\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"3538c43a-dbb2-426d-ba87-dc31a1323e18\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"46bbb1a0-cd36-4782-93d5-d4a7547101f0\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"c964d3e4-19bd-4b66-baee-b1318eaae1b2\",\"type\":\"CDSView\"}},\"id\":\"78b86275-6dc1-4717-9a45-ed9c0ab9b52c\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8669683257918552,0.8751131221719457,0.8506787330316742,0.869683257918552,0.8542986425339366,0.853393665158371]}},\"id\":\"b113458e-c1e1-457e-b187-2d3baa4a7652\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"3a7f00ef-0cb7-4e0f-9576-5d694531cef3\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"83c46e7b-7965-44f6-9028-f8a14d27baa9\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"b8c7cffe-8721-4675-bd63-312bd89c3634\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"91c0c668-9149-4bba-a6fd-37f093a046be\",\"type\":\"CDSView\"}},\"id\":\"e39dc771-4ec8-4cd9-b2bb-e462ac8be89a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"0c2f60db-c49b-47a3-bc89-19ba640ec430\",\"type\":\"BasicTicker\"},{\"attributes\":{\"source\":{\"id\":\"eee329db-242d-43c5-99a2-efe02bf95fd2\",\"type\":\"ColumnDataSource\"}},\"id\":\"7d785d88-0a57-4732-8f5e-881af2cc6da7\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"8795cafb-b9d3-429a-981d-c27bcff4ebb3\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"red\"},\"line_color\":{\"value\":\"red\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"929ae91f-0cff-4c3e-9775-06b3005814ba\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"8dcceb44-c8b8-4ce6-90e6-6bb0a67db2ec\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"pink\"},\"line_color\":{\"value\":\"pink\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3ec144fa-0106-4343-87df-8e98e671942c\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8615384615384616,0.8552036199095022,0.857918552036199,0.8615384615384616,0.860633484162896,0.8570135746606334]}},\"id\":\"edb50718-27e9-420c-bb34-72ff15a6ac78\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"e109bafd-0180-4705-a981-324bec21f282\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"eee329db-242d-43c5-99a2-efe02bf95fd2\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"3ec144fa-0106-4343-87df-8e98e671942c\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"62fe5c1c-8e03-4713-a1c4-2cf9b6eb658e\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"7d785d88-0a57-4732-8f5e-881af2cc6da7\",\"type\":\"CDSView\"}},\"id\":\"2881ddb4-951a-4228-9a88-17e52fbb7a1f\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"b2e2ee91-8bc7-418c-acc8-e0776a108f83\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199,0.857918552036199]}},\"id\":\"f88c70b9-8502-4a74-aeda-b88a25aeb03c\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.853393665158371,0.865158371040724,0.8705882352941177,0.8678733031674208,0.8660633484162896,0.8642533936651584]}},\"id\":\"c707385f-acaa-443b-a636-720879252f0a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"80767432-ca7d-41d3-a8df-56cbcfab8e74\",\"type\":\"LinearScale\"},{\"attributes\":{\"source\":{\"id\":\"5e07e9ba-f0b4-4165-83be-fd1c01156d8a\",\"type\":\"ColumnDataSource\"}},\"id\":\"de739008-fa39-4c36-aa64-a5e86fc04fd1\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_color\":{\"value\":\"brown\"},\"line_color\":{\"value\":\"brown\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3cca5454-46df-455b-8735-d5c02666a7ae\",\"type\":\"Circle\"},{\"attributes\":{\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"5871cdf8-849f-4613-9563-a6d97506ae29\",\"type\":\"Circle\"},{\"attributes\":{\"source\":{\"id\":\"00e18df2-c766-4c5b-95b0-338d0847ec57\",\"type\":\"ColumnDataSource\"}},\"id\":\"476402c8-4229-4814-a5bc-59e0953dfbaa\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"eda8319b-3176-4bd7-990a-16b930cb1815\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8615384615384616,0.860633484162896,0.8642533936651584,0.8642533936651584,0.8633484162895928,0.865158371040724]}},\"id\":\"e106e5fe-ef47-4548-89c6-2ffb779008ba\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"DFS\"},\"renderers\":[{\"id\":\"a9624280-cfeb-42a1-ae99-c7e702021177\",\"type\":\"GlyphRenderer\"},{\"id\":\"c9accb8a-2790-443d-affd-768d7a90e3bf\",\"type\":\"GlyphRenderer\"}]},\"id\":\"e57a1274-9b03-42e8-93c3-619b43beeb60\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"5c0048d9-1892-446c-98ef-ad1fae90348e\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"ee22c741-1e6c-465d-bc5f-3e26d36e9e42\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"3fd7d0be-08ad-40f8-912f-3eec8acdb0b5\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"1b3476cc-fdfc-4c1f-bdcc-90e62a83a3d8\",\"type\":\"CDSView\"}},\"id\":\"a68611d8-dade-47ea-abb0-1ac088ef63c2\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"greenyellow\"},\"line_color\":{\"value\":\"greenyellow\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"df0240ef-96a1-44b5-913d-d039c038f4cd\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8751131221719457,0.890497737556561,0.881447963800905,0.8895927601809954,0.8895927601809954,0.8787330316742081]}},\"id\":\"5c0048d9-1892-446c-98ef-ad1fae90348e\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"cbe7f2f5-facf-4411-81d3-fb300cf691b7\",\"type\":\"ColumnDataSource\"}},\"id\":\"fa2efeb2-e70f-4f4e-9a02-7880de258a7b\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"62fe5c1c-8e03-4713-a1c4-2cf9b6eb658e\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"f6113afd-12e7-42f9-93e8-2bd5fe607d4a\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8615384615384616,0.8552036199095022,0.857918552036199,0.8615384615384616,0.860633484162896,0.8570135746606334]}},\"id\":\"6496817c-2ce8-4cf2-bbbe-4345d684e115\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"orange\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a3dce715-c49e-4d7e-a3e3-e46a3e2937ac\",\"type\":\"Line\"},{\"attributes\":{\"formatter\":{\"id\":\"785bb0c5-c022-4ad3-b937-baa73501f4d0\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"7f664107-64f9-4676-a805-8338163dc9d8\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c4010190-9857-4c5f-bd76-54dc4f78c82e\",\"type\":\"BasicTicker\"}},\"id\":\"01e39dfb-c79a-479c-b319-a7f2a1f171cd\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"ab3621d5-ad97-4b51-883d-40e1140d1841\",\"type\":\"HelpTool\"},{\"attributes\":{\"data_source\":{\"id\":\"72a5af66-863f-4716-b0b7-be1758da480a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"661e0ec4-0df1-4bee-b1ab-eadbc002bc7a\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"4864000a-abcd-41c4-a8da-cc421ecb07a8\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"929041cb-d27a-45a0-9fe8-e1410378cdb4\",\"type\":\"CDSView\"}},\"id\":\"4773ae9d-b866-4b8f-8788-f04f4a4b353f\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"DFS + MLP\"},\"renderers\":[{\"id\":\"b2139dce-de3b-4725-9e61-039a34c36773\",\"type\":\"GlyphRenderer\"},{\"id\":\"afcd7b4f-0145-49da-9045-713ca4136be7\",\"type\":\"GlyphRenderer\"}]},\"id\":\"f81d4389-60b3-435d-9d71-5d2f7e70f766\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a40f61c0-5f1a-4dc7-bfb4-1d468fb34e21\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"f8ffafbb-6093-4ef8-92ee-830675dad534\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8615384615384616,0.860633484162896,0.8642533936651584,0.8642533936651584,0.8633484162895928,0.865158371040724]}},\"id\":\"cbe7f2f5-facf-4411-81d3-fb300cf691b7\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d18292f9-2f8e-4a7c-8985-449a9373da1c\",\"type\":\"Circle\"},{\"attributes\":{},\"id\":\"083bae5a-ff39-43cb-9ce9-424be0df4d36\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"edb50718-27e9-420c-bb34-72ff15a6ac78\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"3e5f009f-cbe6-4384-b104-60e3acb48d69\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"f687dc45-7ccf-4438-864f-e935bf4a9273\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"4f8c161b-9ee1-4a4f-bab5-58c9386fec24\",\"type\":\"CDSView\"}},\"id\":\"a9624280-cfeb-42a1-ae99-c7e702021177\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"f88c70b9-8502-4a74-aeda-b88a25aeb03c\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"a3dce715-c49e-4d7e-a3e3-e46a3e2937ac\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"f8ffafbb-6093-4ef8-92ee-830675dad534\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"5bb21c55-ffb0-47d2-9e4f-fb4f38cd0172\",\"type\":\"CDSView\"}},\"id\":\"c0dd4566-04ed-47c3-baaa-66a698e3426b\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_color\":\"pink\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"3538c43a-dbb2-426d-ba87-dc31a1323e18\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"greenyellow\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a6465d4c-c354-453f-9ca0-88fcd58ec750\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"DFS + GB\"},\"renderers\":[{\"id\":\"e39dc771-4ec8-4cd9-b2bb-e462ac8be89a\",\"type\":\"GlyphRenderer\"},{\"id\":\"a68611d8-dade-47ea-abb0-1ac088ef63c2\",\"type\":\"GlyphRenderer\"}]},\"id\":\"e5600a0b-e749-49dd-b20a-801a4710b485\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"cbe7f2f5-facf-4411-81d3-fb300cf691b7\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"cc688cf7-58ec-424a-9ec7-5c6408a312d8\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"6900ee6c-c679-45ee-8ebd-4fdcf4cde0df\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"fa2efeb2-e70f-4f4e-9a02-7880de258a7b\",\"type\":\"CDSView\"}},\"id\":\"c64ebe7c-e4c8-4176-b4da-07a7d82be974\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"0c2f1791-aad8-48d7-b83f-333f75e27060\",\"type\":\"ColumnDataSource\"}},\"id\":\"47a811c4-8a54-4524-836d-190b8ff63f24\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"785bb0c5-c022-4ad3-b937-baa73501f4d0\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"46bbb1a0-cd36-4782-93d5-d4a7547101f0\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null},\"id\":\"6546b4f7-f579-4858-93b9-808221492c08\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"c707385f-acaa-443b-a636-720879252f0a\",\"type\":\"ColumnDataSource\"}},\"id\":\"ec2237ea-f0a7-4df4-85a1-7ae50aa357c9\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"5bc8d3e8-e7d1-4ffd-8026-7a60e235bfe2\",\"type\":\"ColumnDataSource\"}},\"id\":\"c964d3e4-19bd-4b66-baee-b1318eaae1b2\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"7a78fc10-29ee-4463-babc-83be04f2e0cf\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"0c2f1791-aad8-48d7-b83f-333f75e27060\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"de5d4f27-2f0f-4c6a-a0ff-5e7c7aedfd10\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"a40f61c0-5f1a-4dc7-bfb4-1d468fb34e21\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"47a811c4-8a54-4524-836d-190b8ff63f24\",\"type\":\"CDSView\"}},\"id\":\"8d899cec-48ab-4892-8adb-0ca9cdda2199\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"cyan\"},\"line_color\":{\"value\":\"cyan\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"e9b23ff5-f2d2-48d1-ab3c-3d177bd543f3\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"c1956427-7895-4d03-ad70-dd18a923b193\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"00e18df2-c766-4c5b-95b0-338d0847ec57\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"929ae91f-0cff-4c3e-9775-06b3005814ba\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"b2e2ee91-8bc7-418c-acc8-e0776a108f83\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"476402c8-4229-4814-a5bc-59e0953dfbaa\",\"type\":\"CDSView\"}},\"id\":\"093d6b0f-b7d9-4576-b538-0d45d99e99b0\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"72a5af66-863f-4716-b0b7-be1758da480a\",\"type\":\"ColumnDataSource\"}},\"id\":\"929041cb-d27a-45a0-9fe8-e1410378cdb4\",\"type\":\"CDSView\"},{\"attributes\":{\"label\":{\"value\":\"DFS + RF\"},\"renderers\":[{\"id\":\"8d899cec-48ab-4892-8adb-0ca9cdda2199\",\"type\":\"GlyphRenderer\"},{\"id\":\"3b1bd088-b48a-461b-ad32-e8742da26c3d\",\"type\":\"GlyphRenderer\"}]},\"id\":\"b9b6e23a-91cb-42e9-bbd8-be2e294d710b\",\"type\":\"LegendItem\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"a585864f-a390-430f-8c5b-e9890f228a30\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + RF\"},\"renderers\":[{\"id\":\"4773ae9d-b866-4b8f-8788-f04f4a4b353f\",\"type\":\"GlyphRenderer\"},{\"id\":\"093d6b0f-b7d9-4576-b538-0d45d99e99b0\",\"type\":\"GlyphRenderer\"}]},\"id\":\"c2330b33-e285-4f49-90fa-53268408e09a\",\"type\":\"LegendItem\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"343656f9-5cdd-4b45-8bdd-99e116c75276\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"5e07e9ba-f0b4-4165-83be-fd1c01156d8a\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"a6465d4c-c354-453f-9ca0-88fcd58ec750\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"eda8319b-3176-4bd7-990a-16b930cb1815\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"de739008-fa39-4c36-aa64-a5e86fc04fd1\",\"type\":\"CDSView\"}},\"id\":\"b2139dce-de3b-4725-9e61-039a34c36773\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"6900ee6c-c679-45ee-8ebd-4fdcf4cde0df\",\"type\":\"Line\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"83c46e7b-7965-44f6-9028-f8a14d27baa9\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"6496817c-2ce8-4cf2-bbbe-4345d684e115\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"3cca5454-46df-455b-8735-d5c02666a7ae\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"2f233339-ff52-4190-a304-9affa54571c9\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"42c7387f-be42-4110-8dc1-a0082b1271f9\",\"type\":\"CDSView\"}},\"id\":\"c9accb8a-2790-443d-affd-768d7a90e3bf\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + GB\"},\"renderers\":[{\"id\":\"78b86275-6dc1-4717-9a45-ed9c0ab9b52c\",\"type\":\"GlyphRenderer\"},{\"id\":\"2881ddb4-951a-4228-9a88-17e52fbb7a1f\",\"type\":\"GlyphRenderer\"}]},\"id\":\"c49c2d24-c7f2-475d-a336-9995c246a3e7\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8751131221719457,0.890497737556561,0.881447963800905,0.8895927601809954,0.8895927601809954,0.8787330316742081]}},\"id\":\"3a7f00ef-0cb7-4e0f-9576-5d694531cef3\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"e106e5fe-ef47-4548-89c6-2ffb779008ba\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"e9b23ff5-f2d2-48d1-ab3c-3d177bd543f3\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"d18292f9-2f8e-4a7c-8985-449a9373da1c\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"c987cdcf-5107-4337-bb05-41403b894b55\",\"type\":\"CDSView\"}},\"id\":\"1951b5b8-1d33-4246-82ef-639ff71d07d6\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"cb32c0ab-a53f-4da3-b2c1-046ff4d5c7be\",\"type\":\"LinearScale\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8733031674208145,0.8624434389140272,0.8742081447963801,0.8714932126696833,0.8470588235294118,0.857918552036199]}},\"id\":\"00e18df2-c766-4c5b-95b0-338d0847ec57\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"cyan\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"cc688cf7-58ec-424a-9ec7-5c6408a312d8\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"e106e5fe-ef47-4548-89c6-2ffb779008ba\",\"type\":\"ColumnDataSource\"}},\"id\":\"c987cdcf-5107-4337-bb05-41403b894b55\",\"type\":\"CDSView\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + MLP\"},\"renderers\":[{\"id\":\"c64ebe7c-e4c8-4176-b4da-07a7d82be974\",\"type\":\"GlyphRenderer\"},{\"id\":\"1951b5b8-1d33-4246-82ef-639ff71d07d6\",\"type\":\"GlyphRenderer\"}]},\"id\":\"31d95ce2-43fc-45b2-8f84-0629683f2921\",\"type\":\"LegendItem\"},{\"attributes\":{\"overlay\":{\"id\":\"a585864f-a390-430f-8c5b-e9890f228a30\",\"type\":\"BoxAnnotation\"}},\"id\":\"c5b2496c-fcde-4588-b70e-3e5f31fc9b3c\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[50,100,250,500,1000,1524],\"y\":[0.8733031674208145,0.8624434389140272,0.8742081447963801,0.8714932126696833,0.8470588235294118,0.857918552036199]}},\"id\":\"72a5af66-863f-4716-b0b7-be1758da480a\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"5c0048d9-1892-446c-98ef-ad1fae90348e\",\"type\":\"ColumnDataSource\"}},\"id\":\"1b3476cc-fdfc-4c1f-bdcc-90e62a83a3d8\",\"type\":\"CDSView\"}],\"root_ids\":[\"7f664107-64f9-4676-a805-8338163dc9d8\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.14\"}};\n",
       "  var render_items = [{\"docid\":\"9666f35b-7ca2-46a1-8be2-d912222f2a12\",\"elementid\":\"e8aebb8e-4768-445f-a383-d4c89e6f992e\",\"modelid\":\"7f664107-64f9-4676-a805-8338163dc9d8\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "7f664107-64f9-4676-a805-8338163dc9d8"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T16:00:18.190217Z",
     "start_time": "2018-05-01T16:00:17.202431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"7bdd4e24-7830-4c7d-b20d-33414dd33c6f\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"73ac3751-e09a-4d68-bf42-f8e8d83b6843\":{\"roots\":{\"references\":[{\"attributes\":{\"data_source\":{\"id\":\"9c12b39b-dcab-4dc8-987f-693f2b51a7fa\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"b471f86d-b4ce-4e90-8946-de7157d024a2\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"7e9c0138-d68b-45cc-af1f-09f6b280ea64\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"48cac542-987e-4cf8-83b8-35a2655a83be\",\"type\":\"CDSView\"}},\"id\":\"64e59b87-8ae5-49c1-8280-83fb6566f34a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"DFS + GB\"},\"renderers\":[{\"id\":\"ad8dc31b-4d34-41c4-825e-8ffa3e7de63d\",\"type\":\"GlyphRenderer\"},{\"id\":\"d1cff613-90be-43ae-8b44-dd307876f4c5\",\"type\":\"GlyphRenderer\"}]},\"id\":\"18c6bd76-d4fb-4b49-8606-92e42c93d61d\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"36b2071e-4d17-4903-9ee5-b187000083f5\",\"type\":\"ColumnDataSource\"}},\"id\":\"3cd5050f-0700-4fd6-8b80-73ce5332e026\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"2c6533bc-26c7-467c-9275-0afa623460b9\",\"type\":\"ColumnDataSource\"}},\"id\":\"155ca377-9146-41f1-926b-d2efed246516\",\"type\":\"CDSView\"},{\"attributes\":{\"label\":{\"value\":\"DFS\"},\"renderers\":[{\"id\":\"f89a6551-15ff-45a3-80fc-8c70f0dc03ea\",\"type\":\"GlyphRenderer\"},{\"id\":\"4f7950d5-19e7-435d-a7fb-b7e81a108d09\",\"type\":\"GlyphRenderer\"}]},\"id\":\"9e069200-539c-4e07-a810-743e09facd46\",\"type\":\"LegendItem\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476],\"y\":[0.7601809954751131,0.8669683257918552,0.865158371040724,0.8633484162895928,0.8669683257918552,0.8733031674208145,0.8678733031674208,0.8642533936651584,0.8642533936651584,0.8642533936651584,0.8687782805429864,0.8642533936651584,0.8642533936651584,0.8642533936651584,0.8733031674208145,0.8705882352941177,0.8678733031674208,0.8687782805429864,0.8678733031674208,0.8714932126696833]}},\"id\":\"a749030b-6636-465a-a1b3-845aa9482440\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"758890b4-ec49-4741-9663-de1fa3f6e3b9\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8588235294117647,0.853393665158371,0.8515837104072398,0.8642533936651584,0.8506787330316742,0.853393665158371,0.8524886877828054,0.8570135746606334,0.8552036199095022,0.860633484162896,0.857918552036199]}},\"id\":\"6329075e-6666-40ab-9dac-d7c12db33109\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_color\":{\"value\":\"red\"},\"line_color\":{\"value\":\"red\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"73d43d29-05ce-4fa2-ab42-a2cb4fc38f64\",\"type\":\"Circle\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2c19724c-009e-4041-8e1e-6606b5351a0d\",\"type\":\"BasicTicker\"}},\"id\":\"77a40709-2d43-4667-969a-d2dc6d202b65\",\"type\":\"Grid\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + GB\"},\"renderers\":[{\"id\":\"ee48d83c-26e9-4c55-b092-2e03102d4b3f\",\"type\":\"GlyphRenderer\"},{\"id\":\"cbc26970-3fb3-4595-82ba-20f688f5c24e\",\"type\":\"GlyphRenderer\"}]},\"id\":\"2ea69965-ea20-43de-a204-8c0d778735f2\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"66fad247-eb87-41a9-9618-04909e9b339b\",\"type\":\"ColumnDataSource\"}},\"id\":\"d942ecf2-b15a-442c-9d2b-f3911c6cdbcf\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"2c19724c-009e-4041-8e1e-6606b5351a0d\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"96859c44-b810-46b3-abc9-89324a14c749\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"ad7d813e-2f35-419a-b3de-d66ffeae2874\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"c5273a82-9e78-4918-a1bb-b60d09f539f8\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"7741f809-74fa-410a-8b52-d7c32fbe9bf2\",\"type\":\"CDSView\"}},\"id\":\"d1cff613-90be-43ae-8b44-dd307876f4c5\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"58d5eea9-cf3a-4cbb-9499-5da845215608\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_color\":{\"value\":\"brown\"},\"line_color\":{\"value\":\"brown\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"7ac1703c-8181-43e1-8ac7-f63d98395ef8\",\"type\":\"Circle\"},{\"attributes\":{\"fill_color\":{\"value\":\"pink\"},\"line_color\":{\"value\":\"pink\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"109ac5ba-63f3-4487-9e18-47ece12c09ce\",\"type\":\"Circle\"},{\"attributes\":{\"fill_color\":{\"value\":\"greenyellow\"},\"line_color\":{\"value\":\"greenyellow\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8c8cfdca-1b56-4e38-8a93-c747b66aae0c\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8597285067873304,0.8823529411764706,0.881447963800905,0.8723981900452489,0.8886877828054298,0.8832579185520362,0.885972850678733,0.8923076923076924,0.8850678733031674,0.8805429864253393,0.8796380090497737]}},\"id\":\"96859c44-b810-46b3-abc9-89324a14c749\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"green\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"02fd8ddd-876e-4df2-a488-14191322d5e5\",\"type\":\"Line\"},{\"attributes\":{\"fill_color\":{\"value\":\"green\"},\"line_color\":{\"value\":\"green\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"ad7d813e-2f35-419a-b3de-d66ffeae2874\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8705882352941177,0.8687782805429864,0.8669683257918552,0.8633484162895928,0.8597285067873304,0.865158371040724,0.869683257918552,0.8723981900452489,0.8669683257918552,0.8660633484162896,0.8678733031674208]}},\"id\":\"ab399060-c1ac-412b-96c7-61019cf0bc20\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.860633484162896,0.869683257918552,0.8778280542986425,0.8778280542986425,0.8805429864253393,0.881447963800905,0.8832579185520362,0.8796380090497737,0.8805429864253393,0.8796380090497737,0.8769230769230769]}},\"id\":\"2c6533bc-26c7-467c-9275-0afa623460b9\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"line_color\":\"brown\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"9ad2d9dc-32a5-4f31-b1bb-95929a5e880d\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"18b728aa-2ae3-4b92-b960-d9fb01582d25\",\"type\":\"ColumnDataSource\"}},\"id\":\"8d87fdff-18e0-4680-a6e7-74d84bb63694\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"a749030b-6636-465a-a1b3-845aa9482440\",\"type\":\"ColumnDataSource\"}},\"id\":\"9a3d41c3-dd59-4f88-9eb7-063c62405891\",\"type\":\"CDSView\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"5915fefc-ae81-4e5c-9a32-c5e69e69dc35\",\"type\":\"PanTool\"},{\"id\":\"710be025-a74d-4317-a110-90c76fd49a61\",\"type\":\"WheelZoomTool\"},{\"id\":\"cad30d0a-fd7f-4abc-86b4-a344c1219a10\",\"type\":\"BoxZoomTool\"},{\"id\":\"bba2de44-1b93-40ba-888e-5d47dfb5a7c4\",\"type\":\"SaveTool\"},{\"id\":\"4faec61b-2167-4e30-a7f7-c7431d313b70\",\"type\":\"ResetTool\"},{\"id\":\"97440f1d-eef0-4ef6-95fb-c52c822e7c2d\",\"type\":\"HelpTool\"}]},\"id\":\"b6adb03c-a209-474f-8a37-a83f55802367\",\"type\":\"Toolbar\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + MLP\"},\"renderers\":[{\"id\":\"4195fc27-c4af-4ae4-a8a8-8b5b51159967\",\"type\":\"GlyphRenderer\"},{\"id\":\"fe4ba723-288d-46b4-af9d-1762d7596fdb\",\"type\":\"GlyphRenderer\"}]},\"id\":\"13d9a855-2721-4946-9e3a-567efde737f2\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"929b1bc2-5c8a-4d89-940f-03a33773eb48\",\"type\":\"Line\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"ca4d80a7-10e0-447d-b981-0f5941c0c496\",\"type\":\"Circle\"},{\"attributes\":{\"source\":{\"id\":\"96859c44-b810-46b3-abc9-89324a14c749\",\"type\":\"ColumnDataSource\"}},\"id\":\"7741f809-74fa-410a-8b52-d7c32fbe9bf2\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8425339366515837,0.8669683257918552,0.881447963800905,0.8624434389140272,0.8588235294117647,0.8552036199095022,0.8597285067873304,0.8633484162895928,0.8769230769230769,0.8769230769230769,0.8452488687782805]}},\"id\":\"f04a6192-3bd3-4585-81d9-c7303d437303\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"below\":[{\"id\":\"7f6f838e-8bb0-4b0e-b519-7390c926caa5\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"585654c3-c13f-4796-9bab-684150f61b27\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"7f6f838e-8bb0-4b0e-b519-7390c926caa5\",\"type\":\"LinearAxis\"},{\"id\":\"49921d9b-099c-4f00-a4a4-5e02c5a425d0\",\"type\":\"Grid\"},{\"id\":\"585654c3-c13f-4796-9bab-684150f61b27\",\"type\":\"LinearAxis\"},{\"id\":\"77a40709-2d43-4667-969a-d2dc6d202b65\",\"type\":\"Grid\"},{\"id\":\"58d5eea9-cf3a-4cbb-9499-5da845215608\",\"type\":\"BoxAnnotation\"},{\"id\":\"21d81784-4f5a-4530-af7d-8df99ce3a424\",\"type\":\"Legend\"},{\"id\":\"2234b308-7efe-4bdf-ac91-2771c8ae37af\",\"type\":\"GlyphRenderer\"},{\"id\":\"84bc590d-541c-4e6f-86a0-a508a4aade24\",\"type\":\"GlyphRenderer\"},{\"id\":\"ad8dc31b-4d34-41c4-825e-8ffa3e7de63d\",\"type\":\"GlyphRenderer\"},{\"id\":\"d1cff613-90be-43ae-8b44-dd307876f4c5\",\"type\":\"GlyphRenderer\"},{\"id\":\"72c57186-307d-46e8-8860-0f87b258b083\",\"type\":\"GlyphRenderer\"},{\"id\":\"ba8e464a-a7cc-4b8f-b4f8-a52ed3c1599b\",\"type\":\"GlyphRenderer\"},{\"id\":\"ee48d83c-26e9-4c55-b092-2e03102d4b3f\",\"type\":\"GlyphRenderer\"},{\"id\":\"cbc26970-3fb3-4595-82ba-20f688f5c24e\",\"type\":\"GlyphRenderer\"},{\"id\":\"f89a6551-15ff-45a3-80fc-8c70f0dc03ea\",\"type\":\"GlyphRenderer\"},{\"id\":\"4f7950d5-19e7-435d-a7fb-b7e81a108d09\",\"type\":\"GlyphRenderer\"},{\"id\":\"5d03a47a-5999-485f-844b-8f699b7d1bde\",\"type\":\"GlyphRenderer\"},{\"id\":\"64e59b87-8ae5-49c1-8280-83fb6566f34a\",\"type\":\"GlyphRenderer\"},{\"id\":\"b9a9ed9e-2978-4642-b3ad-d866597dc656\",\"type\":\"GlyphRenderer\"},{\"id\":\"b1d3786b-2bdd-4bd1-8cd4-5273206e443a\",\"type\":\"GlyphRenderer\"},{\"id\":\"4195fc27-c4af-4ae4-a8a8-8b5b51159967\",\"type\":\"GlyphRenderer\"},{\"id\":\"fe4ba723-288d-46b4-af9d-1762d7596fdb\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"6800978e-65a8-4346-af7c-601ca3581047\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"b6adb03c-a209-474f-8a37-a83f55802367\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"779eb862-7686-4115-8260-83ba9af15994\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"ae4ff92d-5dd4-4a71-b535-346a835ec7cf\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"70fb03e6-a3f2-4686-b99e-4605b391bd33\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"67037aa9-3876-4e5e-b303-4a7767744722\",\"type\":\"LinearScale\"}},\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"callback\":null},\"id\":\"779eb862-7686-4115-8260-83ba9af15994\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"96aec5ab-bce0-4a0c-81a7-cd81a0968857\",\"type\":\"ColumnDataSource\"}},\"id\":\"9006dc6f-c198-43d9-a3bb-dd6420daaed1\",\"type\":\"CDSView\"},{\"attributes\":{\"click_policy\":\"hide\",\"items\":[{\"id\":\"88622a37-00cc-47d2-a5a8-2224215000cd\",\"type\":\"LegendItem\"},{\"id\":\"18c6bd76-d4fb-4b49-8606-92e42c93d61d\",\"type\":\"LegendItem\"},{\"id\":\"3ca01c11-146a-4ba4-b9be-264104500ae2\",\"type\":\"LegendItem\"},{\"id\":\"2ea69965-ea20-43de-a204-8c0d778735f2\",\"type\":\"LegendItem\"},{\"id\":\"9e069200-539c-4e07-a810-743e09facd46\",\"type\":\"LegendItem\"},{\"id\":\"c65b6059-ce8b-4288-af11-3bf7e270270a\",\"type\":\"LegendItem\"},{\"id\":\"1016db7c-683d-470d-a756-e052b453c81e\",\"type\":\"LegendItem\"},{\"id\":\"13d9a855-2721-4946-9e3a-567efde737f2\",\"type\":\"LegendItem\"}],\"location\":\"bottom_right\",\"plot\":{\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"}},\"id\":\"21d81784-4f5a-4530-af7d-8df99ce3a424\",\"type\":\"Legend\"},{\"attributes\":{\"line_color\":\"greenyellow\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8f1f8505-6fb4-45dd-ad37-b3f13a95c2af\",\"type\":\"Line\"},{\"attributes\":{\"formatter\":{\"id\":\"f5e82adc-a6c5-4183-9711-5cf22cdf2160\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"985425e9-60c6-4cd3-aacb-868e74f1ee76\",\"type\":\"BasicTicker\"}},\"id\":\"7f6f838e-8bb0-4b0e-b519-7390c926caa5\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"a749030b-6636-465a-a1b3-845aa9482440\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8f1f8505-6fb4-45dd-ad37-b3f13a95c2af\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"929b1bc2-5c8a-4d89-940f-03a33773eb48\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"9a3d41c3-dd59-4f88-9eb7-063c62405891\",\"type\":\"CDSView\"}},\"id\":\"b9a9ed9e-2978-4642-b3ad-d866597dc656\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"cyan\"},\"line_color\":{\"value\":\"cyan\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"44a2533a-0910-4038-a29a-d775c3654c29\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"2c6533bc-26c7-467c-9275-0afa623460b9\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"109ac5ba-63f3-4487-9e18-47ece12c09ce\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1803d9cd-15b0-4ab2-9e6b-f4e96c4f7f57\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"155ca377-9146-41f1-926b-d2efed246516\",\"type\":\"CDSView\"}},\"id\":\"cbc26970-3fb3-4595-82ba-20f688f5c24e\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"ab399060-c1ac-412b-96c7-61019cf0bc20\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"a5b3c80f-3c1a-4fa5-a622-a7df90f3d2e2\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"fa8246cc-a170-4572-bce2-7517fdb78f49\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"ce819162-da1f-4ed9-a438-0d5418d9dd36\",\"type\":\"CDSView\"}},\"id\":\"4195fc27-c4af-4ae4-a8a8-8b5b51159967\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"729cc19e-1ed4-4bc1-932d-2329c515945c\",\"type\":\"Line\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8597285067873304,0.8823529411764706,0.881447963800905,0.8723981900452489,0.8886877828054298,0.8832579185520362,0.885972850678733,0.8923076923076924,0.8850678733031674,0.8805429864253393,0.8796380090497737]}},\"id\":\"18b728aa-2ae3-4b92-b960-d9fb01582d25\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_color\":{\"value\":\"orange\"},\"line_color\":{\"value\":\"orange\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"b471f86d-b4ce-4e90-8946-de7157d024a2\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"6329075e-6666-40ab-9dac-d7c12db33109\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"7ac1703c-8181-43e1-8ac7-f63d98395ef8\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"a50eba59-7d9c-4522-ad4a-5d2413c9fbae\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"0a175ee0-1ebd-4bd3-8ab7-f2ab67bd8ff3\",\"type\":\"CDSView\"}},\"id\":\"4f7950d5-19e7-435d-a7fb-b7e81a108d09\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"4faec61b-2167-4e30-a7f7-c7431d313b70\",\"type\":\"ResetTool\"},{\"attributes\":{\"source\":{\"id\":\"9c12b39b-dcab-4dc8-987f-693f2b51a7fa\",\"type\":\"ColumnDataSource\"}},\"id\":\"48cac542-987e-4cf8-83b8-35a2655a83be\",\"type\":\"CDSView\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"75b457eb-9f51-4618-939a-c2ee7fd3d504\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"ab399060-c1ac-412b-96c7-61019cf0bc20\",\"type\":\"ColumnDataSource\"}},\"id\":\"ce819162-da1f-4ed9-a438-0d5418d9dd36\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"5915fefc-ae81-4e5c-9a32-c5e69e69dc35\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a50eba59-7d9c-4522-ad4a-5d2413c9fbae\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":\"orange\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"5d8c8e9a-6258-4d50-9e51-4c44346e0338\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"cbb800ab-f586-419e-897b-29f9c6182ee5\",\"type\":\"ColumnDataSource\"}},\"id\":\"e76b1149-6766-4ea3-a519-5700c8dac5b3\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"985425e9-60c6-4cd3-aacb-868e74f1ee76\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"460ba517-a9bd-409f-bdc1-998da11c0fb5\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"73d43d29-05ce-4fa2-ab42-a2cb4fc38f64\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"478bf107-076c-4e5a-adb8-f1793e114b6f\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"c988aa4f-0d46-434b-85d2-ce0764718af9\",\"type\":\"CDSView\"}},\"id\":\"ba8e464a-a7cc-4b8f-b4f8-a52ed3c1599b\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"ae4ff92d-5dd4-4a71-b535-346a835ec7cf\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"e4c6422f-8d8b-47de-913c-507dd118caca\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"b0860926-123e-46dc-9644-f8c61193d870\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"01617cfa-6d2b-4c10-a128-c600e9c664a9\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"a5eefdc1-ca13-4887-9333-8915eba09e43\",\"type\":\"CDSView\"}},\"id\":\"2234b308-7efe-4bdf-ac91-2771c8ae37af\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8452488687782805,0.8624434389140272,0.8705882352941177,0.8561085972850678,0.8660633484162896,0.8678733031674208,0.8642533936651584,0.8669683257918552,0.8687782805429864,0.8642533936651584,0.857918552036199]}},\"id\":\"96aec5ab-bce0-4a0c-81a7-cd81a0968857\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"18b728aa-2ae3-4b92-b960-d9fb01582d25\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"02fd8ddd-876e-4df2-a488-14191322d5e5\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"16a5851f-b51c-4197-b5c1-39e22c5d70be\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"8d87fdff-18e0-4680-a6e7-74d84bb63694\",\"type\":\"CDSView\"}},\"id\":\"ad8dc31b-4d34-41c4-825e-8ffa3e7de63d\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"bba2de44-1b93-40ba-888e-5d47dfb5a7c4\",\"type\":\"SaveTool\"},{\"attributes\":{\"source\":{\"id\":\"460ba517-a9bd-409f-bdc1-998da11c0fb5\",\"type\":\"ColumnDataSource\"}},\"id\":\"c988aa4f-0d46-434b-85d2-ce0764718af9\",\"type\":\"CDSView\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"6800978e-65a8-4346-af7c-601ca3581047\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[1,26,51,76,101,126,151,176,201,226,251,276,301,326,351,376,401,426,451,476],\"y\":[0.7601809954751131,0.8669683257918552,0.865158371040724,0.8633484162895928,0.8669683257918552,0.8733031674208145,0.8678733031674208,0.8642533936651584,0.8642533936651584,0.8642533936651584,0.8687782805429864,0.8642533936651584,0.8642533936651584,0.8642533936651584,0.8733031674208145,0.8705882352941177,0.8678733031674208,0.8687782805429864,0.8678733031674208,0.8714932126696833]}},\"id\":\"66fad247-eb87-41a9-9618-04909e9b339b\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"c5273a82-9e78-4918-a1bb-b60d09f539f8\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8425339366515837,0.8669683257918552,0.881447963800905,0.8624434389140272,0.8588235294117647,0.8552036199095022,0.8597285067873304,0.8633484162895928,0.8769230769230769,0.8769230769230769,0.8452488687782805]}},\"id\":\"e4c6422f-8d8b-47de-913c-507dd118caca\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"d4c98373-9985-43e9-8834-bf8e244ba3ed\",\"type\":\"ColumnDataSource\"}},\"id\":\"d6adf3ca-7b43-43ad-9320-aa264456b976\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"red\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d77950a9-e9bd-4854-b19d-1e16ea1d7a19\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"f5e82adc-a6c5-4183-9711-5cf22cdf2160\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"7e9c0138-d68b-45cc-af1f-09f6b280ea64\",\"type\":\"Circle\"},{\"attributes\":{\"data_source\":{\"id\":\"96aec5ab-bce0-4a0c-81a7-cd81a0968857\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"d77950a9-e9bd-4854-b19d-1e16ea1d7a19\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"729cc19e-1ed4-4bc1-932d-2329c515945c\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"9006dc6f-c198-43d9-a3bb-dd6420daaed1\",\"type\":\"CDSView\"}},\"id\":\"72c57186-307d-46e8-8860-0f87b258b083\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8705882352941177,0.8687782805429864,0.8669683257918552,0.8633484162895928,0.8597285067873304,0.865158371040724,0.869683257918552,0.8723981900452489,0.8669683257918552,0.8660633484162896,0.8678733031674208]}},\"id\":\"cbb800ab-f586-419e-897b-29f9c6182ee5\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.860633484162896,0.869683257918552,0.8778280542986425,0.8778280542986425,0.8805429864253393,0.881447963800905,0.8832579185520362,0.8796380090497737,0.8805429864253393,0.8796380090497737,0.8769230769230769]}},\"id\":\"36b2071e-4d17-4903-9ee5-b187000083f5\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"formatter\":{\"id\":\"758890b4-ec49-4741-9663-de1fa3f6e3b9\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"2c19724c-009e-4041-8e1e-6606b5351a0d\",\"type\":\"BasicTicker\"}},\"id\":\"585654c3-c13f-4796-9bab-684150f61b27\",\"type\":\"LinearAxis\"},{\"attributes\":{\"data_source\":{\"id\":\"cbb800ab-f586-419e-897b-29f9c6182ee5\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"44a2533a-0910-4038-a29a-d775c3654c29\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"67a45780-2a37-40da-b3b3-5a86639e17a0\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"e76b1149-6766-4ea3-a519-5700c8dac5b3\",\"type\":\"CDSView\"}},\"id\":\"fe4ba723-288d-46b4-af9d-1762d7596fdb\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"label\":{\"value\":\"DFS + MLP\"},\"renderers\":[{\"id\":\"b9a9ed9e-2978-4642-b3ad-d866597dc656\",\"type\":\"GlyphRenderer\"},{\"id\":\"b1d3786b-2bdd-4bd1-8cd4-5273206e443a\",\"type\":\"GlyphRenderer\"}]},\"id\":\"1016db7c-683d-470d-a756-e052b453c81e\",\"type\":\"LegendItem\"},{\"attributes\":{\"data_source\":{\"id\":\"66fad247-eb87-41a9-9618-04909e9b339b\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8c8cfdca-1b56-4e38-8a93-c747b66aae0c\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"f1454da1-7a4f-4a74-98c9-2fce58286d8c\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"d942ecf2-b15a-442c-9d2b-f3911c6cdbcf\",\"type\":\"CDSView\"}},\"id\":\"b1d3786b-2bdd-4bd1-8cd4-5273206e443a\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"fa8246cc-a170-4572-bce2-7517fdb78f49\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"f04a6192-3bd3-4585-81d9-c7303d437303\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1de1bbeb-5ea8-4629-8121-5245db2aac0e\",\"type\":\"Circle\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"ca4d80a7-10e0-447d-b981-0f5941c0c496\",\"type\":\"Circle\"},\"selection_glyph\":null,\"view\":{\"id\":\"a1ed176e-79aa-4fc2-a612-dee249548f25\",\"type\":\"CDSView\"}},\"id\":\"84bc590d-541c-4e6f-86a0-a508a4aade24\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"data_source\":{\"id\":\"d4c98373-9985-43e9-8834-bf8e244ba3ed\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"9ad2d9dc-32a5-4f31-b1bb-95929a5e880d\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"75b457eb-9f51-4618-939a-c2ee7fd3d504\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"d6adf3ca-7b43-43ad-9320-aa264456b976\",\"type\":\"CDSView\"}},\"id\":\"f89a6551-15ff-45a3-80fc-8c70f0dc03ea\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"line_color\":\"cyan\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"a5b3c80f-3c1a-4fa5-a622-a7df90f3d2e2\",\"type\":\"Line\"},{\"attributes\":{\"label\":{\"value\":\"Lasso + RF\"},\"renderers\":[{\"id\":\"72c57186-307d-46e8-8860-0f87b258b083\",\"type\":\"GlyphRenderer\"},{\"id\":\"ba8e464a-a7cc-4b8f-b4f8-a52ed3c1599b\",\"type\":\"GlyphRenderer\"}]},\"id\":\"3ca01c11-146a-4ba4-b9be-264104500ae2\",\"type\":\"LegendItem\"},{\"attributes\":{},\"id\":\"710be025-a74d-4317-a110-90c76fd49a61\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"dd26257d-edf1-4ea6-911c-2ef8c5cbfa42\",\"type\":\"Line\"},{\"attributes\":{\"source\":{\"id\":\"6329075e-6666-40ab-9dac-d7c12db33109\",\"type\":\"ColumnDataSource\"}},\"id\":\"0a175ee0-1ebd-4bd3-8ab7-f2ab67bd8ff3\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8497737556561086,0.8506787330316742,0.8506787330316742,0.8506787330316742]}},\"id\":\"063cc559-3a17-48bf-9e1a-1413c2b6c288\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"f1454da1-7a4f-4a74-98c9-2fce58286d8c\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null},\"id\":\"70fb03e6-a3f2-4686-b99e-4605b391bd33\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8506787330316742,0.8497737556561086,0.8506787330316742,0.8506787330316742,0.8506787330316742]}},\"id\":\"9c12b39b-dcab-4dc8-987f-693f2b51a7fa\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"label\":{\"value\":\"DFS + RF\"},\"renderers\":[{\"id\":\"2234b308-7efe-4bdf-ac91-2771c8ae37af\",\"type\":\"GlyphRenderer\"},{\"id\":\"84bc590d-541c-4e6f-86a0-a508a4aade24\",\"type\":\"GlyphRenderer\"}]},\"id\":\"88622a37-00cc-47d2-a5a8-2224215000cd\",\"type\":\"LegendItem\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"16a5851f-b51c-4197-b5c1-39e22c5d70be\",\"type\":\"Line\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"01617cfa-6d2b-4c10-a128-c600e9c664a9\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"67037aa9-3876-4e5e-b303-4a7767744722\",\"type\":\"LinearScale\"},{\"attributes\":{\"fill_color\":{\"value\":\"blue\"},\"line_color\":{\"value\":\"blue\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1de1bbeb-5ea8-4629-8121-5245db2aac0e\",\"type\":\"Circle\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"478bf107-076c-4e5a-adb8-f1793e114b6f\",\"type\":\"Circle\"},{\"attributes\":{\"label\":{\"value\":\"Lasso\"},\"renderers\":[{\"id\":\"5d03a47a-5999-485f-844b-8f699b7d1bde\",\"type\":\"GlyphRenderer\"},{\"id\":\"64e59b87-8ae5-49c1-8280-83fb6566f34a\",\"type\":\"GlyphRenderer\"}]},\"id\":\"c65b6059-ce8b-4288-af11-3bf7e270270a\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"e4c6422f-8d8b-47de-913c-507dd118caca\",\"type\":\"ColumnDataSource\"}},\"id\":\"a5eefdc1-ca13-4887-9333-8915eba09e43\",\"type\":\"CDSView\"},{\"attributes\":{\"line_color\":\"pink\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"8b6d3458-a9c5-409e-9d26-22f887aaf8c2\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"36b2071e-4d17-4903-9ee5-b187000083f5\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"8b6d3458-a9c5-409e-9d26-22f887aaf8c2\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"d8e7c73c-5a1c-4263-9630-915f7557a845\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"3cd5050f-0700-4fd6-8b80-73ce5332e026\",\"type\":\"CDSView\"}},\"id\":\"ee48d83c-26e9-4c55-b092-2e03102d4b3f\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"plot\":{\"id\":\"61a7fc57-857a-4436-aca4-5936d1939698\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"985425e9-60c6-4cd3-aacb-868e74f1ee76\",\"type\":\"BasicTicker\"}},\"id\":\"49921d9b-099c-4f00-a4a4-5e02c5a425d0\",\"type\":\"Grid\"},{\"attributes\":{\"line_alpha\":0.1,\"line_color\":\"#1f77b4\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"d8e7c73c-5a1c-4263-9630-915f7557a845\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"97440f1d-eef0-4ef6-95fb-c52c822e7c2d\",\"type\":\"HelpTool\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8588235294117647,0.853393665158371,0.8515837104072398,0.8642533936651584,0.8506787330316742,0.853393665158371,0.8524886877828054,0.8570135746606334,0.8552036199095022,0.860633484162896,0.857918552036199]}},\"id\":\"d4c98373-9985-43e9-8834-bf8e244ba3ed\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"source\":{\"id\":\"063cc559-3a17-48bf-9e1a-1413c2b6c288\",\"type\":\"ColumnDataSource\"}},\"id\":\"2339dca1-e7be-4b9a-b41c-7d94e558f8e1\",\"type\":\"CDSView\"},{\"attributes\":{\"source\":{\"id\":\"f04a6192-3bd3-4585-81d9-c7303d437303\",\"type\":\"ColumnDataSource\"}},\"id\":\"a1ed176e-79aa-4fc2-a612-dee249548f25\",\"type\":\"CDSView\"},{\"attributes\":{\"overlay\":{\"id\":\"58d5eea9-cf3a-4cbb-9499-5da845215608\",\"type\":\"BoxAnnotation\"}},\"id\":\"cad30d0a-fd7f-4abc-86b4-a344c1219a10\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1803d9cd-15b0-4ab2-9e6b-f4e96c4f7f57\",\"type\":\"Circle\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"y\"],\"data\":{\"x\":[10,25,50,75,100,125,150,175,200,225,1524],\"y\":[0.8452488687782805,0.8624434389140272,0.8705882352941177,0.8561085972850678,0.8660633484162896,0.8678733031674208,0.8642533936651584,0.8669683257918552,0.8687782805429864,0.8642533936651584,0.857918552036199]}},\"id\":\"460ba517-a9bd-409f-bdc1-998da11c0fb5\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"67a45780-2a37-40da-b3b3-5a86639e17a0\",\"type\":\"Circle\"},{\"attributes\":{\"line_color\":\"blue\",\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"b0860926-123e-46dc-9644-f8c61193d870\",\"type\":\"Line\"},{\"attributes\":{\"data_source\":{\"id\":\"063cc559-3a17-48bf-9e1a-1413c2b6c288\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"5d8c8e9a-6258-4d50-9e51-4c44346e0338\",\"type\":\"Line\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"dd26257d-edf1-4ea6-911c-2ef8c5cbfa42\",\"type\":\"Line\"},\"selection_glyph\":null,\"view\":{\"id\":\"2339dca1-e7be-4b9a-b41c-7d94e558f8e1\",\"type\":\"CDSView\"}},\"id\":\"5d03a47a-5999-485f-844b-8f699b7d1bde\",\"type\":\"GlyphRenderer\"}],\"root_ids\":[\"61a7fc57-857a-4436-aca4-5936d1939698\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.14\"}};\n",
       "  var render_items = [{\"docid\":\"73ac3751-e09a-4d68-bf42-f8e8d83b6843\",\"elementid\":\"7bdd4e24-7830-4c7d-b20d-33414dd33c6f\",\"modelid\":\"61a7fc57-857a-4436-aca4-5936d1939698\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "61a7fc57-857a-4436-aca4-5936d1939698"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T18:55:30.253218Z",
     "start_time": "2018-05-01T18:55:30.108421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature-321\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'feature-321'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2524\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feature-321'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-525-93ec801ce47c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1842\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3843\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3844\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'feature-321'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T18:30:51.015070Z",
     "start_time": "2018-05-01T18:30:51.009875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.])"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['feature-806'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T20:41:51.648754Z",
     "start_time": "2018-05-01T20:41:51.639900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5775076572131588"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.402251 / 0.156062"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "250px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "998px",
    "left": "0px",
    "right": "1496px",
    "top": "52px",
    "width": "184px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
