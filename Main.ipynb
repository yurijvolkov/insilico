{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:19:39.358270Z",
     "start_time": "2018-04-30T15:19:34.936611Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as mtcs\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.ensemble as ens\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading&preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:02.784552Z",
     "start_time": "2018-04-30T15:11:01.869505Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature-0</th>\n",
       "      <th>feature-1</th>\n",
       "      <th>feature-2</th>\n",
       "      <th>feature-3</th>\n",
       "      <th>feature-4</th>\n",
       "      <th>feature-5</th>\n",
       "      <th>feature-6</th>\n",
       "      <th>feature-7</th>\n",
       "      <th>feature-8</th>\n",
       "      <th>feature-9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature-1515</th>\n",
       "      <th>feature-1516</th>\n",
       "      <th>feature-1517</th>\n",
       "      <th>feature-1518</th>\n",
       "      <th>feature-1519</th>\n",
       "      <th>feature-1520</th>\n",
       "      <th>feature-1521</th>\n",
       "      <th>feature-1522</th>\n",
       "      <th>feature-1523</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.977273</td>\n",
       "      <td>6.758452</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>10.792929</td>\n",
       "      <td>160.801682</td>\n",
       "      <td>151.109783</td>\n",
       "      <td>1.791689</td>\n",
       "      <td>6.818675</td>\n",
       "      <td>8.138413</td>\n",
       "      <td>8.270161</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658393</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.540632</td>\n",
       "      <td>4.953183</td>\n",
       "      <td>5.351562</td>\n",
       "      <td>5.311048</td>\n",
       "      <td>5.560922</td>\n",
       "      <td>5.643015</td>\n",
       "      <td>5.715999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.408163</td>\n",
       "      <td>5.933978</td>\n",
       "      <td>2.816327</td>\n",
       "      <td>5.877551</td>\n",
       "      <td>162.949911</td>\n",
       "      <td>76.153796</td>\n",
       "      <td>1.381401</td>\n",
       "      <td>6.002651</td>\n",
       "      <td>5.080499</td>\n",
       "      <td>7.514421</td>\n",
       "      <td>...</td>\n",
       "      <td>4.830811</td>\n",
       "      <td>3.817712</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.426343</td>\n",
       "      <td>4.823804</td>\n",
       "      <td>4.652173</td>\n",
       "      <td>4.795274</td>\n",
       "      <td>4.860781</td>\n",
       "      <td>5.001426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.265306</td>\n",
       "      <td>7.425645</td>\n",
       "      <td>3.734694</td>\n",
       "      <td>13.160998</td>\n",
       "      <td>172.099640</td>\n",
       "      <td>161.790879</td>\n",
       "      <td>1.603976</td>\n",
       "      <td>7.410120</td>\n",
       "      <td>10.114794</td>\n",
       "      <td>8.805738</td>\n",
       "      <td>...</td>\n",
       "      <td>6.397659</td>\n",
       "      <td>4.223177</td>\n",
       "      <td>4.685597</td>\n",
       "      <td>5.116870</td>\n",
       "      <td>5.333926</td>\n",
       "      <td>5.504569</td>\n",
       "      <td>5.797956</td>\n",
       "      <td>6.009581</td>\n",
       "      <td>6.200889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.976744</td>\n",
       "      <td>7.648293</td>\n",
       "      <td>3.837209</td>\n",
       "      <td>14.392765</td>\n",
       "      <td>168.885456</td>\n",
       "      <td>175.277251</td>\n",
       "      <td>1.622298</td>\n",
       "      <td>7.629033</td>\n",
       "      <td>12.180817</td>\n",
       "      <td>9.070719</td>\n",
       "      <td>...</td>\n",
       "      <td>5.879135</td>\n",
       "      <td>4.280132</td>\n",
       "      <td>4.563045</td>\n",
       "      <td>5.007714</td>\n",
       "      <td>5.159773</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.640132</td>\n",
       "      <td>5.472271</td>\n",
       "      <td>5.741399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.320988</td>\n",
       "      <td>6.534011</td>\n",
       "      <td>3.567901</td>\n",
       "      <td>8.913580</td>\n",
       "      <td>163.076959</td>\n",
       "      <td>96.019681</td>\n",
       "      <td>1.380679</td>\n",
       "      <td>6.566695</td>\n",
       "      <td>4.417010</td>\n",
       "      <td>8.058783</td>\n",
       "      <td>...</td>\n",
       "      <td>8.148663</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.173321</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>6.259342</td>\n",
       "      <td>6.626469</td>\n",
       "      <td>7.062406</td>\n",
       "      <td>7.472998</td>\n",
       "      <td>7.829842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.924051</td>\n",
       "      <td>6.134299</td>\n",
       "      <td>3.037975</td>\n",
       "      <td>6.506329</td>\n",
       "      <td>165.707039</td>\n",
       "      <td>82.761541</td>\n",
       "      <td>1.381957</td>\n",
       "      <td>6.187547</td>\n",
       "      <td>4.684599</td>\n",
       "      <td>7.660347</td>\n",
       "      <td>...</td>\n",
       "      <td>6.087556</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.820282</td>\n",
       "      <td>5.183187</td>\n",
       "      <td>5.595176</td>\n",
       "      <td>5.489454</td>\n",
       "      <td>5.604998</td>\n",
       "      <td>5.847522</td>\n",
       "      <td>5.987080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.150000</td>\n",
       "      <td>6.740695</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.214815</td>\n",
       "      <td>164.252922</td>\n",
       "      <td>135.639059</td>\n",
       "      <td>1.620887</td>\n",
       "      <td>6.781702</td>\n",
       "      <td>8.631090</td>\n",
       "      <td>8.248393</td>\n",
       "      <td>...</td>\n",
       "      <td>6.198225</td>\n",
       "      <td>4.471639</td>\n",
       "      <td>4.801970</td>\n",
       "      <td>5.237107</td>\n",
       "      <td>5.493833</td>\n",
       "      <td>5.573816</td>\n",
       "      <td>5.764799</td>\n",
       "      <td>5.865760</td>\n",
       "      <td>5.998937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.833333</td>\n",
       "      <td>6.395508</td>\n",
       "      <td>3.141026</td>\n",
       "      <td>8.717949</td>\n",
       "      <td>163.221967</td>\n",
       "      <td>94.106131</td>\n",
       "      <td>1.435936</td>\n",
       "      <td>6.443753</td>\n",
       "      <td>5.834402</td>\n",
       "      <td>7.904135</td>\n",
       "      <td>...</td>\n",
       "      <td>6.582328</td>\n",
       "      <td>4.600158</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.499726</td>\n",
       "      <td>5.978728</td>\n",
       "      <td>5.995208</td>\n",
       "      <td>6.179952</td>\n",
       "      <td>6.364051</td>\n",
       "      <td>6.481290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.380952</td>\n",
       "      <td>6.152543</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>6.402116</td>\n",
       "      <td>164.380868</td>\n",
       "      <td>128.391104</td>\n",
       "      <td>1.687697</td>\n",
       "      <td>6.232890</td>\n",
       "      <td>4.476844</td>\n",
       "      <td>7.736528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.449988</td>\n",
       "      <td>3.865979</td>\n",
       "      <td>4.506730</td>\n",
       "      <td>4.765906</td>\n",
       "      <td>4.965028</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.595598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.228571</td>\n",
       "      <td>6.608449</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>9.180952</td>\n",
       "      <td>159.167580</td>\n",
       "      <td>180.141749</td>\n",
       "      <td>1.981354</td>\n",
       "      <td>6.690537</td>\n",
       "      <td>8.428546</td>\n",
       "      <td>8.221041</td>\n",
       "      <td>...</td>\n",
       "      <td>5.214936</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.682131</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>5.342334</td>\n",
       "      <td>5.402677</td>\n",
       "      <td>5.303305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.757143</td>\n",
       "      <td>6.241471</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>6.949206</td>\n",
       "      <td>167.005923</td>\n",
       "      <td>97.692813</td>\n",
       "      <td>1.408460</td>\n",
       "      <td>6.289021</td>\n",
       "      <td>5.919754</td>\n",
       "      <td>7.789862</td>\n",
       "      <td>...</td>\n",
       "      <td>6.175997</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>4.716264</td>\n",
       "      <td>5.155457</td>\n",
       "      <td>5.591686</td>\n",
       "      <td>5.680173</td>\n",
       "      <td>5.977302</td>\n",
       "      <td>6.030986</td>\n",
       "      <td>6.214671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.096774</td>\n",
       "      <td>7.206768</td>\n",
       "      <td>3.838710</td>\n",
       "      <td>13.806452</td>\n",
       "      <td>160.469926</td>\n",
       "      <td>127.646528</td>\n",
       "      <td>1.591140</td>\n",
       "      <td>7.228381</td>\n",
       "      <td>11.071685</td>\n",
       "      <td>8.598289</td>\n",
       "      <td>...</td>\n",
       "      <td>7.242977</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>4.639572</td>\n",
       "      <td>5.151845</td>\n",
       "      <td>5.678037</td>\n",
       "      <td>5.850765</td>\n",
       "      <td>6.134888</td>\n",
       "      <td>6.451753</td>\n",
       "      <td>6.793466</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29.833333</td>\n",
       "      <td>6.436864</td>\n",
       "      <td>3.476190</td>\n",
       "      <td>8.296296</td>\n",
       "      <td>162.859109</td>\n",
       "      <td>118.257591</td>\n",
       "      <td>1.600911</td>\n",
       "      <td>6.497038</td>\n",
       "      <td>6.283812</td>\n",
       "      <td>7.960386</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>4.051785</td>\n",
       "      <td>4.519067</td>\n",
       "      <td>4.935373</td>\n",
       "      <td>5.281616</td>\n",
       "      <td>5.221369</td>\n",
       "      <td>5.465948</td>\n",
       "      <td>5.520210</td>\n",
       "      <td>5.499982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.500000</td>\n",
       "      <td>6.700508</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>10.709402</td>\n",
       "      <td>163.669041</td>\n",
       "      <td>145.244215</td>\n",
       "      <td>1.785651</td>\n",
       "      <td>6.764423</td>\n",
       "      <td>7.295706</td>\n",
       "      <td>8.158660</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768126</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>5.326662</td>\n",
       "      <td>5.340239</td>\n",
       "      <td>5.395331</td>\n",
       "      <td>5.454787</td>\n",
       "      <td>5.557552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.851852</td>\n",
       "      <td>7.402900</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>12.979424</td>\n",
       "      <td>170.018323</td>\n",
       "      <td>154.527750</td>\n",
       "      <td>1.476580</td>\n",
       "      <td>7.381493</td>\n",
       "      <td>11.933931</td>\n",
       "      <td>8.867678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.543295</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.534483</td>\n",
       "      <td>6.269883</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>7.973180</td>\n",
       "      <td>161.381053</td>\n",
       "      <td>109.038626</td>\n",
       "      <td>1.618478</td>\n",
       "      <td>6.345124</td>\n",
       "      <td>4.994148</td>\n",
       "      <td>7.791228</td>\n",
       "      <td>...</td>\n",
       "      <td>8.934004</td>\n",
       "      <td>4.644391</td>\n",
       "      <td>5.241747</td>\n",
       "      <td>5.930586</td>\n",
       "      <td>6.610360</td>\n",
       "      <td>7.088878</td>\n",
       "      <td>7.641069</td>\n",
       "      <td>8.134765</td>\n",
       "      <td>8.607916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.837500</td>\n",
       "      <td>6.722804</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>172.043543</td>\n",
       "      <td>93.939718</td>\n",
       "      <td>1.156748</td>\n",
       "      <td>6.710586</td>\n",
       "      <td>6.512587</td>\n",
       "      <td>8.234018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.638355</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>5.043425</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.337538</td>\n",
       "      <td>5.497168</td>\n",
       "      <td>5.568345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55.263158</td>\n",
       "      <td>7.389305</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>12.257310</td>\n",
       "      <td>170.139845</td>\n",
       "      <td>220.606751</td>\n",
       "      <td>1.969099</td>\n",
       "      <td>7.412105</td>\n",
       "      <td>10.847813</td>\n",
       "      <td>8.885381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.617652</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.357510</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>4.789573</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>3.944006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.057692</td>\n",
       "      <td>7.061350</td>\n",
       "      <td>3.942308</td>\n",
       "      <td>11.871795</td>\n",
       "      <td>163.441127</td>\n",
       "      <td>171.586122</td>\n",
       "      <td>1.834505</td>\n",
       "      <td>7.104088</td>\n",
       "      <td>9.456211</td>\n",
       "      <td>8.542274</td>\n",
       "      <td>...</td>\n",
       "      <td>6.706174</td>\n",
       "      <td>4.508108</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.374989</td>\n",
       "      <td>5.679959</td>\n",
       "      <td>5.755742</td>\n",
       "      <td>6.060582</td>\n",
       "      <td>6.240763</td>\n",
       "      <td>6.434747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.886792</td>\n",
       "      <td>7.052642</td>\n",
       "      <td>3.943396</td>\n",
       "      <td>12.100629</td>\n",
       "      <td>164.829574</td>\n",
       "      <td>170.922688</td>\n",
       "      <td>1.833416</td>\n",
       "      <td>7.095513</td>\n",
       "      <td>8.143800</td>\n",
       "      <td>8.514148</td>\n",
       "      <td>...</td>\n",
       "      <td>6.976085</td>\n",
       "      <td>4.572130</td>\n",
       "      <td>5.115746</td>\n",
       "      <td>5.566195</td>\n",
       "      <td>5.936711</td>\n",
       "      <td>6.080505</td>\n",
       "      <td>6.198796</td>\n",
       "      <td>6.333335</td>\n",
       "      <td>6.652742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35.761905</td>\n",
       "      <td>6.858500</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>9.640212</td>\n",
       "      <td>168.739205</td>\n",
       "      <td>141.875852</td>\n",
       "      <td>1.456355</td>\n",
       "      <td>6.864043</td>\n",
       "      <td>11.470610</td>\n",
       "      <td>8.439979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.528571</td>\n",
       "      <td>6.482779</td>\n",
       "      <td>2.942857</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>171.920444</td>\n",
       "      <td>84.572241</td>\n",
       "      <td>1.127633</td>\n",
       "      <td>6.481077</td>\n",
       "      <td>5.269841</td>\n",
       "      <td>8.031493</td>\n",
       "      <td>...</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.672829</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>4.969813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.054795</td>\n",
       "      <td>6.395890</td>\n",
       "      <td>3.164384</td>\n",
       "      <td>8.301370</td>\n",
       "      <td>162.696886</td>\n",
       "      <td>90.885447</td>\n",
       "      <td>1.370996</td>\n",
       "      <td>6.435699</td>\n",
       "      <td>6.796613</td>\n",
       "      <td>7.942436</td>\n",
       "      <td>...</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.652489</td>\n",
       "      <td>5.831882</td>\n",
       "      <td>6.040255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.160000</td>\n",
       "      <td>6.067396</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>167.413784</td>\n",
       "      <td>75.058797</td>\n",
       "      <td>1.241288</td>\n",
       "      <td>6.107552</td>\n",
       "      <td>6.423333</td>\n",
       "      <td>7.651508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.131868</td>\n",
       "      <td>5.762305</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.109890</td>\n",
       "      <td>165.080662</td>\n",
       "      <td>66.885356</td>\n",
       "      <td>1.294529</td>\n",
       "      <td>5.828765</td>\n",
       "      <td>2.738324</td>\n",
       "      <td>7.375662</td>\n",
       "      <td>...</td>\n",
       "      <td>6.448889</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.351858</td>\n",
       "      <td>5.590987</td>\n",
       "      <td>5.834811</td>\n",
       "      <td>6.077642</td>\n",
       "      <td>6.293419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28.432432</td>\n",
       "      <td>6.603514</td>\n",
       "      <td>4.324324</td>\n",
       "      <td>11.027027</td>\n",
       "      <td>153.571767</td>\n",
       "      <td>112.900411</td>\n",
       "      <td>1.738632</td>\n",
       "      <td>6.683730</td>\n",
       "      <td>6.055743</td>\n",
       "      <td>8.059202</td>\n",
       "      <td>...</td>\n",
       "      <td>8.293510</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>5.012301</td>\n",
       "      <td>5.675469</td>\n",
       "      <td>6.310259</td>\n",
       "      <td>6.686641</td>\n",
       "      <td>7.157565</td>\n",
       "      <td>7.635515</td>\n",
       "      <td>8.018008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>6.207650</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>164.257367</td>\n",
       "      <td>85.172313</td>\n",
       "      <td>1.392726</td>\n",
       "      <td>6.258106</td>\n",
       "      <td>5.808081</td>\n",
       "      <td>7.738226</td>\n",
       "      <td>...</td>\n",
       "      <td>7.443490</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>5.422745</td>\n",
       "      <td>6.018593</td>\n",
       "      <td>6.373640</td>\n",
       "      <td>6.751321</td>\n",
       "      <td>7.048332</td>\n",
       "      <td>7.302612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.428571</td>\n",
       "      <td>6.252383</td>\n",
       "      <td>2.885714</td>\n",
       "      <td>7.384127</td>\n",
       "      <td>164.322747</td>\n",
       "      <td>112.473778</td>\n",
       "      <td>1.536709</td>\n",
       "      <td>6.312869</td>\n",
       "      <td>6.333953</td>\n",
       "      <td>7.825781</td>\n",
       "      <td>...</td>\n",
       "      <td>3.791267</td>\n",
       "      <td>3.772761</td>\n",
       "      <td>4.081766</td>\n",
       "      <td>4.514972</td>\n",
       "      <td>4.873765</td>\n",
       "      <td>4.997212</td>\n",
       "      <td>4.848606</td>\n",
       "      <td>4.455074</td>\n",
       "      <td>4.352694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38.658537</td>\n",
       "      <td>6.752610</td>\n",
       "      <td>3.682927</td>\n",
       "      <td>10.753388</td>\n",
       "      <td>159.812730</td>\n",
       "      <td>153.837907</td>\n",
       "      <td>1.814410</td>\n",
       "      <td>6.815551</td>\n",
       "      <td>8.392444</td>\n",
       "      <td>8.278695</td>\n",
       "      <td>...</td>\n",
       "      <td>5.800228</td>\n",
       "      <td>4.098503</td>\n",
       "      <td>4.492841</td>\n",
       "      <td>4.938513</td>\n",
       "      <td>5.120237</td>\n",
       "      <td>5.262042</td>\n",
       "      <td>5.561162</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.681878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.670588</td>\n",
       "      <td>6.292286</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>6.776471</td>\n",
       "      <td>167.982829</td>\n",
       "      <td>85.273347</td>\n",
       "      <td>1.296235</td>\n",
       "      <td>6.326075</td>\n",
       "      <td>4.991830</td>\n",
       "      <td>7.822375</td>\n",
       "      <td>...</td>\n",
       "      <td>6.577992</td>\n",
       "      <td>4.385147</td>\n",
       "      <td>4.798885</td>\n",
       "      <td>5.216633</td>\n",
       "      <td>5.602695</td>\n",
       "      <td>5.857442</td>\n",
       "      <td>6.151851</td>\n",
       "      <td>6.484856</td>\n",
       "      <td>6.339973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>23.849057</td>\n",
       "      <td>6.443296</td>\n",
       "      <td>3.584906</td>\n",
       "      <td>8.490566</td>\n",
       "      <td>165.656310</td>\n",
       "      <td>94.855312</td>\n",
       "      <td>1.429258</td>\n",
       "      <td>6.483247</td>\n",
       "      <td>5.732180</td>\n",
       "      <td>7.917090</td>\n",
       "      <td>...</td>\n",
       "      <td>8.145414</td>\n",
       "      <td>4.375757</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.702949</td>\n",
       "      <td>6.339698</td>\n",
       "      <td>6.834210</td>\n",
       "      <td>7.294229</td>\n",
       "      <td>7.631557</td>\n",
       "      <td>8.007074</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>20.528302</td>\n",
       "      <td>6.044432</td>\n",
       "      <td>3.603774</td>\n",
       "      <td>6.075472</td>\n",
       "      <td>162.424590</td>\n",
       "      <td>80.699025</td>\n",
       "      <td>1.402911</td>\n",
       "      <td>6.109594</td>\n",
       "      <td>3.789570</td>\n",
       "      <td>7.610526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.481855</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.734003</td>\n",
       "      <td>5.287636</td>\n",
       "      <td>5.850225</td>\n",
       "      <td>6.211102</td>\n",
       "      <td>6.591717</td>\n",
       "      <td>6.991033</td>\n",
       "      <td>7.274674</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>20.440000</td>\n",
       "      <td>5.968498</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>161.064334</td>\n",
       "      <td>80.380964</td>\n",
       "      <td>1.456437</td>\n",
       "      <td>6.045898</td>\n",
       "      <td>3.411944</td>\n",
       "      <td>7.531384</td>\n",
       "      <td>...</td>\n",
       "      <td>7.520150</td>\n",
       "      <td>4.166665</td>\n",
       "      <td>4.725173</td>\n",
       "      <td>5.302683</td>\n",
       "      <td>5.881406</td>\n",
       "      <td>6.250458</td>\n",
       "      <td>6.631384</td>\n",
       "      <td>7.030719</td>\n",
       "      <td>7.325005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>5.957881</td>\n",
       "      <td>3.062500</td>\n",
       "      <td>5.509259</td>\n",
       "      <td>164.383290</td>\n",
       "      <td>95.963738</td>\n",
       "      <td>1.496938</td>\n",
       "      <td>6.033144</td>\n",
       "      <td>3.958207</td>\n",
       "      <td>7.545887</td>\n",
       "      <td>...</td>\n",
       "      <td>7.057353</td>\n",
       "      <td>4.131159</td>\n",
       "      <td>4.601413</td>\n",
       "      <td>5.158696</td>\n",
       "      <td>5.690359</td>\n",
       "      <td>6.054403</td>\n",
       "      <td>6.455334</td>\n",
       "      <td>6.873095</td>\n",
       "      <td>6.939417</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>23.375000</td>\n",
       "      <td>6.322417</td>\n",
       "      <td>3.203125</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>162.588954</td>\n",
       "      <td>92.261393</td>\n",
       "      <td>1.451475</td>\n",
       "      <td>6.377366</td>\n",
       "      <td>5.672743</td>\n",
       "      <td>7.837564</td>\n",
       "      <td>...</td>\n",
       "      <td>6.824985</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>5.421641</td>\n",
       "      <td>5.892335</td>\n",
       "      <td>6.042336</td>\n",
       "      <td>6.333613</td>\n",
       "      <td>6.572807</td>\n",
       "      <td>6.736893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>9.073735</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>180.387654</td>\n",
       "      <td>166.508642</td>\n",
       "      <td>1.000430</td>\n",
       "      <td>8.896735</td>\n",
       "      <td>19.768056</td>\n",
       "      <td>10.349211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>18.390244</td>\n",
       "      <td>5.605241</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.731707</td>\n",
       "      <td>158.624476</td>\n",
       "      <td>72.159797</td>\n",
       "      <td>1.549658</td>\n",
       "      <td>5.717783</td>\n",
       "      <td>2.198509</td>\n",
       "      <td>7.184370</td>\n",
       "      <td>...</td>\n",
       "      <td>7.163209</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>4.559126</td>\n",
       "      <td>5.140200</td>\n",
       "      <td>5.685703</td>\n",
       "      <td>5.885409</td>\n",
       "      <td>6.272759</td>\n",
       "      <td>6.609097</td>\n",
       "      <td>6.919128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>23.458333</td>\n",
       "      <td>6.397181</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>166.060607</td>\n",
       "      <td>93.350066</td>\n",
       "      <td>1.430009</td>\n",
       "      <td>6.439448</td>\n",
       "      <td>5.322917</td>\n",
       "      <td>7.869501</td>\n",
       "      <td>...</td>\n",
       "      <td>8.177485</td>\n",
       "      <td>4.297285</td>\n",
       "      <td>4.978456</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>6.322790</td>\n",
       "      <td>6.818753</td>\n",
       "      <td>7.288522</td>\n",
       "      <td>7.655010</td>\n",
       "      <td>8.058613</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>25.809524</td>\n",
       "      <td>5.873886</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>5.962963</td>\n",
       "      <td>162.051123</td>\n",
       "      <td>102.034265</td>\n",
       "      <td>1.656037</td>\n",
       "      <td>5.975364</td>\n",
       "      <td>2.317542</td>\n",
       "      <td>7.431153</td>\n",
       "      <td>...</td>\n",
       "      <td>7.555979</td>\n",
       "      <td>4.073291</td>\n",
       "      <td>4.650383</td>\n",
       "      <td>5.246695</td>\n",
       "      <td>5.735564</td>\n",
       "      <td>6.103816</td>\n",
       "      <td>6.545754</td>\n",
       "      <td>6.979000</td>\n",
       "      <td>7.346393</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>21.414634</td>\n",
       "      <td>5.975237</td>\n",
       "      <td>3.487805</td>\n",
       "      <td>6.926829</td>\n",
       "      <td>161.339153</td>\n",
       "      <td>84.405257</td>\n",
       "      <td>1.534660</td>\n",
       "      <td>6.062522</td>\n",
       "      <td>3.468157</td>\n",
       "      <td>7.496018</td>\n",
       "      <td>...</td>\n",
       "      <td>7.446209</td>\n",
       "      <td>4.212128</td>\n",
       "      <td>4.888468</td>\n",
       "      <td>5.525951</td>\n",
       "      <td>5.975081</td>\n",
       "      <td>6.299380</td>\n",
       "      <td>6.681394</td>\n",
       "      <td>7.067183</td>\n",
       "      <td>7.171165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.884906</td>\n",
       "      <td>2.759259</td>\n",
       "      <td>5.370370</td>\n",
       "      <td>162.831246</td>\n",
       "      <td>74.502082</td>\n",
       "      <td>1.381487</td>\n",
       "      <td>5.956572</td>\n",
       "      <td>3.660108</td>\n",
       "      <td>7.471168</td>\n",
       "      <td>...</td>\n",
       "      <td>5.514688</td>\n",
       "      <td>3.901973</td>\n",
       "      <td>4.327438</td>\n",
       "      <td>4.732904</td>\n",
       "      <td>5.096431</td>\n",
       "      <td>5.080239</td>\n",
       "      <td>5.365684</td>\n",
       "      <td>5.482980</td>\n",
       "      <td>5.400140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>23.318182</td>\n",
       "      <td>6.330339</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>8.045455</td>\n",
       "      <td>163.149857</td>\n",
       "      <td>92.872249</td>\n",
       "      <td>1.484800</td>\n",
       "      <td>6.384780</td>\n",
       "      <td>4.174874</td>\n",
       "      <td>7.815815</td>\n",
       "      <td>...</td>\n",
       "      <td>6.172744</td>\n",
       "      <td>4.060443</td>\n",
       "      <td>4.569543</td>\n",
       "      <td>5.057837</td>\n",
       "      <td>5.539301</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.728170</td>\n",
       "      <td>5.901779</td>\n",
       "      <td>5.917717</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>20.560000</td>\n",
       "      <td>6.123796</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>165.088853</td>\n",
       "      <td>80.788719</td>\n",
       "      <td>1.335059</td>\n",
       "      <td>6.174052</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>7.684390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.238678</td>\n",
       "      <td>3.669951</td>\n",
       "      <td>4.112921</td>\n",
       "      <td>4.525180</td>\n",
       "      <td>4.251170</td>\n",
       "      <td>4.104707</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>22.380952</td>\n",
       "      <td>6.182614</td>\n",
       "      <td>3.349206</td>\n",
       "      <td>7.396825</td>\n",
       "      <td>164.113227</td>\n",
       "      <td>89.449143</td>\n",
       "      <td>1.521852</td>\n",
       "      <td>6.248717</td>\n",
       "      <td>4.873016</td>\n",
       "      <td>7.649627</td>\n",
       "      <td>...</td>\n",
       "      <td>7.231355</td>\n",
       "      <td>4.545951</td>\n",
       "      <td>5.133590</td>\n",
       "      <td>5.739994</td>\n",
       "      <td>6.264469</td>\n",
       "      <td>6.422816</td>\n",
       "      <td>6.771282</td>\n",
       "      <td>7.089077</td>\n",
       "      <td>7.031227</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>25.161290</td>\n",
       "      <td>5.774461</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>4.143369</td>\n",
       "      <td>166.212698</td>\n",
       "      <td>99.236969</td>\n",
       "      <td>1.531852</td>\n",
       "      <td>5.860845</td>\n",
       "      <td>2.388103</td>\n",
       "      <td>7.386058</td>\n",
       "      <td>...</td>\n",
       "      <td>3.954843</td>\n",
       "      <td>3.409496</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.382808</td>\n",
       "      <td>4.539297</td>\n",
       "      <td>4.549261</td>\n",
       "      <td>4.743845</td>\n",
       "      <td>4.850075</td>\n",
       "      <td>4.323304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>18.074074</td>\n",
       "      <td>5.806478</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>164.385174</td>\n",
       "      <td>70.742453</td>\n",
       "      <td>1.348570</td>\n",
       "      <td>5.877785</td>\n",
       "      <td>3.391975</td>\n",
       "      <td>7.397610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.198673</td>\n",
       "      <td>3.630985</td>\n",
       "      <td>4.009603</td>\n",
       "      <td>4.409003</td>\n",
       "      <td>3.997053</td>\n",
       "      <td>4.066888</td>\n",
       "      <td>3.725693</td>\n",
       "      <td>2.784239</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>21.284553</td>\n",
       "      <td>6.179102</td>\n",
       "      <td>3.219512</td>\n",
       "      <td>6.975610</td>\n",
       "      <td>164.084221</td>\n",
       "      <td>83.736404</td>\n",
       "      <td>1.363989</td>\n",
       "      <td>6.230333</td>\n",
       "      <td>4.664747</td>\n",
       "      <td>7.732390</td>\n",
       "      <td>...</td>\n",
       "      <td>8.626406</td>\n",
       "      <td>4.923624</td>\n",
       "      <td>5.447814</td>\n",
       "      <td>5.972218</td>\n",
       "      <td>6.539676</td>\n",
       "      <td>7.003520</td>\n",
       "      <td>7.460310</td>\n",
       "      <td>7.890512</td>\n",
       "      <td>8.276919</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>19.658537</td>\n",
       "      <td>5.914988</td>\n",
       "      <td>3.048780</td>\n",
       "      <td>5.756098</td>\n",
       "      <td>166.863082</td>\n",
       "      <td>77.226327</td>\n",
       "      <td>1.395619</td>\n",
       "      <td>5.985012</td>\n",
       "      <td>2.447832</td>\n",
       "      <td>7.443992</td>\n",
       "      <td>...</td>\n",
       "      <td>6.598232</td>\n",
       "      <td>3.886705</td>\n",
       "      <td>4.389809</td>\n",
       "      <td>4.926801</td>\n",
       "      <td>5.261718</td>\n",
       "      <td>5.494090</td>\n",
       "      <td>5.865848</td>\n",
       "      <td>6.204873</td>\n",
       "      <td>6.369954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>17.720930</td>\n",
       "      <td>5.750228</td>\n",
       "      <td>2.697674</td>\n",
       "      <td>4.604651</td>\n",
       "      <td>164.527087</td>\n",
       "      <td>69.324920</td>\n",
       "      <td>1.357124</td>\n",
       "      <td>5.825972</td>\n",
       "      <td>3.025840</td>\n",
       "      <td>7.340613</td>\n",
       "      <td>...</td>\n",
       "      <td>5.196423</td>\n",
       "      <td>3.676301</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.647990</td>\n",
       "      <td>5.085665</td>\n",
       "      <td>5.169063</td>\n",
       "      <td>5.358942</td>\n",
       "      <td>5.594479</td>\n",
       "      <td>5.380329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>17.225806</td>\n",
       "      <td>5.579190</td>\n",
       "      <td>2.548387</td>\n",
       "      <td>4.193548</td>\n",
       "      <td>161.392477</td>\n",
       "      <td>67.383958</td>\n",
       "      <td>1.452391</td>\n",
       "      <td>5.679165</td>\n",
       "      <td>2.590502</td>\n",
       "      <td>7.175556</td>\n",
       "      <td>...</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.349904</td>\n",
       "      <td>3.682610</td>\n",
       "      <td>4.071161</td>\n",
       "      <td>4.447785</td>\n",
       "      <td>4.003918</td>\n",
       "      <td>4.150055</td>\n",
       "      <td>4.274928</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>25.841270</td>\n",
       "      <td>6.257190</td>\n",
       "      <td>3.301587</td>\n",
       "      <td>7.499118</td>\n",
       "      <td>165.929853</td>\n",
       "      <td>102.114039</td>\n",
       "      <td>1.475191</td>\n",
       "      <td>6.312625</td>\n",
       "      <td>4.628210</td>\n",
       "      <td>7.790568</td>\n",
       "      <td>...</td>\n",
       "      <td>8.508140</td>\n",
       "      <td>4.551242</td>\n",
       "      <td>5.176856</td>\n",
       "      <td>5.812076</td>\n",
       "      <td>6.371825</td>\n",
       "      <td>6.850507</td>\n",
       "      <td>7.327036</td>\n",
       "      <td>7.760788</td>\n",
       "      <td>8.122028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>19.416667</td>\n",
       "      <td>6.040173</td>\n",
       "      <td>2.826389</td>\n",
       "      <td>5.763889</td>\n",
       "      <td>164.563288</td>\n",
       "      <td>76.121662</td>\n",
       "      <td>1.298125</td>\n",
       "      <td>6.090735</td>\n",
       "      <td>4.419416</td>\n",
       "      <td>7.632631</td>\n",
       "      <td>...</td>\n",
       "      <td>6.257668</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>5.105945</td>\n",
       "      <td>5.313206</td>\n",
       "      <td>5.537334</td>\n",
       "      <td>5.620401</td>\n",
       "      <td>5.805135</td>\n",
       "      <td>5.976351</td>\n",
       "      <td>6.091310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>22.877193</td>\n",
       "      <td>5.824621</td>\n",
       "      <td>2.912281</td>\n",
       "      <td>5.130604</td>\n",
       "      <td>162.065226</td>\n",
       "      <td>90.113408</td>\n",
       "      <td>1.522993</td>\n",
       "      <td>5.913365</td>\n",
       "      <td>2.939753</td>\n",
       "      <td>7.426243</td>\n",
       "      <td>...</td>\n",
       "      <td>5.575239</td>\n",
       "      <td>4.056123</td>\n",
       "      <td>4.501198</td>\n",
       "      <td>4.960657</td>\n",
       "      <td>5.184939</td>\n",
       "      <td>5.169418</td>\n",
       "      <td>5.403803</td>\n",
       "      <td>5.604422</td>\n",
       "      <td>5.480118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>18.912281</td>\n",
       "      <td>5.693595</td>\n",
       "      <td>2.929825</td>\n",
       "      <td>5.228070</td>\n",
       "      <td>158.876430</td>\n",
       "      <td>74.254334</td>\n",
       "      <td>1.530849</td>\n",
       "      <td>5.798249</td>\n",
       "      <td>2.397661</td>\n",
       "      <td>7.270916</td>\n",
       "      <td>...</td>\n",
       "      <td>7.533869</td>\n",
       "      <td>4.255613</td>\n",
       "      <td>4.826312</td>\n",
       "      <td>5.403240</td>\n",
       "      <td>5.979993</td>\n",
       "      <td>6.177425</td>\n",
       "      <td>6.654314</td>\n",
       "      <td>7.047680</td>\n",
       "      <td>7.348266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>25.464286</td>\n",
       "      <td>6.616246</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>163.099820</td>\n",
       "      <td>100.684505</td>\n",
       "      <td>1.421288</td>\n",
       "      <td>6.649411</td>\n",
       "      <td>7.740823</td>\n",
       "      <td>8.113419</td>\n",
       "      <td>...</td>\n",
       "      <td>7.724212</td>\n",
       "      <td>4.424847</td>\n",
       "      <td>5.028803</td>\n",
       "      <td>5.577369</td>\n",
       "      <td>6.031136</td>\n",
       "      <td>6.363244</td>\n",
       "      <td>6.711588</td>\n",
       "      <td>7.060610</td>\n",
       "      <td>7.424184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>19.133333</td>\n",
       "      <td>5.909660</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>165.378392</td>\n",
       "      <td>75.050748</td>\n",
       "      <td>1.361830</td>\n",
       "      <td>5.976143</td>\n",
       "      <td>4.137963</td>\n",
       "      <td>7.473203</td>\n",
       "      <td>...</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.481240</td>\n",
       "      <td>3.907010</td>\n",
       "      <td>4.281861</td>\n",
       "      <td>4.720172</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>4.320816</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.342993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>19.211268</td>\n",
       "      <td>5.922814</td>\n",
       "      <td>2.788732</td>\n",
       "      <td>5.802817</td>\n",
       "      <td>163.585967</td>\n",
       "      <td>75.352194</td>\n",
       "      <td>1.368851</td>\n",
       "      <td>5.990213</td>\n",
       "      <td>3.049394</td>\n",
       "      <td>7.502897</td>\n",
       "      <td>...</td>\n",
       "      <td>5.234112</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>4.791650</td>\n",
       "      <td>5.245707</td>\n",
       "      <td>5.667723</td>\n",
       "      <td>5.793776</td>\n",
       "      <td>5.960844</td>\n",
       "      <td>5.839187</td>\n",
       "      <td>5.411088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>27.312500</td>\n",
       "      <td>6.923266</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>170.933703</td>\n",
       "      <td>108.146947</td>\n",
       "      <td>1.321618</td>\n",
       "      <td>6.921481</td>\n",
       "      <td>8.122396</td>\n",
       "      <td>8.348757</td>\n",
       "      <td>...</td>\n",
       "      <td>6.003578</td>\n",
       "      <td>3.921973</td>\n",
       "      <td>4.409763</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.348595</td>\n",
       "      <td>5.559479</td>\n",
       "      <td>5.495630</td>\n",
       "      <td>5.703366</td>\n",
       "      <td>5.926259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>31.785714</td>\n",
       "      <td>6.290479</td>\n",
       "      <td>3.357143</td>\n",
       "      <td>7.658730</td>\n",
       "      <td>161.230794</td>\n",
       "      <td>126.116779</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>6.371939</td>\n",
       "      <td>5.511035</td>\n",
       "      <td>7.836350</td>\n",
       "      <td>...</td>\n",
       "      <td>5.931955</td>\n",
       "      <td>3.789855</td>\n",
       "      <td>4.367864</td>\n",
       "      <td>4.954506</td>\n",
       "      <td>5.430442</td>\n",
       "      <td>5.715330</td>\n",
       "      <td>6.065365</td>\n",
       "      <td>6.387320</td>\n",
       "      <td>5.916728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>20.761905</td>\n",
       "      <td>6.057493</td>\n",
       "      <td>3.119048</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>164.681027</td>\n",
       "      <td>81.668748</td>\n",
       "      <td>1.401802</td>\n",
       "      <td>6.120762</td>\n",
       "      <td>3.509921</td>\n",
       "      <td>7.596294</td>\n",
       "      <td>...</td>\n",
       "      <td>6.846092</td>\n",
       "      <td>3.974998</td>\n",
       "      <td>4.517704</td>\n",
       "      <td>5.125079</td>\n",
       "      <td>5.599347</td>\n",
       "      <td>5.933364</td>\n",
       "      <td>6.335871</td>\n",
       "      <td>6.708575</td>\n",
       "      <td>6.648544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105 rows × 1525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature-0  feature-1  feature-2  feature-3   feature-4   feature-5  \\\n",
       "0     37.977273   6.758452   3.636364  10.792929  160.801682  151.109783   \n",
       "1     19.408163   5.933978   2.816327   5.877551  162.949911   76.153796   \n",
       "2     40.265306   7.425645   3.734694  13.160998  172.099640  161.790879   \n",
       "3     43.976744   7.648293   3.837209  14.392765  168.885456  175.277251   \n",
       "4     24.320988   6.534011   3.567901   8.913580  163.076959   96.019681   \n",
       "5     20.924051   6.134299   3.037975   6.506329  165.707039   82.761541   \n",
       "6     34.150000   6.740695   3.733333  10.214815  164.252922  135.639059   \n",
       "7     23.833333   6.395508   3.141026   8.717949  163.221967   94.106131   \n",
       "8     32.380952   6.152543   2.857143   6.402116  164.380868  128.391104   \n",
       "9     45.228571   6.608449   3.714286   9.180952  159.167580  180.141749   \n",
       "10    24.757143   6.241471   3.071429   6.949206  167.005923   97.692813   \n",
       "11    32.096774   7.206768   3.838710  13.806452  160.469926  127.646528   \n",
       "12    29.833333   6.436864   3.476190   8.296296  162.859109  118.257591   \n",
       "13    36.500000   6.700508   3.653846  10.709402  163.669041  145.244215   \n",
       "14    38.851852   7.402900   3.777778  12.979424  170.018323  154.527750   \n",
       "15    27.534483   6.269883   3.500000   7.973180  161.381053  109.038626   \n",
       "16    23.837500   6.722804   3.125000   7.875000  172.043543   93.939718   \n",
       "17    55.263158   7.389305   3.684211  12.257310  170.139845  220.606751   \n",
       "18    43.057692   7.061350   3.942308  11.871795  163.441127  171.586122   \n",
       "19    42.886792   7.052642   3.943396  12.100629  164.829574  170.922688   \n",
       "20    35.761905   6.858500   2.666667   9.640212  168.739205  141.875852   \n",
       "21    21.528571   6.482779   2.942857   6.400000  171.920444   84.572241   \n",
       "22    23.054795   6.395890   3.164384   8.301370  162.696886   90.885447   \n",
       "23    19.160000   6.067396   2.560000   5.920000  167.413784   75.058797   \n",
       "24    17.131868   5.762305   3.000000   4.109890  165.080662   66.885356   \n",
       "25    28.432432   6.603514   4.324324  11.027027  153.571767  112.900411   \n",
       "26    21.500000   6.207650   2.954545   7.272727  164.257367   85.172313   \n",
       "27    28.428571   6.252383   2.885714   7.384127  164.322747  112.473778   \n",
       "28    38.658537   6.752610   3.682927  10.753388  159.812730  153.837907   \n",
       "29    21.670588   6.292286   2.882353   6.776471  167.982829   85.273347   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "1075  23.849057   6.443296   3.584906   8.490566  165.656310   94.855312   \n",
       "1076  20.528302   6.044432   3.603774   6.075472  162.424590   80.699025   \n",
       "1077  20.440000   5.968498   3.700000   6.080000  161.064334   80.380964   \n",
       "1078  24.333333   5.957881   3.062500   5.509259  164.383290   95.963738   \n",
       "1079  23.375000   6.322417   3.203125   8.375000  162.588954   92.261393   \n",
       "1080  41.800000   9.073735   3.350000  21.400000  180.387654  166.508642   \n",
       "1081  18.390244   5.605241   3.000000   4.731707  158.624476   72.159797   \n",
       "1082  23.458333   6.397181   3.500000   8.375000  166.060607   93.350066   \n",
       "1083  25.809524   5.873886   3.380952   5.962963  162.051123  102.034265   \n",
       "1084  21.414634   5.975237   3.487805   6.926829  161.339153   84.405257   \n",
       "1085  19.000000   5.884906   2.759259   5.370370  162.831246   74.502082   \n",
       "1086  23.318182   6.330339   3.363636   8.045455  163.149857   92.872249   \n",
       "1087  20.560000   6.123796   2.680000   5.920000  165.088853   80.788719   \n",
       "1088  22.380952   6.182614   3.349206   7.396825  164.113227   89.449143   \n",
       "1089  25.161290   5.774461   2.806452   4.143369  166.212698   99.236969   \n",
       "1090  18.074074   5.806478   2.555556   4.518519  164.385174   70.742453   \n",
       "1091  21.284553   6.179102   3.219512   6.975610  164.084221   83.736404   \n",
       "1092  19.658537   5.914988   3.048780   5.756098  166.863082   77.226327   \n",
       "1093  17.720930   5.750228   2.697674   4.604651  164.527087   69.324920   \n",
       "1094  17.225806   5.579190   2.548387   4.193548  161.392477   67.383958   \n",
       "1095  25.841270   6.257190   3.301587   7.499118  165.929853  102.114039   \n",
       "1096  19.416667   6.040173   2.826389   5.763889  164.563288   76.121662   \n",
       "1097  22.877193   5.824621   2.912281   5.130604  162.065226   90.113408   \n",
       "1098  18.912281   5.693595   2.929825   5.228070  158.876430   74.254334   \n",
       "1099  25.464286   6.616246   3.750000   9.285714  163.099820  100.684505   \n",
       "1100  19.133333   5.909660   2.733333   5.466667  165.378392   75.050748   \n",
       "1101  19.211268   5.922814   2.788732   5.802817  163.585967   75.352194   \n",
       "1102  27.312500   6.923266   3.406250  10.750000  170.933703  108.146947   \n",
       "1103  31.785714   6.290479   3.357143   7.658730  161.230794  126.116779   \n",
       "1104  20.761905   6.057493   3.119048   6.476190  164.681027   81.668748   \n",
       "\n",
       "      feature-6  feature-7  feature-8  feature-9 ...   feature-1515  \\\n",
       "0      1.791689   6.818675   8.138413   8.270161 ...       5.658393   \n",
       "1      1.381401   6.002651   5.080499   7.514421 ...       4.830811   \n",
       "2      1.603976   7.410120  10.114794   8.805738 ...       6.397659   \n",
       "3      1.622298   7.629033  12.180817   9.070719 ...       5.879135   \n",
       "4      1.380679   6.566695   4.417010   8.058783 ...       8.148663   \n",
       "5      1.381957   6.187547   4.684599   7.660347 ...       6.087556   \n",
       "6      1.620887   6.781702   8.631090   8.248393 ...       6.198225   \n",
       "7      1.435936   6.443753   5.834402   7.904135 ...       6.582328   \n",
       "8      1.687697   6.232890   4.476844   7.736528 ...       0.000000   \n",
       "9      1.981354   6.690537   8.428546   8.221041 ...       5.214936   \n",
       "10     1.408460   6.289021   5.919754   7.789862 ...       6.175997   \n",
       "11     1.591140   7.228381  11.071685   8.598289 ...       7.242977   \n",
       "12     1.600911   6.497038   6.283812   7.960386 ...       5.658611   \n",
       "13     1.785651   6.764423   7.295706   8.158660 ...       5.768126   \n",
       "14     1.476580   7.381493  11.933931   8.867678 ...       3.218876   \n",
       "15     1.618478   6.345124   4.994148   7.791228 ...       8.934004   \n",
       "16     1.156748   6.710586   6.512587   8.234018 ...       5.638355   \n",
       "17     1.969099   7.412105  10.847813   8.885381 ...       0.000000   \n",
       "18     1.834505   7.104088   9.456211   8.542274 ...       6.706174   \n",
       "19     1.833416   7.095513   8.143800   8.514148 ...       6.976085   \n",
       "20     1.456355   6.864043  11.470610   8.439979 ...       0.000000   \n",
       "21     1.127633   6.481077   5.269841   8.031493 ...       5.036953   \n",
       "22     1.370996   6.435699   6.796613   7.942436 ...       6.182085   \n",
       "23     1.241288   6.107552   6.423333   7.651508 ...       0.000000   \n",
       "24     1.294529   5.828765   2.738324   7.375662 ...       6.448889   \n",
       "25     1.738632   6.683730   6.055743   8.059202 ...       8.293510   \n",
       "26     1.392726   6.258106   5.808081   7.738226 ...       7.443490   \n",
       "27     1.536709   6.312869   6.333953   7.825781 ...       3.791267   \n",
       "28     1.814410   6.815551   8.392444   8.278695 ...       5.800228   \n",
       "29     1.296235   6.326075   4.991830   7.822375 ...       6.577992   \n",
       "...         ...        ...        ...        ... ...            ...   \n",
       "1075   1.429258   6.483247   5.732180   7.917090 ...       8.145414   \n",
       "1076   1.402911   6.109594   3.789570   7.610526 ...       7.481855   \n",
       "1077   1.456437   6.045898   3.411944   7.531384 ...       7.520150   \n",
       "1078   1.496938   6.033144   3.958207   7.545887 ...       7.057353   \n",
       "1079   1.451475   6.377366   5.672743   7.837564 ...       6.824985   \n",
       "1080   1.000430   8.896735  19.768056  10.349211 ...       0.000000   \n",
       "1081   1.549658   5.717783   2.198509   7.184370 ...       7.163209   \n",
       "1082   1.430009   6.439448   5.322917   7.869501 ...       8.177485   \n",
       "1083   1.656037   5.975364   2.317542   7.431153 ...       7.555979   \n",
       "1084   1.534660   6.062522   3.468157   7.496018 ...       7.446209   \n",
       "1085   1.381487   5.956572   3.660108   7.471168 ...       5.514688   \n",
       "1086   1.484800   6.384780   4.174874   7.815815 ...       6.172744   \n",
       "1087   1.335059   6.174052   4.833333   7.684390 ...       0.000000   \n",
       "1088   1.521852   6.248717   4.873016   7.649627 ...       7.231355   \n",
       "1089   1.531852   5.860845   2.388103   7.386058 ...       3.954843   \n",
       "1090   1.348570   5.877785   3.391975   7.397610 ...       0.000000   \n",
       "1091   1.363989   6.230333   4.664747   7.732390 ...       8.626406   \n",
       "1092   1.395619   5.985012   2.447832   7.443992 ...       6.598232   \n",
       "1093   1.357124   5.825972   3.025840   7.340613 ...       5.196423   \n",
       "1094   1.452391   5.679165   2.590502   7.175556 ...       3.840795   \n",
       "1095   1.475191   6.312625   4.628210   7.790568 ...       8.508140   \n",
       "1096   1.298125   6.090735   4.419416   7.632631 ...       6.257668   \n",
       "1097   1.522993   5.913365   2.939753   7.426243 ...       5.575239   \n",
       "1098   1.530849   5.798249   2.397661   7.270916 ...       7.533869   \n",
       "1099   1.421288   6.649411   7.740823   8.113419 ...       7.724212   \n",
       "1100   1.361830   5.976143   4.137963   7.473203 ...       3.446011   \n",
       "1101   1.368851   5.990213   3.049394   7.502897 ...       5.234112   \n",
       "1102   1.321618   6.921481   8.122396   8.348757 ...       6.003578   \n",
       "1103   1.718917   6.371939   5.511035   7.836350 ...       5.931955   \n",
       "1104   1.401802   6.120762   3.509921   7.596294 ...       6.846092   \n",
       "\n",
       "      feature-1516  feature-1517  feature-1518  feature-1519  feature-1520  \\\n",
       "0         4.151040      4.540632      4.953183      5.351562      5.311048   \n",
       "1         3.817712      4.123094      4.426343      4.823804      4.652173   \n",
       "2         4.223177      4.685597      5.116870      5.333926      5.504569   \n",
       "3         4.280132      4.563045      5.007714      5.159773      5.393628   \n",
       "4         4.624973      5.173321      5.720312      6.259342      6.626469   \n",
       "5         4.430817      4.820282      5.183187      5.595176      5.489454   \n",
       "6         4.471639      4.801970      5.237107      5.493833      5.573816   \n",
       "7         4.600158      5.032071      5.499726      5.978728      5.995208   \n",
       "8         3.449988      3.865979      4.506730      4.765906      4.965028   \n",
       "9         3.828641      4.234107      4.682131      4.890349      5.192957   \n",
       "10        4.363099      4.716264      5.155457      5.591686      5.680173   \n",
       "11        4.094345      4.639572      5.151845      5.678037      5.850765   \n",
       "12        4.051785      4.519067      4.935373      5.281616      5.221369   \n",
       "13        3.931826      4.356709      4.844187      5.326662      5.340239   \n",
       "14        3.688879      3.761200      4.110874      4.442651      4.406719   \n",
       "15        4.644391      5.241747      5.930586      6.610360      7.088878   \n",
       "16        4.234107      4.532599      4.804021      5.043425      5.181784   \n",
       "17        3.617652      3.868593      4.357510      4.523146      4.789573   \n",
       "18        4.508108      4.898772      5.374989      5.679959      5.755742   \n",
       "19        4.572130      5.115746      5.566195      5.936711      6.080505   \n",
       "20        3.135494      2.484907      2.397895      2.302585      2.197225   \n",
       "21        3.931826      4.234107      4.442651      4.672829      4.753590   \n",
       "22        4.330733      4.605170      4.912655      5.181784      5.393628   \n",
       "23        2.833213      2.944439      2.944439      3.044522      3.044522   \n",
       "24        4.262680      4.624973      5.010635      5.351858      5.590987   \n",
       "25        4.363099      5.012301      5.675469      6.310259      6.686641   \n",
       "26        4.304065      4.828314      5.422745      6.018593      6.373640   \n",
       "27        3.772761      4.081766      4.514972      4.873765      4.997212   \n",
       "28        4.098503      4.492841      4.938513      5.120237      5.262042   \n",
       "29        4.385147      4.798885      5.216633      5.602695      5.857442   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1075      4.375757      5.032071      5.702949      6.339698      6.834210   \n",
       "1076      4.151040      4.734003      5.287636      5.850225      6.211102   \n",
       "1077      4.166665      4.725173      5.302683      5.881406      6.250458   \n",
       "1078      4.131159      4.601413      5.158696      5.690359      6.054403   \n",
       "1079      4.430817      4.919981      5.421641      5.892335      6.042336   \n",
       "1080      3.178054      2.890372      2.995732      3.135494      2.995732   \n",
       "1081      3.951244      4.559126      5.140200      5.685703      5.885409   \n",
       "1082      4.297285      4.978456      5.658611      6.322790      6.818753   \n",
       "1083      4.073291      4.650383      5.246695      5.735564      6.103816   \n",
       "1084      4.212128      4.888468      5.525951      5.975081      6.299380   \n",
       "1085      3.901973      4.327438      4.732904      5.096431      5.080239   \n",
       "1086      4.060443      4.569543      5.057837      5.539301      5.568821   \n",
       "1087      3.238678      3.669951      4.112921      4.525180      4.251170   \n",
       "1088      4.545951      5.133590      5.739994      6.264469      6.422816   \n",
       "1089      3.409496      3.868593      4.382808      4.539297      4.549261   \n",
       "1090      3.198673      3.630985      4.009603      4.409003      3.997053   \n",
       "1091      4.923624      5.447814      5.972218      6.539676      7.003520   \n",
       "1092      3.886705      4.389809      4.926801      5.261718      5.494090   \n",
       "1093      3.676301      4.123094      4.647990      5.085665      5.169063   \n",
       "1094      3.349904      3.682610      4.071161      4.447785      4.003918   \n",
       "1095      4.551242      5.176856      5.812076      6.371825      6.850507   \n",
       "1096      4.762174      5.105945      5.313206      5.537334      5.620401   \n",
       "1097      4.056123      4.501198      4.960657      5.184939      5.169418   \n",
       "1098      4.255613      4.826312      5.403240      5.979993      6.177425   \n",
       "1099      4.424847      5.028803      5.577369      6.031136      6.363244   \n",
       "1100      3.481240      3.907010      4.281861      4.720172      4.574711   \n",
       "1101      4.248495      4.791650      5.245707      5.667723      5.793776   \n",
       "1102      3.921973      4.409763      4.898772      5.348595      5.559479   \n",
       "1103      3.789855      4.367864      4.954506      5.430442      5.715330   \n",
       "1104      3.974998      4.517704      5.125079      5.599347      5.933364   \n",
       "\n",
       "      feature-1521  feature-1522  feature-1523    y  \n",
       "0         5.560922      5.643015      5.715999  0.0  \n",
       "1         4.795274      4.860781      5.001426  0.0  \n",
       "2         5.797956      6.009581      6.200889  0.0  \n",
       "3         5.640132      5.472271      5.741399  0.0  \n",
       "4         7.062406      7.472998      7.829842  0.0  \n",
       "5         5.604998      5.847522      5.987080  0.0  \n",
       "6         5.764799      5.865760      5.998937  0.0  \n",
       "7         6.179952      6.364051      6.481290  0.0  \n",
       "8         3.840795      3.595598      0.000000  0.0  \n",
       "9         5.342334      5.402677      5.303305  0.0  \n",
       "10        5.977302      6.030986      6.214671  0.0  \n",
       "11        6.134888      6.451753      6.793466  0.0  \n",
       "12        5.465948      5.520210      5.499982  0.0  \n",
       "13        5.395331      5.454787      5.557552  0.0  \n",
       "14        4.543295      4.605170      4.290459  0.0  \n",
       "15        7.641069      8.134765      8.607916  0.0  \n",
       "16        5.337538      5.497168      5.568345  0.0  \n",
       "17        4.523146      3.944006      0.000000  0.0  \n",
       "18        6.060582      6.240763      6.434747  0.0  \n",
       "19        6.198796      6.333335      6.652742  0.0  \n",
       "20        2.772589      0.000000      0.000000  0.0  \n",
       "21        4.762174      4.890349      4.969813  0.0  \n",
       "22        5.652489      5.831882      6.040255  0.0  \n",
       "23        2.708050      0.000000      0.000000  0.0  \n",
       "24        5.834811      6.077642      6.293419  0.0  \n",
       "25        7.157565      7.635515      8.018008  0.0  \n",
       "26        6.751321      7.048332      7.302612  0.0  \n",
       "27        4.848606      4.455074      4.352694  0.0  \n",
       "28        5.561162      5.568821      5.681878  0.0  \n",
       "29        6.151851      6.484856      6.339973  0.0  \n",
       "...            ...           ...           ...  ...  \n",
       "1075      7.294229      7.631557      8.007074  1.0  \n",
       "1076      6.591717      6.991033      7.274674  1.0  \n",
       "1077      6.631384      7.030719      7.325005  1.0  \n",
       "1078      6.455334      6.873095      6.939417  1.0  \n",
       "1079      6.333613      6.572807      6.736893  1.0  \n",
       "1080      3.218876      2.302585      0.000000  1.0  \n",
       "1081      6.272759      6.609097      6.919128  1.0  \n",
       "1082      7.288522      7.655010      8.058613  1.0  \n",
       "1083      6.545754      6.979000      7.346393  1.0  \n",
       "1084      6.681394      7.067183      7.171165  1.0  \n",
       "1085      5.365684      5.482980      5.400140  1.0  \n",
       "1086      5.728170      5.901779      5.917717  1.0  \n",
       "1087      4.104707      3.446011      3.056357  1.0  \n",
       "1088      6.771282      7.089077      7.031227  1.0  \n",
       "1089      4.743845      4.850075      4.323304  1.0  \n",
       "1090      4.066888      3.725693      2.784239  1.0  \n",
       "1091      7.460310      7.890512      8.276919  1.0  \n",
       "1092      5.865848      6.204873      6.369954  1.0  \n",
       "1093      5.358942      5.594479      5.380329  1.0  \n",
       "1094      4.150055      4.274928      4.123094  1.0  \n",
       "1095      7.327036      7.760788      8.122028  1.0  \n",
       "1096      5.805135      5.976351      6.091310  1.0  \n",
       "1097      5.403803      5.604422      5.480118  1.0  \n",
       "1098      6.654314      7.047680      7.348266  1.0  \n",
       "1099      6.711588      7.060610      7.424184  1.0  \n",
       "1100      4.320816      4.406719      4.342993  1.0  \n",
       "1101      5.960844      5.839187      5.411088  1.0  \n",
       "1102      5.495630      5.703366      5.926259  1.0  \n",
       "1103      6.065365      6.387320      5.916728  1.0  \n",
       "1104      6.335871      6.708575      6.648544  1.0  \n",
       "\n",
       "[1105 rows x 1525 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _data_ = pd.read_csv('./data/train.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:05.166889Z",
     "start_time": "2018-04-30T15:11:04.912138Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48da94f8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.y.unique())\n",
    "plt.hist(data.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:07.103087Z",
     "start_time": "2018-04-30T15:11:07.005779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "790 315 0.7149321266968326\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum().sum())\n",
    "\n",
    "num_1 = data[data.y == 1].shape[0] \n",
    "num_0 = data[data.y == 0].shape[0]\n",
    "\n",
    "print(num_1, num_0, num_1 / (num_0 + num_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:09.176385Z",
     "start_time": "2018-04-30T15:11:09.013167Z"
    }
   },
   "outputs": [],
   "source": [
    "def weight_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.random_normal(shape=shape,\n",
    "                                                              stddev=0.1))\n",
    "def bias_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.constant(0.1, shape=shape))\n",
    "\n",
    "def elastic_net(x, l1, l2):\n",
    "    return l1 * ( (1-l2) / 2 * tf.norm(x, 2) ** 2 + \n",
    "                   l2 * tf.norm(x, 1))\n",
    "\n",
    "def batch_data(*matrxs, batch_size):\n",
    "    for batch_i in range(matrxs[0].shape[0] // batch_size):\n",
    "        yield tuple(x.iloc[batch_i * batch_size : (batch_i + 1) * batch_size] \n",
    "                     for x in matrxs)\n",
    "        \n",
    "def split_data(data, train_size=0.5, test_size=0.25, validate_size=0.25):\n",
    "    train_data, test_val_data = ms.train_test_split(data, train_size=train_size)\n",
    "    test_data, val_data = ms.train_test_split(test_val_data,\n",
    "                                              train_size=test_size / (test_size+validate_size))\n",
    "    \n",
    "    train_X, train_y = train_data.drop('y', axis=1), train_data[['y']]\n",
    "    test_X, test_y = test_data.drop('y', axis=1), test_data[['y']]\n",
    "    validate_X, validate_y = val_data.drop('y', axis=1), val_data[['y']]\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y, validate_X, validate_y)\n",
    "\n",
    "def pp_pipeline(data):\n",
    "    scaler = pp.StandardScaler()\n",
    "    \n",
    "    data_pp = scaler.fit_transform(data)\n",
    "    \n",
    "    data_pp = pd.DataFrame(data_pp,\n",
    "                        index=data.index,\n",
    "                        columns=data.columns)\n",
    "    \n",
    "    #restore target\n",
    "    data_pp.y = data.y\n",
    "    \n",
    "    return data_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:14.191387Z",
     "start_time": "2018-04-30T15:11:13.473629Z"
    }
   },
   "outputs": [],
   "source": [
    "class DFS:\n",
    "    def __init__(self, layers_sizes, batch_size=32, lambda1=1e-3, lambda2=1.,\n",
    "                 alpha1=1e-3, alpha2=0.):\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.num_layers = len(layers_sizes)\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, num_epochs=10, test_data=None):\n",
    "        self._build_graph_(X.shape[1])\n",
    "        self.features = X.columns #Persisting for `select_most_important_ftrs`\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(num_epochs):\n",
    "            X_cur = X.sample(frac=1, random_state=epoch_i)\n",
    "            y_cur = y.sample(frac=1, random_state=epoch_i)\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in batch_data(X_cur, y_cur, \n",
    "                                               batch_size=self.batch_size):\n",
    "                train_loss, _ = self.sess.run([self.total_loss, self.train_step],\n",
    "                                               feed_dict = {self.x: batch_X,\n",
    "                                                            self.y: batch_y})\n",
    "                epoch_loss += train_loss\n",
    "            epoch_loss /= X.shape[0] // self.batch_size\n",
    "            \n",
    "            train_predict = self.predict(X_cur)\n",
    "            train_accuracy = mtcs.accuracy_score(y_cur, train_predict)\n",
    "            if test_data is not None:\n",
    "                test_X, test_y = test_data\n",
    "                test_predict = self.predict(test_X)\n",
    "                test_accuracy = mtcs.accuracy_score(test_y, test_predict)\n",
    "                print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}.\"\n",
    "                      f\"Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "            else:\n",
    "                print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}. \"\n",
    "                      f\"Train accuracy: {train_accuracy}.\")\n",
    "       \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions_proba = self.sess.run(self.predictions, feed_dict={self.x: X})\n",
    "        \n",
    "        return predictions_proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions_proba = self.predict_proba(X)\n",
    "        \n",
    "        return list(map(np.argmax, predictions_proba))\n",
    "    \n",
    "    \n",
    "    def get_features_weights(self):\n",
    "        weights = self.sess.run(self.features_weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs(self, features, N):\n",
    "        weights = self.get_features_weights()\n",
    "        feature_weight = sorted(zip(weights, features), \n",
    "                                key=lambda x: abs(x[0]))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight[-N:])\n",
    "        \n",
    "        \n",
    "    def _build_graph_(self, num_features):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        ###Placeholders \n",
    "        x = tf.placeholder(tf.float32, [None, num_features], 'x_ph')\n",
    "        y = tf.placeholder(tf.int32, [None], 'y_ph')\n",
    "        \n",
    "        ###Weights initialization\n",
    "        w = tf.get_variable(\"dfs_features_weight\", \n",
    "                            initializer = tf.constant(1., shape=[num_features]))\n",
    "        self.layers_sizes = [num_features] + self.layers_sizes\n",
    "        W, b = [], []\n",
    "        for layer_i in range(self.num_layers):\n",
    "            W.append(weight_init(f\"layer_{layer_i}_weights\",\n",
    "                                 shape=[self.layers_sizes[layer_i],\n",
    "                                        self.layers_sizes[layer_i+1]]))\n",
    "            b.append(bias_init(f\"layer_{layer_i}_bias\",\n",
    "                               shape=[self.layers_sizes[layer_i+1]]))\n",
    "        \n",
    "        ###Input transformations\n",
    "        logits = x * w #feature selection\n",
    "        for layer_i in range(self.num_layers):\n",
    "            if layer_i != self.num_layers - 1:\n",
    "                logits = tf.nn.tanh(tf.matmul(logits, W[layer_i]) + b[layer_i])\n",
    "            else:\n",
    "                logits = tf.matmul(logits, W[layer_i]) + b[layer_i]\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "            \n",
    "        ###Loss calculation\n",
    "        logloss = tf.reduce_sum(\n",
    "                        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                                       logits=logits))\n",
    "        w_loss = elastic_net(w, self.lambda1, self.lambda2)\n",
    "        W_loss = tf.reduce_sum([elastic_net(W_i, self.alpha1, self.alpha2) for W_i in W])\n",
    "        \n",
    "        total_loss = tf.reduce_sum(logloss + w_loss + W_loss)\n",
    "        \n",
    "        ###Optimizer\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.predictions = predictions\n",
    "        self.total_loss = total_loss\n",
    "        self.train_step = train_step\n",
    "        self.features_weights = w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:11:15.286893Z",
     "start_time": "2018-04-30T15:11:15.117293Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pp_pipeline(_data_)\n",
    "\n",
    "(train_X, train_y,\n",
    " test_X, test_y,\n",
    " validate_X, validate_y) = split_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:00:10.931302Z",
     "start_time": "2018-04-30T14:59:04.627856Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 18.631548040053424.Train accuracy: 0.8876811594202898. Test accuracy: 0.8152173913043478.\n",
      "==> Epoch: 1. Train loss: 11.625561153187471.Train accuracy: 0.9492753623188406. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 2. Train loss: 8.371415194343118.Train accuracy: 0.9710144927536232. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 3. Train loss: 6.179288415347829.Train accuracy: 0.9891304347826086. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 4. Train loss: 4.687725824468276.Train accuracy: 0.9927536231884058. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 5. Train loss: 3.9291446068707634.Train accuracy: 0.9927536231884058. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 6. Train loss: 3.2915194455315087.Train accuracy: 0.9981884057971014. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 7. Train loss: 3.0706697071299835.Train accuracy: 0.9981884057971014. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 8. Train loss: 2.8810760554145363.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 9. Train loss: 2.771640931858736.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 10. Train loss: 2.7343465440413532.Train accuracy: 0.9981884057971014. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 11. Train loss: 2.6421220723320458.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 12. Train loss: 2.6165234481587127.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 13. Train loss: 2.6588988023645737.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 14. Train loss: 2.6234892115873447.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 15. Train loss: 2.6489501560435578.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 16. Train loss: 2.5354699527516082.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 17. Train loss: 2.527294902240529.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 18. Train loss: 2.479693020091337.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 19. Train loss: 2.4560418549705956.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 20. Train loss: 2.4439096030066993.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 21. Train loss: 2.4270411519443287.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 22. Train loss: 2.4104399540845085.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 23. Train loss: 2.415344406576718.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 24. Train loss: 2.4807472509496353.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 25. Train loss: 2.7026978100047394.Train accuracy: 0.9981884057971014. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 26. Train loss: 2.436591162401087.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 27. Train loss: 2.445960661944221.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 28. Train loss: 2.4056311214671418.Train accuracy: 0.9981884057971014. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 29. Train loss: 2.4737459771773396.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 30. Train loss: 2.406894753961002.Train accuracy: 0.9981884057971014. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 31. Train loss: 2.35559448073892.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 32. Train loss: 2.3594272837919346.Train accuracy: 0.9981884057971014. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 33. Train loss: 2.472551051308127.Train accuracy: 0.9981884057971014. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 34. Train loss: 2.4484608874601475.Train accuracy: 0.9981884057971014. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 35. Train loss: 2.408423381693223.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 36. Train loss: 2.361194091684678.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 37. Train loss: 2.3288841247558594.Train accuracy: 0.9981884057971014. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 38. Train loss: 2.363669479594511.Train accuracy: 0.9981884057971014. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 39. Train loss: 2.3209244503694424.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 40. Train loss: 2.303837678011726.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 41. Train loss: 2.2665011041304646.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 42. Train loss: 2.253369836246266.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 43. Train loss: 2.2561884066637825.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 44. Train loss: 2.296818592969109.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 45. Train loss: 2.3022198116078094.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 46. Train loss: 2.2718373046201816.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 47. Train loss: 2.21813208916608.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 48. Train loss: 2.1873619696673225.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 49. Train loss: 2.1779911097358253.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 50. Train loss: 2.166102170944214.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 51. Train loss: 2.156532469917746.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 52. Train loss: 2.1465881992788876.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 53. Train loss: 2.1374309623942658.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 54. Train loss: 2.127904681598439.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 55. Train loss: 2.1189137627096737.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 56. Train loss: 2.1091580250683952.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 57. Train loss: 2.1003856939427994.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 58. Train loss: 2.091138713500079.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 59. Train loss: 2.0836611355052277.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 60. Train loss: 2.076463264577529.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 61. Train loss: 2.069288688547471.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 62. Train loss: 2.0624157260445988.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 63. Train loss: 2.0544387032003963.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 64. Train loss: 2.0479938422932342.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 65. Train loss: 2.0408785343170166.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 66. Train loss: 2.033511863035314.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 67. Train loss: 2.0263130103840545.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 68. Train loss: 2.0192504069384407.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 69. Train loss: 2.0119355145622704.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 70. Train loss: 2.004752088995541.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 71. Train loss: 1.9974489422405468.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 72. Train loss: 1.990065097808838.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 73. Train loss: 1.9828146836336922.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 74. Train loss: 1.9755257157718433.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 75. Train loss: 1.968077771803912.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 76. Train loss: 1.9607758171418135.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 77. Train loss: 1.9534183319877176.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 78. Train loss: 1.9459743149140303.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 79. Train loss: 1.9384793604121489.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 80. Train loss: 1.931036346098956.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 81. Train loss: 1.923534126842723.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 82. Train loss: 1.9159379776786356.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 83. Train loss: 1.9084740105797262.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 84. Train loss: 1.9007664989022648.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 85. Train loss: 1.8931103734409107.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 86. Train loss: 1.8854799901737886.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 87. Train loss: 1.877676816547618.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 88. Train loss: 1.869778086157406.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 89. Train loss: 1.8622749272514791.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 90. Train loss: 1.8545875899931963.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 91. Train loss: 1.8466762023813583.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 92. Train loss: 1.838818486999063.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 93. Train loss: 1.830969726338106.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 94. Train loss: 1.8230590680066276.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 95. Train loss: 1.8151220363729141.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 96. Train loss: 1.8070740699768066.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 97. Train loss: 1.7991947987500359.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 98. Train loss: 1.791131664724911.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 99. Train loss: 1.7831410800709444.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 100. Train loss: 1.775033628239351.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 101. Train loss: 1.7668796497232773.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 102. Train loss: 1.758821459377513.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 103. Train loss: 1.7506357431411743.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 104. Train loss: 1.7424715897616219.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 105. Train loss: 1.734306629966287.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 106. Train loss: 1.7260468847611372.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 107. Train loss: 1.717787602368523.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 108. Train loss: 1.7094592206618364.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 109. Train loss: 1.7011635583989761.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 110. Train loss: 1.6928006340475643.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 111. Train loss: 1.6844203331891228.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 112. Train loss: 1.676049877615536.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 113. Train loss: 1.667616977411158.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 114. Train loss: 1.6591790423673742.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 115. Train loss: 1.6507254067589254.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 116. Train loss: 1.6422394654330086.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 117. Train loss: 1.6336963246850407.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 118. Train loss: 1.6251966742908253.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 119. Train loss: 1.6165952962987564.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 120. Train loss: 1.6079935887280632.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 121. Train loss: 1.5993829825345207.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 122. Train loss: 1.5907774462419397.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 123. Train loss: 1.5821490498150097.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 124. Train loss: 1.5734558876822977.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 125. Train loss: 1.5648014124702005.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 126. Train loss: 1.556068827124203.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 127. Train loss: 1.5473629867329317.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 128. Train loss: 1.5386299876605762.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 129. Train loss: 1.5298153652864344.Train accuracy: 1.0. Test accuracy: 0.8369565217391305.\n",
      "==> Epoch: 130. Train loss: 1.5210155669380636.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 131. Train loss: 1.5121714998694027.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 132. Train loss: 1.5033256025875317.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 133. Train loss: 1.4944390759748571.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 134. Train loss: 1.485520411940182.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 135. Train loss: 1.476631192600026.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 136. Train loss: 1.4677324224920834.Train accuracy: 1.0. Test accuracy: 0.8405797101449275.\n",
      "==> Epoch: 137. Train loss: 1.4588459589902092.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 138. Train loss: 1.449666647350087.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 139. Train loss: 1.4406051425372852.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 140. Train loss: 1.4316837857751286.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 141. Train loss: 1.4225408820544971.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 142. Train loss: 1.4135744221070234.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 143. Train loss: 1.4045179170720719.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 144. Train loss: 1.3953596914515776.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 145. Train loss: 1.3862142142127543.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 146. Train loss: 1.3770684284322403.Train accuracy: 1.0. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 147. Train loss: 1.3679176919600542.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 148. Train loss: 1.3587157726287842.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 149. Train loss: 1.3495719362707699.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 150. Train loss: 1.340414657312281.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 151. Train loss: 1.3309482055551864.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 152. Train loss: 1.3216957204482134.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 153. Train loss: 1.31232890662025.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 154. Train loss: 1.3030592834248262.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 155. Train loss: 1.2937813857022453.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 156. Train loss: 1.2842971998102524.Train accuracy: 1.0. Test accuracy: 0.8478260869565217.\n",
      "==> Epoch: 157. Train loss: 1.2749339552486645.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 158. Train loss: 1.265490223379696.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 159. Train loss: 1.2560432167614208.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 160. Train loss: 1.2466292661779068.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 161. Train loss: 1.2370232554043041.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 162. Train loss: 1.2276179860619938.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 163. Train loss: 1.2180299899157356.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 164. Train loss: 1.2085862720713896.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 165. Train loss: 1.1989108955158907.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 166. Train loss: 1.1893729392219992.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 167. Train loss: 1.1798295133254106.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 168. Train loss: 1.170226482784047.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 169. Train loss: 1.160530398873722.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 170. Train loss: 1.1509639094857609.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 171. Train loss: 1.1411797649720137.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 172. Train loss: 1.1316349927116842.Train accuracy: 1.0. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 173. Train loss: 1.121775143286761.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 174. Train loss: 1.11209719321307.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 175. Train loss: 1.1022171202827902.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 176. Train loss: 1.0925090242834652.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 177. Train loss: 1.0826576246934778.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 178. Train loss: 1.072891873471877.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 179. Train loss: 1.0629828186596142.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 180. Train loss: 1.053047208225026.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 181. Train loss: 1.0432951310101677.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 182. Train loss: 1.0332457037533032.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 183. Train loss: 1.0232962930903715.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 184. Train loss: 1.0134403425104477.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 185. Train loss: 1.0034078710219438.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 186. Train loss: 0.9933877166579751.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 187. Train loss: 0.9835385119213778.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 188. Train loss: 0.9732355019625496.Train accuracy: 1.0. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 189. Train loss: 0.9634858965873718.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 190. Train loss: 0.953559531885035.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 191. Train loss: 0.9434742366566378.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 192. Train loss: 0.9336498940692228.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 193. Train loss: 0.9232334564713871.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 194. Train loss: 0.9137495265287512.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 195. Train loss: 0.9031483250505784.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 196. Train loss: 0.8929134607315063.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 197. Train loss: 0.8830066884265226.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 198. Train loss: 0.8728131020770353.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 199. Train loss: 0.8626075842801262.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 200. Train loss: 0.8532269281499526.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 201. Train loss: 0.8444301170461318.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 202. Train loss: 6.0996580719947815.Train accuracy: 0.8496376811594203. Test accuracy: 0.8079710144927537.\n",
      "==> Epoch: 203. Train loss: 13.001273043015424.Train accuracy: 0.9293478260869565. Test accuracy: 0.8188405797101449.\n",
      "==> Epoch: 204. Train loss: 5.3332021516912125.Train accuracy: 0.9764492753623188. Test accuracy: 0.8514492753623188.\n",
      "==> Epoch: 205. Train loss: 3.098443199606503.Train accuracy: 0.9909420289855072. Test accuracy: 0.8623188405797102.\n",
      "==> Epoch: 206. Train loss: 1.997283094069537.Train accuracy: 0.9945652173913043. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 207. Train loss: 1.5878270864486694.Train accuracy: 0.9963768115942029. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 208. Train loss: 1.412638618665583.Train accuracy: 0.9981884057971014. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 209. Train loss: 1.1537019084481632.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 210. Train loss: 1.0436962422202616.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 211. Train loss: 0.9803584638763877.Train accuracy: 1.0. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 212. Train loss: 0.94988333828309.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 213. Train loss: 0.937711354564218.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 214. Train loss: 0.9307966688100029.Train accuracy: 1.0. Test accuracy: 0.8586956521739131.\n",
      "==> Epoch: 215. Train loss: 0.924830065054052.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 216. Train loss: 0.9204884872717016.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 217. Train loss: 0.9164942573098576.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 218. Train loss: 0.9130101905149572.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 219. Train loss: 0.9097608468111824.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 220. Train loss: 0.9070148643325356.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 221. Train loss: 0.9042074820574593.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 222. Train loss: 0.9018493925823885.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 223. Train loss: 0.8995184688007131.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 224. Train loss: 0.8971965873942656.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 225. Train loss: 0.8941583387991962.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 226. Train loss: 0.8928286222850575.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 227. Train loss: 0.8904949566897224.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 228. Train loss: 0.8883941629353691.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 229. Train loss: 0.8858536937657524.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 230. Train loss: 0.884427571997923.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 231. Train loss: 0.8822214182685403.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 232. Train loss: 0.8803546253372642.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 233. Train loss: 0.8782482357586131.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 234. Train loss: 0.8763074980062597.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 235. Train loss: 0.8744390045895296.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 236. Train loss: 0.872482576790978.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 237. Train loss: 0.8706496498164009.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 238. Train loss: 0.868712256936466.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 239. Train loss: 0.866637250956367.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 240. Train loss: 0.8648386983310475.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 241. Train loss: 0.8631009108879987.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 242. Train loss: 0.8610959228347329.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 243. Train loss: 0.859296483152053.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 244. Train loss: 0.8573977491434883.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 245. Train loss: 0.8555521509226631.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 246. Train loss: 0.8536995228599099.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 247. Train loss: 0.851829493747038.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 248. Train loss: 0.8499565475127276.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 249. Train loss: 0.8480119775323307.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 250. Train loss: 0.8461257044006797.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 251. Train loss: 0.844269706922419.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 252. Train loss: 0.8424656321020687.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 253. Train loss: 0.8405747553881477.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 254. Train loss: 0.8385884691687191.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 255. Train loss: 0.8368281055899227.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 256. Train loss: 0.8348361954969519.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 257. Train loss: 0.8330128052655388.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 258. Train loss: 0.8311208416433895.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 259. Train loss: 0.829273472813999.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 260. Train loss: 0.827377645408406.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 261. Train loss: 0.825486470671261.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 262. Train loss: 0.8235710193129147.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 263. Train loss: 0.8215435603085686.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 264. Train loss: 0.8196713117992177.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 265. Train loss: 0.817850905306199.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 266. Train loss: 0.8159027941086713.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 267. Train loss: 0.8139494482208701.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 268. Train loss: 0.8120777887456557.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 269. Train loss: 0.8100206080605002.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 270. Train loss: 0.8081821623970481.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 271. Train loss: 0.8061292662340052.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 272. Train loss: 0.8042489079868093.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 273. Train loss: 0.8022826243849361.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 274. Train loss: 0.8002114927067476.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 275. Train loss: 0.7982880893875571.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 276. Train loss: 0.7963292774032144.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 277. Train loss: 0.7942045120631948.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 278. Train loss: 0.7922961361267987.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 279. Train loss: 0.7902648273636314.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 280. Train loss: 0.7882613609818852.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 281. Train loss: 0.7861820950227625.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 282. Train loss: 0.7841472590670866.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 283. Train loss: 0.7820809448466581.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 284. Train loss: 0.7800467364928302.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 285. Train loss: 0.7779827678904814.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 286. Train loss: 0.7758614455952364.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 287. Train loss: 0.7737927436828613.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 288. Train loss: 0.7716818311635185.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 289. Train loss: 0.7695956685963798.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 290. Train loss: 0.7674547994838041.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 291. Train loss: 0.7652608366573558.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 292. Train loss: 0.7632147073745728.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 293. Train loss: 0.7609920361462761.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 294. Train loss: 0.7589177559403812.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 295. Train loss: 0.7567525996881372.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 296. Train loss: 0.7545675039291382.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 297. Train loss: 0.7524159375359031.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 298. Train loss: 0.7502148712382597.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 299. Train loss: 0.7479769868009231.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 300. Train loss: 0.7457615277346443.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 301. Train loss: 0.7435882056460661.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 302. Train loss: 0.7413636200568255.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 303. Train loss: 0.7391427860540503.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 304. Train loss: 0.7368468501988579.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 305. Train loss: 0.7346040080575382.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 306. Train loss: 0.7323768103823942.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 307. Train loss: 0.7300828309620128.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 308. Train loss: 0.727827878559337.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 309. Train loss: 0.725466878975139.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 310. Train loss: 0.7232236686874839.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 311. Train loss: 0.720952735227697.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 312. Train loss: 0.7186254473293529.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 313. Train loss: 0.7162870694609249.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 314. Train loss: 0.7139771440449882.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 315. Train loss: 0.7116420829997343.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 316. Train loss: 0.7093139325871187.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 317. Train loss: 0.7069515130099129.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 318. Train loss: 0.7045312944580527.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 319. Train loss: 0.7021699723075417.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 320. Train loss: 0.6998022198677063.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 321. Train loss: 0.697404987671796.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 322. Train loss: 0.6948085041607127.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 323. Train loss: 0.6924702314769521.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 324. Train loss: 0.6901316818069009.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 325. Train loss: 0.6875080290962668.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 326. Train loss: 0.6852754003861371.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 327. Train loss: 0.6827896482804242.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 328. Train loss: 0.6802436779527103.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 329. Train loss: 0.6778660767218646.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 330. Train loss: 0.6753968421150657.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 331. Train loss: 0.672837025978986.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 332. Train loss: 0.6703633490730735.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 333. Train loss: 0.6678096827338723.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 334. Train loss: 0.665271990439471.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 335. Train loss: 0.6627372117603526.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 336. Train loss: 0.6601888747776256.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 337. Train loss: 0.6576015949249268.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 338. Train loss: 0.6550284729284399.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 339. Train loss: 0.6524381883004132.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 340. Train loss: 0.649768706630258.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 341. Train loss: 0.6471984526690315.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 342. Train loss: 0.6445626055493074.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 343. Train loss: 0.6419200897216797.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 344. Train loss: 0.6392352721270393.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 345. Train loss: 0.6365861787515528.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 346. Train loss: 0.6339483787031734.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 347. Train loss: 0.6312570642022526.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 348. Train loss: 0.6285935465027305.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 349. Train loss: 0.6258559051681968.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 350. Train loss: 0.6231879311449388.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 351. Train loss: 0.620457295109244.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 352. Train loss: 0.6177704719936147.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 353. Train loss: 0.6150120987611658.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 354. Train loss: 0.6122727499288672.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 355. Train loss: 0.6094703288639293.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 356. Train loss: 0.6067319617551916.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 357. Train loss: 0.6039032971157747.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 358. Train loss: 0.6010887272217694.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 359. Train loss: 0.5983526531387778.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 360. Train loss: 0.5954956412315369.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 361. Train loss: 0.5927337828804465.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 362. Train loss: 0.5899563221370473.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 363. Train loss: 0.587063778849209.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 364. Train loss: 0.5842482798239764.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 365. Train loss: 0.5814017933957717.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 366. Train loss: 0.5785507454591639.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 367. Train loss: 0.5756037515752456.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 368. Train loss: 0.5727339036324445.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 369. Train loss: 0.5698758083231309.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 370. Train loss: 0.5669612884521484.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 371. Train loss: 0.564046323299408.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 372. Train loss: 0.5611577980658587.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 373. Train loss: 0.5582220273859361.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 374. Train loss: 0.5552987701752606.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 375. Train loss: 0.5523450900526607.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 376. Train loss: 0.5493954700582168.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 377. Train loss: 0.5464365201837876.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 378. Train loss: 0.5434010695008671.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 379. Train loss: 0.5404591735671548.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 380. Train loss: 0.5374977308161119.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 381. Train loss: 0.5344905011794147.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 382. Train loss: 0.5315109561471378.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 383. Train loss: 0.5285045890247121.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 384. Train loss: 0.5254816728479722.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 385. Train loss: 0.5224392589400796.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 386. Train loss: 0.5194858663222369.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 387. Train loss: 0.5164119461003471.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 388. Train loss: 0.5135028747951284.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 389. Train loss: 0.5103849453084609.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 390. Train loss: 0.5074175035252291.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 391. Train loss: 0.5043783433297101.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 392. Train loss: 0.5012955402626711.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 393. Train loss: 0.49831582167569327.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 394. Train loss: 0.4952854391406564.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 395. Train loss: 0.49219245770398307.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 396. Train loss: 0.48924118105102987.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 397. Train loss: 0.4861777372219983.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 398. Train loss: 0.48307785041192.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 399. Train loss: 0.48010289493729086.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 400. Train loss: 0.47702551764600415.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 401. Train loss: 0.47403975269373727.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 402. Train loss: 0.4708344743532293.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 403. Train loss: 0.4679476706420674.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 404. Train loss: 0.4650034027941087.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 405. Train loss: 0.4617481126504786.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 406. Train loss: 0.4587284273961011.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 407. Train loss: 0.45562292547786937.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 408. Train loss: 0.45255885404698987.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 409. Train loss: 0.4495765005840975.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 410. Train loss: 0.44646195629063773.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 411. Train loss: 0.4434693315449883.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 412. Train loss: 0.4403048823861515.Train accuracy: 1.0. Test accuracy: 0.8804347826086957.\n",
      "==> Epoch: 413. Train loss: 0.43721988446572246.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 414. Train loss: 0.43424561619758606.Train accuracy: 1.0. Test accuracy: 0.8804347826086957.\n",
      "==> Epoch: 415. Train loss: 0.43121050911791187.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 416. Train loss: 0.4281662632437313.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 417. Train loss: 0.42512674016111035.Train accuracy: 1.0. Test accuracy: 0.8804347826086957.\n",
      "==> Epoch: 418. Train loss: 0.4220006834058201.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 419. Train loss: 0.4189881787580602.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 420. Train loss: 0.41592009803828073.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 421. Train loss: 0.41310298618148356.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 422. Train loss: 0.4099766278968138.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 423. Train loss: 0.4065911822459277.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 424. Train loss: 0.4038754719145158.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 425. Train loss: 0.4008079399080837.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 426. Train loss: 0.3978206217288971.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 427. Train loss: 0.3947228175752303.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 428. Train loss: 0.3916891813278198.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 429. Train loss: 0.38896208125002246.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 430. Train loss: 0.3857907284708584.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 431. Train loss: 0.38269855344996734.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 432. Train loss: 0.3797392178984249.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 433. Train loss: 0.37677552945473614.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 434. Train loss: 0.37387364576844606.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 435. Train loss: 0.37094297829796286.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 436. Train loss: 0.36795652438612547.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 437. Train loss: 0.365135834497564.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 438. Train loss: 0.36216704109135794.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 439. Train loss: 0.359421407475191.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 440. Train loss: 0.35666172118747935.Train accuracy: 1.0. Test accuracy: 0.8731884057971014.\n",
      "==> Epoch: 441. Train loss: 0.3541625966044033.Train accuracy: 1.0. Test accuracy: 0.8768115942028986.\n",
      "==> Epoch: 442. Train loss: 0.35265978644875917.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 443. Train loss: 1.6880307653371025.Train accuracy: 0.9655797101449275. Test accuracy: 0.8260869565217391.\n",
      "==> Epoch: 444. Train loss: 10.021608617375879.Train accuracy: 0.9384057971014492. Test accuracy: 0.8260869565217391.\n",
      "==> Epoch: 445. Train loss: 4.651598264189327.Train accuracy: 0.9855072463768116. Test accuracy: 0.855072463768116.\n",
      "==> Epoch: 446. Train loss: 1.742982012384078.Train accuracy: 0.9963768115942029. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 447. Train loss: 1.0007793552735273.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 448. Train loss: 0.748044119161718.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 449. Train loss: 0.4943974140812369.Train accuracy: 0.9981884057971014. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 450. Train loss: 0.5427188803167904.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 451. Train loss: 0.4434814014855553.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 452. Train loss: 0.4362984247067395.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 453. Train loss: 0.4259949203799753.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 454. Train loss: 0.41934963008936715.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 455. Train loss: 0.41630478992181663.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 456. Train loss: 0.41257757649702187.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 457. Train loss: 0.409731445943608.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 458. Train loss: 0.40726036008666544.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 459. Train loss: 0.4045276361353257.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 460. Train loss: 0.4024471465279074.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 461. Train loss: 0.4004482872345868.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 462. Train loss: 0.3982367340256186.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 463. Train loss: 0.39671653509140015.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 464. Train loss: 0.39483224994996013.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 465. Train loss: 0.39344626840423136.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 466. Train loss: 0.3919142011333914.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 467. Train loss: 0.390254828859778.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 468. Train loss: 0.38889704381718354.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 469. Train loss: 0.3872924797675189.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 470. Train loss: 0.3860853875384611.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 471. Train loss: 0.38466538927134347.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 472. Train loss: 0.3834962406579186.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 473. Train loss: 0.3820312408839955.Train accuracy: 1.0. Test accuracy: 0.8695652173913043.\n",
      "==> Epoch: 474. Train loss: 0.38071517383351045.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 475. Train loss: 0.3794488275752348.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 476. Train loss: 0.37846114179667306.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 477. Train loss: 0.37720539114054513.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 478. Train loss: 0.3761022949919981.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 479. Train loss: 0.3748134812887977.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 480. Train loss: 0.37381013176020456.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 481. Train loss: 0.3726359027273515.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 482. Train loss: 0.3714418411254883.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 483. Train loss: 0.37028612284099355.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 484. Train loss: 0.3693501493510078.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 485. Train loss: 0.36829930368591757.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 486. Train loss: 0.3670301086762372.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 487. Train loss: 0.3660469458383672.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 488. Train loss: 0.3650019011076759.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 489. Train loss: 0.36396057640804963.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 490. Train loss: 0.36267124554690194.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 491. Train loss: 0.3618096183328068.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 492. Train loss: 0.3606199604623458.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 493. Train loss: 0.35964006711454954.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 494. Train loss: 0.3587370970669915.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 495. Train loss: 0.3576427592950709.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 496. Train loss: 0.3567106793908512.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 497. Train loss: 0.3556403149576748.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 498. Train loss: 0.35472018753781037.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n",
      "==> Epoch: 499. Train loss: 0.3537093821693869.Train accuracy: 1.0. Test accuracy: 0.8659420289855072.\n"
     ]
    }
   ],
   "source": [
    "dfs = DFS([128, 64, 2])\n",
    "\n",
    "dfs.fit(train_X, train_y['y'], num_epochs=500, test_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:00:24.703498Z",
     "start_time": "2018-04-30T15:00:24.377209Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 795.,  212.,  172.,  138.,   98.,   65.,   32.,    6.,    3.,    3.]),\n",
       " array([-0.0353074 ,  0.06684053,  0.16898846,  0.27113639,  0.37328432,\n",
       "         0.47543225,  0.57758018,  0.67972811,  0.78187604,  0.88402397,\n",
       "         0.9861719 ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf043e05c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dfs.get_features_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:18:09.043878Z",
     "start_time": "2018-04-30T15:18:09.032794Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_most_important_ftrs(est, features, N):\n",
    "    feature_weight = sorted(zip(est.coef_[0], features),\n",
    "                            key=lambda x: abs(x[0]))\n",
    "    \n",
    "    return list(map(lambda x: x[1], feature_weight[-N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:17:56.569797Z",
     "start_time": "2018-04-30T15:17:56.311085Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = lm.LogisticRegression(penalty='l1')\n",
    "\n",
    "est.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:12:21.688878Z",
     "start_time": "2018-04-30T15:12:19.627787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84057971014492749"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:12:40.198529Z",
     "start_time": "2018-04-30T15:12:39.919760Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          2.00000000e+00,   6.00000000e+00,   2.10000000e+01,\n",
       "          1.42300000e+03,   5.20000000e+01,   1.40000000e+01,\n",
       "          4.00000000e+00]),\n",
       " array([-2.09498806, -1.7910745 , -1.48716093, -1.18324737, -0.8793338 ,\n",
       "        -0.57542024, -0.27150668,  0.03240689,  0.33632045,  0.64023402,\n",
       "         0.94414758]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d3e75550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(est.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:20:02.760110Z",
     "start_time": "2018-04-30T15:20:02.583442Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = ens.RandomForestClassifier()\n",
    "\n",
    "est.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:20:29.243325Z",
     "start_time": "2018-04-30T15:20:29.229946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84420289855072461"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T15:23:45.055446Z",
     "start_time": "2018-04-30T15:23:43.942756Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1418.,    57.,    21.,     9.,     4.,     6.,     2.,     2.,\n",
       "            2.,     3.]),\n",
       " array([ 0.        ,  0.00257219,  0.00514438,  0.00771656,  0.01028875,\n",
       "         0.01286094,  0.01543313,  0.01800531,  0.0205775 ,  0.02314969,\n",
       "         0.02572188]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d1d0f320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(est.feature_importances_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "250px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1182px",
    "left": "0px",
    "right": "1792.4815673828125px",
    "top": "52.22426223754883px",
    "width": "184px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
