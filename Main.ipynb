{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:10:18.735426Z",
     "start_time": "2018-04-30T20:10:16.411523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as ms\n",
    "import sklearn.metrics as mtcs\n",
    "import sklearn.preprocessing as pp\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.ensemble as ens\n",
    "import sklearn.feature_selection as fs\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model.base import BaseEstimator\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data uploading&preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:10:20.667889Z",
     "start_time": "2018-04-30T20:10:19.702579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature-0</th>\n",
       "      <th>feature-1</th>\n",
       "      <th>feature-2</th>\n",
       "      <th>feature-3</th>\n",
       "      <th>feature-4</th>\n",
       "      <th>feature-5</th>\n",
       "      <th>feature-6</th>\n",
       "      <th>feature-7</th>\n",
       "      <th>feature-8</th>\n",
       "      <th>feature-9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature-1515</th>\n",
       "      <th>feature-1516</th>\n",
       "      <th>feature-1517</th>\n",
       "      <th>feature-1518</th>\n",
       "      <th>feature-1519</th>\n",
       "      <th>feature-1520</th>\n",
       "      <th>feature-1521</th>\n",
       "      <th>feature-1522</th>\n",
       "      <th>feature-1523</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.977273</td>\n",
       "      <td>6.758452</td>\n",
       "      <td>3.636364</td>\n",
       "      <td>10.792929</td>\n",
       "      <td>160.801682</td>\n",
       "      <td>151.109783</td>\n",
       "      <td>1.791689</td>\n",
       "      <td>6.818675</td>\n",
       "      <td>8.138413</td>\n",
       "      <td>8.270161</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658393</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.540632</td>\n",
       "      <td>4.953183</td>\n",
       "      <td>5.351562</td>\n",
       "      <td>5.311048</td>\n",
       "      <td>5.560922</td>\n",
       "      <td>5.643015</td>\n",
       "      <td>5.715999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.408163</td>\n",
       "      <td>5.933978</td>\n",
       "      <td>2.816327</td>\n",
       "      <td>5.877551</td>\n",
       "      <td>162.949911</td>\n",
       "      <td>76.153796</td>\n",
       "      <td>1.381401</td>\n",
       "      <td>6.002651</td>\n",
       "      <td>5.080499</td>\n",
       "      <td>7.514421</td>\n",
       "      <td>...</td>\n",
       "      <td>4.830811</td>\n",
       "      <td>3.817712</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.426343</td>\n",
       "      <td>4.823804</td>\n",
       "      <td>4.652173</td>\n",
       "      <td>4.795274</td>\n",
       "      <td>4.860781</td>\n",
       "      <td>5.001426</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.265306</td>\n",
       "      <td>7.425645</td>\n",
       "      <td>3.734694</td>\n",
       "      <td>13.160998</td>\n",
       "      <td>172.099640</td>\n",
       "      <td>161.790879</td>\n",
       "      <td>1.603976</td>\n",
       "      <td>7.410120</td>\n",
       "      <td>10.114794</td>\n",
       "      <td>8.805738</td>\n",
       "      <td>...</td>\n",
       "      <td>6.397659</td>\n",
       "      <td>4.223177</td>\n",
       "      <td>4.685597</td>\n",
       "      <td>5.116870</td>\n",
       "      <td>5.333926</td>\n",
       "      <td>5.504569</td>\n",
       "      <td>5.797956</td>\n",
       "      <td>6.009581</td>\n",
       "      <td>6.200889</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43.976744</td>\n",
       "      <td>7.648293</td>\n",
       "      <td>3.837209</td>\n",
       "      <td>14.392765</td>\n",
       "      <td>168.885456</td>\n",
       "      <td>175.277251</td>\n",
       "      <td>1.622298</td>\n",
       "      <td>7.629033</td>\n",
       "      <td>12.180817</td>\n",
       "      <td>9.070719</td>\n",
       "      <td>...</td>\n",
       "      <td>5.879135</td>\n",
       "      <td>4.280132</td>\n",
       "      <td>4.563045</td>\n",
       "      <td>5.007714</td>\n",
       "      <td>5.159773</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.640132</td>\n",
       "      <td>5.472271</td>\n",
       "      <td>5.741399</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24.320988</td>\n",
       "      <td>6.534011</td>\n",
       "      <td>3.567901</td>\n",
       "      <td>8.913580</td>\n",
       "      <td>163.076959</td>\n",
       "      <td>96.019681</td>\n",
       "      <td>1.380679</td>\n",
       "      <td>6.566695</td>\n",
       "      <td>4.417010</td>\n",
       "      <td>8.058783</td>\n",
       "      <td>...</td>\n",
       "      <td>8.148663</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.173321</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>6.259342</td>\n",
       "      <td>6.626469</td>\n",
       "      <td>7.062406</td>\n",
       "      <td>7.472998</td>\n",
       "      <td>7.829842</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.924051</td>\n",
       "      <td>6.134299</td>\n",
       "      <td>3.037975</td>\n",
       "      <td>6.506329</td>\n",
       "      <td>165.707039</td>\n",
       "      <td>82.761541</td>\n",
       "      <td>1.381957</td>\n",
       "      <td>6.187547</td>\n",
       "      <td>4.684599</td>\n",
       "      <td>7.660347</td>\n",
       "      <td>...</td>\n",
       "      <td>6.087556</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.820282</td>\n",
       "      <td>5.183187</td>\n",
       "      <td>5.595176</td>\n",
       "      <td>5.489454</td>\n",
       "      <td>5.604998</td>\n",
       "      <td>5.847522</td>\n",
       "      <td>5.987080</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>34.150000</td>\n",
       "      <td>6.740695</td>\n",
       "      <td>3.733333</td>\n",
       "      <td>10.214815</td>\n",
       "      <td>164.252922</td>\n",
       "      <td>135.639059</td>\n",
       "      <td>1.620887</td>\n",
       "      <td>6.781702</td>\n",
       "      <td>8.631090</td>\n",
       "      <td>8.248393</td>\n",
       "      <td>...</td>\n",
       "      <td>6.198225</td>\n",
       "      <td>4.471639</td>\n",
       "      <td>4.801970</td>\n",
       "      <td>5.237107</td>\n",
       "      <td>5.493833</td>\n",
       "      <td>5.573816</td>\n",
       "      <td>5.764799</td>\n",
       "      <td>5.865760</td>\n",
       "      <td>5.998937</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23.833333</td>\n",
       "      <td>6.395508</td>\n",
       "      <td>3.141026</td>\n",
       "      <td>8.717949</td>\n",
       "      <td>163.221967</td>\n",
       "      <td>94.106131</td>\n",
       "      <td>1.435936</td>\n",
       "      <td>6.443753</td>\n",
       "      <td>5.834402</td>\n",
       "      <td>7.904135</td>\n",
       "      <td>...</td>\n",
       "      <td>6.582328</td>\n",
       "      <td>4.600158</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.499726</td>\n",
       "      <td>5.978728</td>\n",
       "      <td>5.995208</td>\n",
       "      <td>6.179952</td>\n",
       "      <td>6.364051</td>\n",
       "      <td>6.481290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32.380952</td>\n",
       "      <td>6.152543</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>6.402116</td>\n",
       "      <td>164.380868</td>\n",
       "      <td>128.391104</td>\n",
       "      <td>1.687697</td>\n",
       "      <td>6.232890</td>\n",
       "      <td>4.476844</td>\n",
       "      <td>7.736528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.449988</td>\n",
       "      <td>3.865979</td>\n",
       "      <td>4.506730</td>\n",
       "      <td>4.765906</td>\n",
       "      <td>4.965028</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.595598</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>45.228571</td>\n",
       "      <td>6.608449</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>9.180952</td>\n",
       "      <td>159.167580</td>\n",
       "      <td>180.141749</td>\n",
       "      <td>1.981354</td>\n",
       "      <td>6.690537</td>\n",
       "      <td>8.428546</td>\n",
       "      <td>8.221041</td>\n",
       "      <td>...</td>\n",
       "      <td>5.214936</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.682131</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>5.192957</td>\n",
       "      <td>5.342334</td>\n",
       "      <td>5.402677</td>\n",
       "      <td>5.303305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>24.757143</td>\n",
       "      <td>6.241471</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>6.949206</td>\n",
       "      <td>167.005923</td>\n",
       "      <td>97.692813</td>\n",
       "      <td>1.408460</td>\n",
       "      <td>6.289021</td>\n",
       "      <td>5.919754</td>\n",
       "      <td>7.789862</td>\n",
       "      <td>...</td>\n",
       "      <td>6.175997</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>4.716264</td>\n",
       "      <td>5.155457</td>\n",
       "      <td>5.591686</td>\n",
       "      <td>5.680173</td>\n",
       "      <td>5.977302</td>\n",
       "      <td>6.030986</td>\n",
       "      <td>6.214671</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32.096774</td>\n",
       "      <td>7.206768</td>\n",
       "      <td>3.838710</td>\n",
       "      <td>13.806452</td>\n",
       "      <td>160.469926</td>\n",
       "      <td>127.646528</td>\n",
       "      <td>1.591140</td>\n",
       "      <td>7.228381</td>\n",
       "      <td>11.071685</td>\n",
       "      <td>8.598289</td>\n",
       "      <td>...</td>\n",
       "      <td>7.242977</td>\n",
       "      <td>4.094345</td>\n",
       "      <td>4.639572</td>\n",
       "      <td>5.151845</td>\n",
       "      <td>5.678037</td>\n",
       "      <td>5.850765</td>\n",
       "      <td>6.134888</td>\n",
       "      <td>6.451753</td>\n",
       "      <td>6.793466</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29.833333</td>\n",
       "      <td>6.436864</td>\n",
       "      <td>3.476190</td>\n",
       "      <td>8.296296</td>\n",
       "      <td>162.859109</td>\n",
       "      <td>118.257591</td>\n",
       "      <td>1.600911</td>\n",
       "      <td>6.497038</td>\n",
       "      <td>6.283812</td>\n",
       "      <td>7.960386</td>\n",
       "      <td>...</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>4.051785</td>\n",
       "      <td>4.519067</td>\n",
       "      <td>4.935373</td>\n",
       "      <td>5.281616</td>\n",
       "      <td>5.221369</td>\n",
       "      <td>5.465948</td>\n",
       "      <td>5.520210</td>\n",
       "      <td>5.499982</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>36.500000</td>\n",
       "      <td>6.700508</td>\n",
       "      <td>3.653846</td>\n",
       "      <td>10.709402</td>\n",
       "      <td>163.669041</td>\n",
       "      <td>145.244215</td>\n",
       "      <td>1.785651</td>\n",
       "      <td>6.764423</td>\n",
       "      <td>7.295706</td>\n",
       "      <td>8.158660</td>\n",
       "      <td>...</td>\n",
       "      <td>5.768126</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.356709</td>\n",
       "      <td>4.844187</td>\n",
       "      <td>5.326662</td>\n",
       "      <td>5.340239</td>\n",
       "      <td>5.395331</td>\n",
       "      <td>5.454787</td>\n",
       "      <td>5.557552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38.851852</td>\n",
       "      <td>7.402900</td>\n",
       "      <td>3.777778</td>\n",
       "      <td>12.979424</td>\n",
       "      <td>170.018323</td>\n",
       "      <td>154.527750</td>\n",
       "      <td>1.476580</td>\n",
       "      <td>7.381493</td>\n",
       "      <td>11.933931</td>\n",
       "      <td>8.867678</td>\n",
       "      <td>...</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>3.688879</td>\n",
       "      <td>3.761200</td>\n",
       "      <td>4.110874</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.543295</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.290459</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>27.534483</td>\n",
       "      <td>6.269883</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>7.973180</td>\n",
       "      <td>161.381053</td>\n",
       "      <td>109.038626</td>\n",
       "      <td>1.618478</td>\n",
       "      <td>6.345124</td>\n",
       "      <td>4.994148</td>\n",
       "      <td>7.791228</td>\n",
       "      <td>...</td>\n",
       "      <td>8.934004</td>\n",
       "      <td>4.644391</td>\n",
       "      <td>5.241747</td>\n",
       "      <td>5.930586</td>\n",
       "      <td>6.610360</td>\n",
       "      <td>7.088878</td>\n",
       "      <td>7.641069</td>\n",
       "      <td>8.134765</td>\n",
       "      <td>8.607916</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.837500</td>\n",
       "      <td>6.722804</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>172.043543</td>\n",
       "      <td>93.939718</td>\n",
       "      <td>1.156748</td>\n",
       "      <td>6.710586</td>\n",
       "      <td>6.512587</td>\n",
       "      <td>8.234018</td>\n",
       "      <td>...</td>\n",
       "      <td>5.638355</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.532599</td>\n",
       "      <td>4.804021</td>\n",
       "      <td>5.043425</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.337538</td>\n",
       "      <td>5.497168</td>\n",
       "      <td>5.568345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>55.263158</td>\n",
       "      <td>7.389305</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>12.257310</td>\n",
       "      <td>170.139845</td>\n",
       "      <td>220.606751</td>\n",
       "      <td>1.969099</td>\n",
       "      <td>7.412105</td>\n",
       "      <td>10.847813</td>\n",
       "      <td>8.885381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.617652</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.357510</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>4.789573</td>\n",
       "      <td>4.523146</td>\n",
       "      <td>3.944006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43.057692</td>\n",
       "      <td>7.061350</td>\n",
       "      <td>3.942308</td>\n",
       "      <td>11.871795</td>\n",
       "      <td>163.441127</td>\n",
       "      <td>171.586122</td>\n",
       "      <td>1.834505</td>\n",
       "      <td>7.104088</td>\n",
       "      <td>9.456211</td>\n",
       "      <td>8.542274</td>\n",
       "      <td>...</td>\n",
       "      <td>6.706174</td>\n",
       "      <td>4.508108</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.374989</td>\n",
       "      <td>5.679959</td>\n",
       "      <td>5.755742</td>\n",
       "      <td>6.060582</td>\n",
       "      <td>6.240763</td>\n",
       "      <td>6.434747</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42.886792</td>\n",
       "      <td>7.052642</td>\n",
       "      <td>3.943396</td>\n",
       "      <td>12.100629</td>\n",
       "      <td>164.829574</td>\n",
       "      <td>170.922688</td>\n",
       "      <td>1.833416</td>\n",
       "      <td>7.095513</td>\n",
       "      <td>8.143800</td>\n",
       "      <td>8.514148</td>\n",
       "      <td>...</td>\n",
       "      <td>6.976085</td>\n",
       "      <td>4.572130</td>\n",
       "      <td>5.115746</td>\n",
       "      <td>5.566195</td>\n",
       "      <td>5.936711</td>\n",
       "      <td>6.080505</td>\n",
       "      <td>6.198796</td>\n",
       "      <td>6.333335</td>\n",
       "      <td>6.652742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>35.761905</td>\n",
       "      <td>6.858500</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>9.640212</td>\n",
       "      <td>168.739205</td>\n",
       "      <td>141.875852</td>\n",
       "      <td>1.456355</td>\n",
       "      <td>6.864043</td>\n",
       "      <td>11.470610</td>\n",
       "      <td>8.439979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21.528571</td>\n",
       "      <td>6.482779</td>\n",
       "      <td>2.942857</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>171.920444</td>\n",
       "      <td>84.572241</td>\n",
       "      <td>1.127633</td>\n",
       "      <td>6.481077</td>\n",
       "      <td>5.269841</td>\n",
       "      <td>8.031493</td>\n",
       "      <td>...</td>\n",
       "      <td>5.036953</td>\n",
       "      <td>3.931826</td>\n",
       "      <td>4.234107</td>\n",
       "      <td>4.442651</td>\n",
       "      <td>4.672829</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>4.890349</td>\n",
       "      <td>4.969813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.054795</td>\n",
       "      <td>6.395890</td>\n",
       "      <td>3.164384</td>\n",
       "      <td>8.301370</td>\n",
       "      <td>162.696886</td>\n",
       "      <td>90.885447</td>\n",
       "      <td>1.370996</td>\n",
       "      <td>6.435699</td>\n",
       "      <td>6.796613</td>\n",
       "      <td>7.942436</td>\n",
       "      <td>...</td>\n",
       "      <td>6.182085</td>\n",
       "      <td>4.330733</td>\n",
       "      <td>4.605170</td>\n",
       "      <td>4.912655</td>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>5.652489</td>\n",
       "      <td>5.831882</td>\n",
       "      <td>6.040255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>19.160000</td>\n",
       "      <td>6.067396</td>\n",
       "      <td>2.560000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>167.413784</td>\n",
       "      <td>75.058797</td>\n",
       "      <td>1.241288</td>\n",
       "      <td>6.107552</td>\n",
       "      <td>6.423333</td>\n",
       "      <td>7.651508</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>17.131868</td>\n",
       "      <td>5.762305</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.109890</td>\n",
       "      <td>165.080662</td>\n",
       "      <td>66.885356</td>\n",
       "      <td>1.294529</td>\n",
       "      <td>5.828765</td>\n",
       "      <td>2.738324</td>\n",
       "      <td>7.375662</td>\n",
       "      <td>...</td>\n",
       "      <td>6.448889</td>\n",
       "      <td>4.262680</td>\n",
       "      <td>4.624973</td>\n",
       "      <td>5.010635</td>\n",
       "      <td>5.351858</td>\n",
       "      <td>5.590987</td>\n",
       "      <td>5.834811</td>\n",
       "      <td>6.077642</td>\n",
       "      <td>6.293419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>28.432432</td>\n",
       "      <td>6.603514</td>\n",
       "      <td>4.324324</td>\n",
       "      <td>11.027027</td>\n",
       "      <td>153.571767</td>\n",
       "      <td>112.900411</td>\n",
       "      <td>1.738632</td>\n",
       "      <td>6.683730</td>\n",
       "      <td>6.055743</td>\n",
       "      <td>8.059202</td>\n",
       "      <td>...</td>\n",
       "      <td>8.293510</td>\n",
       "      <td>4.363099</td>\n",
       "      <td>5.012301</td>\n",
       "      <td>5.675469</td>\n",
       "      <td>6.310259</td>\n",
       "      <td>6.686641</td>\n",
       "      <td>7.157565</td>\n",
       "      <td>7.635515</td>\n",
       "      <td>8.018008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>21.500000</td>\n",
       "      <td>6.207650</td>\n",
       "      <td>2.954545</td>\n",
       "      <td>7.272727</td>\n",
       "      <td>164.257367</td>\n",
       "      <td>85.172313</td>\n",
       "      <td>1.392726</td>\n",
       "      <td>6.258106</td>\n",
       "      <td>5.808081</td>\n",
       "      <td>7.738226</td>\n",
       "      <td>...</td>\n",
       "      <td>7.443490</td>\n",
       "      <td>4.304065</td>\n",
       "      <td>4.828314</td>\n",
       "      <td>5.422745</td>\n",
       "      <td>6.018593</td>\n",
       "      <td>6.373640</td>\n",
       "      <td>6.751321</td>\n",
       "      <td>7.048332</td>\n",
       "      <td>7.302612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.428571</td>\n",
       "      <td>6.252383</td>\n",
       "      <td>2.885714</td>\n",
       "      <td>7.384127</td>\n",
       "      <td>164.322747</td>\n",
       "      <td>112.473778</td>\n",
       "      <td>1.536709</td>\n",
       "      <td>6.312869</td>\n",
       "      <td>6.333953</td>\n",
       "      <td>7.825781</td>\n",
       "      <td>...</td>\n",
       "      <td>3.791267</td>\n",
       "      <td>3.772761</td>\n",
       "      <td>4.081766</td>\n",
       "      <td>4.514972</td>\n",
       "      <td>4.873765</td>\n",
       "      <td>4.997212</td>\n",
       "      <td>4.848606</td>\n",
       "      <td>4.455074</td>\n",
       "      <td>4.352694</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38.658537</td>\n",
       "      <td>6.752610</td>\n",
       "      <td>3.682927</td>\n",
       "      <td>10.753388</td>\n",
       "      <td>159.812730</td>\n",
       "      <td>153.837907</td>\n",
       "      <td>1.814410</td>\n",
       "      <td>6.815551</td>\n",
       "      <td>8.392444</td>\n",
       "      <td>8.278695</td>\n",
       "      <td>...</td>\n",
       "      <td>5.800228</td>\n",
       "      <td>4.098503</td>\n",
       "      <td>4.492841</td>\n",
       "      <td>4.938513</td>\n",
       "      <td>5.120237</td>\n",
       "      <td>5.262042</td>\n",
       "      <td>5.561162</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.681878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>21.670588</td>\n",
       "      <td>6.292286</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>6.776471</td>\n",
       "      <td>167.982829</td>\n",
       "      <td>85.273347</td>\n",
       "      <td>1.296235</td>\n",
       "      <td>6.326075</td>\n",
       "      <td>4.991830</td>\n",
       "      <td>7.822375</td>\n",
       "      <td>...</td>\n",
       "      <td>6.577992</td>\n",
       "      <td>4.385147</td>\n",
       "      <td>4.798885</td>\n",
       "      <td>5.216633</td>\n",
       "      <td>5.602695</td>\n",
       "      <td>5.857442</td>\n",
       "      <td>6.151851</td>\n",
       "      <td>6.484856</td>\n",
       "      <td>6.339973</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>23.849057</td>\n",
       "      <td>6.443296</td>\n",
       "      <td>3.584906</td>\n",
       "      <td>8.490566</td>\n",
       "      <td>165.656310</td>\n",
       "      <td>94.855312</td>\n",
       "      <td>1.429258</td>\n",
       "      <td>6.483247</td>\n",
       "      <td>5.732180</td>\n",
       "      <td>7.917090</td>\n",
       "      <td>...</td>\n",
       "      <td>8.145414</td>\n",
       "      <td>4.375757</td>\n",
       "      <td>5.032071</td>\n",
       "      <td>5.702949</td>\n",
       "      <td>6.339698</td>\n",
       "      <td>6.834210</td>\n",
       "      <td>7.294229</td>\n",
       "      <td>7.631557</td>\n",
       "      <td>8.007074</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1076</th>\n",
       "      <td>20.528302</td>\n",
       "      <td>6.044432</td>\n",
       "      <td>3.603774</td>\n",
       "      <td>6.075472</td>\n",
       "      <td>162.424590</td>\n",
       "      <td>80.699025</td>\n",
       "      <td>1.402911</td>\n",
       "      <td>6.109594</td>\n",
       "      <td>3.789570</td>\n",
       "      <td>7.610526</td>\n",
       "      <td>...</td>\n",
       "      <td>7.481855</td>\n",
       "      <td>4.151040</td>\n",
       "      <td>4.734003</td>\n",
       "      <td>5.287636</td>\n",
       "      <td>5.850225</td>\n",
       "      <td>6.211102</td>\n",
       "      <td>6.591717</td>\n",
       "      <td>6.991033</td>\n",
       "      <td>7.274674</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>20.440000</td>\n",
       "      <td>5.968498</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>6.080000</td>\n",
       "      <td>161.064334</td>\n",
       "      <td>80.380964</td>\n",
       "      <td>1.456437</td>\n",
       "      <td>6.045898</td>\n",
       "      <td>3.411944</td>\n",
       "      <td>7.531384</td>\n",
       "      <td>...</td>\n",
       "      <td>7.520150</td>\n",
       "      <td>4.166665</td>\n",
       "      <td>4.725173</td>\n",
       "      <td>5.302683</td>\n",
       "      <td>5.881406</td>\n",
       "      <td>6.250458</td>\n",
       "      <td>6.631384</td>\n",
       "      <td>7.030719</td>\n",
       "      <td>7.325005</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>24.333333</td>\n",
       "      <td>5.957881</td>\n",
       "      <td>3.062500</td>\n",
       "      <td>5.509259</td>\n",
       "      <td>164.383290</td>\n",
       "      <td>95.963738</td>\n",
       "      <td>1.496938</td>\n",
       "      <td>6.033144</td>\n",
       "      <td>3.958207</td>\n",
       "      <td>7.545887</td>\n",
       "      <td>...</td>\n",
       "      <td>7.057353</td>\n",
       "      <td>4.131159</td>\n",
       "      <td>4.601413</td>\n",
       "      <td>5.158696</td>\n",
       "      <td>5.690359</td>\n",
       "      <td>6.054403</td>\n",
       "      <td>6.455334</td>\n",
       "      <td>6.873095</td>\n",
       "      <td>6.939417</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>23.375000</td>\n",
       "      <td>6.322417</td>\n",
       "      <td>3.203125</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>162.588954</td>\n",
       "      <td>92.261393</td>\n",
       "      <td>1.451475</td>\n",
       "      <td>6.377366</td>\n",
       "      <td>5.672743</td>\n",
       "      <td>7.837564</td>\n",
       "      <td>...</td>\n",
       "      <td>6.824985</td>\n",
       "      <td>4.430817</td>\n",
       "      <td>4.919981</td>\n",
       "      <td>5.421641</td>\n",
       "      <td>5.892335</td>\n",
       "      <td>6.042336</td>\n",
       "      <td>6.333613</td>\n",
       "      <td>6.572807</td>\n",
       "      <td>6.736893</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>41.800000</td>\n",
       "      <td>9.073735</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>21.400000</td>\n",
       "      <td>180.387654</td>\n",
       "      <td>166.508642</td>\n",
       "      <td>1.000430</td>\n",
       "      <td>8.896735</td>\n",
       "      <td>19.768056</td>\n",
       "      <td>10.349211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.178054</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>2.995732</td>\n",
       "      <td>3.218876</td>\n",
       "      <td>2.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>18.390244</td>\n",
       "      <td>5.605241</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.731707</td>\n",
       "      <td>158.624476</td>\n",
       "      <td>72.159797</td>\n",
       "      <td>1.549658</td>\n",
       "      <td>5.717783</td>\n",
       "      <td>2.198509</td>\n",
       "      <td>7.184370</td>\n",
       "      <td>...</td>\n",
       "      <td>7.163209</td>\n",
       "      <td>3.951244</td>\n",
       "      <td>4.559126</td>\n",
       "      <td>5.140200</td>\n",
       "      <td>5.685703</td>\n",
       "      <td>5.885409</td>\n",
       "      <td>6.272759</td>\n",
       "      <td>6.609097</td>\n",
       "      <td>6.919128</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>23.458333</td>\n",
       "      <td>6.397181</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.375000</td>\n",
       "      <td>166.060607</td>\n",
       "      <td>93.350066</td>\n",
       "      <td>1.430009</td>\n",
       "      <td>6.439448</td>\n",
       "      <td>5.322917</td>\n",
       "      <td>7.869501</td>\n",
       "      <td>...</td>\n",
       "      <td>8.177485</td>\n",
       "      <td>4.297285</td>\n",
       "      <td>4.978456</td>\n",
       "      <td>5.658611</td>\n",
       "      <td>6.322790</td>\n",
       "      <td>6.818753</td>\n",
       "      <td>7.288522</td>\n",
       "      <td>7.655010</td>\n",
       "      <td>8.058613</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>25.809524</td>\n",
       "      <td>5.873886</td>\n",
       "      <td>3.380952</td>\n",
       "      <td>5.962963</td>\n",
       "      <td>162.051123</td>\n",
       "      <td>102.034265</td>\n",
       "      <td>1.656037</td>\n",
       "      <td>5.975364</td>\n",
       "      <td>2.317542</td>\n",
       "      <td>7.431153</td>\n",
       "      <td>...</td>\n",
       "      <td>7.555979</td>\n",
       "      <td>4.073291</td>\n",
       "      <td>4.650383</td>\n",
       "      <td>5.246695</td>\n",
       "      <td>5.735564</td>\n",
       "      <td>6.103816</td>\n",
       "      <td>6.545754</td>\n",
       "      <td>6.979000</td>\n",
       "      <td>7.346393</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>21.414634</td>\n",
       "      <td>5.975237</td>\n",
       "      <td>3.487805</td>\n",
       "      <td>6.926829</td>\n",
       "      <td>161.339153</td>\n",
       "      <td>84.405257</td>\n",
       "      <td>1.534660</td>\n",
       "      <td>6.062522</td>\n",
       "      <td>3.468157</td>\n",
       "      <td>7.496018</td>\n",
       "      <td>...</td>\n",
       "      <td>7.446209</td>\n",
       "      <td>4.212128</td>\n",
       "      <td>4.888468</td>\n",
       "      <td>5.525951</td>\n",
       "      <td>5.975081</td>\n",
       "      <td>6.299380</td>\n",
       "      <td>6.681394</td>\n",
       "      <td>7.067183</td>\n",
       "      <td>7.171165</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1085</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>5.884906</td>\n",
       "      <td>2.759259</td>\n",
       "      <td>5.370370</td>\n",
       "      <td>162.831246</td>\n",
       "      <td>74.502082</td>\n",
       "      <td>1.381487</td>\n",
       "      <td>5.956572</td>\n",
       "      <td>3.660108</td>\n",
       "      <td>7.471168</td>\n",
       "      <td>...</td>\n",
       "      <td>5.514688</td>\n",
       "      <td>3.901973</td>\n",
       "      <td>4.327438</td>\n",
       "      <td>4.732904</td>\n",
       "      <td>5.096431</td>\n",
       "      <td>5.080239</td>\n",
       "      <td>5.365684</td>\n",
       "      <td>5.482980</td>\n",
       "      <td>5.400140</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>23.318182</td>\n",
       "      <td>6.330339</td>\n",
       "      <td>3.363636</td>\n",
       "      <td>8.045455</td>\n",
       "      <td>163.149857</td>\n",
       "      <td>92.872249</td>\n",
       "      <td>1.484800</td>\n",
       "      <td>6.384780</td>\n",
       "      <td>4.174874</td>\n",
       "      <td>7.815815</td>\n",
       "      <td>...</td>\n",
       "      <td>6.172744</td>\n",
       "      <td>4.060443</td>\n",
       "      <td>4.569543</td>\n",
       "      <td>5.057837</td>\n",
       "      <td>5.539301</td>\n",
       "      <td>5.568821</td>\n",
       "      <td>5.728170</td>\n",
       "      <td>5.901779</td>\n",
       "      <td>5.917717</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>20.560000</td>\n",
       "      <td>6.123796</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>5.920000</td>\n",
       "      <td>165.088853</td>\n",
       "      <td>80.788719</td>\n",
       "      <td>1.335059</td>\n",
       "      <td>6.174052</td>\n",
       "      <td>4.833333</td>\n",
       "      <td>7.684390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.238678</td>\n",
       "      <td>3.669951</td>\n",
       "      <td>4.112921</td>\n",
       "      <td>4.525180</td>\n",
       "      <td>4.251170</td>\n",
       "      <td>4.104707</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.056357</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088</th>\n",
       "      <td>22.380952</td>\n",
       "      <td>6.182614</td>\n",
       "      <td>3.349206</td>\n",
       "      <td>7.396825</td>\n",
       "      <td>164.113227</td>\n",
       "      <td>89.449143</td>\n",
       "      <td>1.521852</td>\n",
       "      <td>6.248717</td>\n",
       "      <td>4.873016</td>\n",
       "      <td>7.649627</td>\n",
       "      <td>...</td>\n",
       "      <td>7.231355</td>\n",
       "      <td>4.545951</td>\n",
       "      <td>5.133590</td>\n",
       "      <td>5.739994</td>\n",
       "      <td>6.264469</td>\n",
       "      <td>6.422816</td>\n",
       "      <td>6.771282</td>\n",
       "      <td>7.089077</td>\n",
       "      <td>7.031227</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>25.161290</td>\n",
       "      <td>5.774461</td>\n",
       "      <td>2.806452</td>\n",
       "      <td>4.143369</td>\n",
       "      <td>166.212698</td>\n",
       "      <td>99.236969</td>\n",
       "      <td>1.531852</td>\n",
       "      <td>5.860845</td>\n",
       "      <td>2.388103</td>\n",
       "      <td>7.386058</td>\n",
       "      <td>...</td>\n",
       "      <td>3.954843</td>\n",
       "      <td>3.409496</td>\n",
       "      <td>3.868593</td>\n",
       "      <td>4.382808</td>\n",
       "      <td>4.539297</td>\n",
       "      <td>4.549261</td>\n",
       "      <td>4.743845</td>\n",
       "      <td>4.850075</td>\n",
       "      <td>4.323304</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>18.074074</td>\n",
       "      <td>5.806478</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>4.518519</td>\n",
       "      <td>164.385174</td>\n",
       "      <td>70.742453</td>\n",
       "      <td>1.348570</td>\n",
       "      <td>5.877785</td>\n",
       "      <td>3.391975</td>\n",
       "      <td>7.397610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.198673</td>\n",
       "      <td>3.630985</td>\n",
       "      <td>4.009603</td>\n",
       "      <td>4.409003</td>\n",
       "      <td>3.997053</td>\n",
       "      <td>4.066888</td>\n",
       "      <td>3.725693</td>\n",
       "      <td>2.784239</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>21.284553</td>\n",
       "      <td>6.179102</td>\n",
       "      <td>3.219512</td>\n",
       "      <td>6.975610</td>\n",
       "      <td>164.084221</td>\n",
       "      <td>83.736404</td>\n",
       "      <td>1.363989</td>\n",
       "      <td>6.230333</td>\n",
       "      <td>4.664747</td>\n",
       "      <td>7.732390</td>\n",
       "      <td>...</td>\n",
       "      <td>8.626406</td>\n",
       "      <td>4.923624</td>\n",
       "      <td>5.447814</td>\n",
       "      <td>5.972218</td>\n",
       "      <td>6.539676</td>\n",
       "      <td>7.003520</td>\n",
       "      <td>7.460310</td>\n",
       "      <td>7.890512</td>\n",
       "      <td>8.276919</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>19.658537</td>\n",
       "      <td>5.914988</td>\n",
       "      <td>3.048780</td>\n",
       "      <td>5.756098</td>\n",
       "      <td>166.863082</td>\n",
       "      <td>77.226327</td>\n",
       "      <td>1.395619</td>\n",
       "      <td>5.985012</td>\n",
       "      <td>2.447832</td>\n",
       "      <td>7.443992</td>\n",
       "      <td>...</td>\n",
       "      <td>6.598232</td>\n",
       "      <td>3.886705</td>\n",
       "      <td>4.389809</td>\n",
       "      <td>4.926801</td>\n",
       "      <td>5.261718</td>\n",
       "      <td>5.494090</td>\n",
       "      <td>5.865848</td>\n",
       "      <td>6.204873</td>\n",
       "      <td>6.369954</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>17.720930</td>\n",
       "      <td>5.750228</td>\n",
       "      <td>2.697674</td>\n",
       "      <td>4.604651</td>\n",
       "      <td>164.527087</td>\n",
       "      <td>69.324920</td>\n",
       "      <td>1.357124</td>\n",
       "      <td>5.825972</td>\n",
       "      <td>3.025840</td>\n",
       "      <td>7.340613</td>\n",
       "      <td>...</td>\n",
       "      <td>5.196423</td>\n",
       "      <td>3.676301</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>4.647990</td>\n",
       "      <td>5.085665</td>\n",
       "      <td>5.169063</td>\n",
       "      <td>5.358942</td>\n",
       "      <td>5.594479</td>\n",
       "      <td>5.380329</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094</th>\n",
       "      <td>17.225806</td>\n",
       "      <td>5.579190</td>\n",
       "      <td>2.548387</td>\n",
       "      <td>4.193548</td>\n",
       "      <td>161.392477</td>\n",
       "      <td>67.383958</td>\n",
       "      <td>1.452391</td>\n",
       "      <td>5.679165</td>\n",
       "      <td>2.590502</td>\n",
       "      <td>7.175556</td>\n",
       "      <td>...</td>\n",
       "      <td>3.840795</td>\n",
       "      <td>3.349904</td>\n",
       "      <td>3.682610</td>\n",
       "      <td>4.071161</td>\n",
       "      <td>4.447785</td>\n",
       "      <td>4.003918</td>\n",
       "      <td>4.150055</td>\n",
       "      <td>4.274928</td>\n",
       "      <td>4.123094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>25.841270</td>\n",
       "      <td>6.257190</td>\n",
       "      <td>3.301587</td>\n",
       "      <td>7.499118</td>\n",
       "      <td>165.929853</td>\n",
       "      <td>102.114039</td>\n",
       "      <td>1.475191</td>\n",
       "      <td>6.312625</td>\n",
       "      <td>4.628210</td>\n",
       "      <td>7.790568</td>\n",
       "      <td>...</td>\n",
       "      <td>8.508140</td>\n",
       "      <td>4.551242</td>\n",
       "      <td>5.176856</td>\n",
       "      <td>5.812076</td>\n",
       "      <td>6.371825</td>\n",
       "      <td>6.850507</td>\n",
       "      <td>7.327036</td>\n",
       "      <td>7.760788</td>\n",
       "      <td>8.122028</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1096</th>\n",
       "      <td>19.416667</td>\n",
       "      <td>6.040173</td>\n",
       "      <td>2.826389</td>\n",
       "      <td>5.763889</td>\n",
       "      <td>164.563288</td>\n",
       "      <td>76.121662</td>\n",
       "      <td>1.298125</td>\n",
       "      <td>6.090735</td>\n",
       "      <td>4.419416</td>\n",
       "      <td>7.632631</td>\n",
       "      <td>...</td>\n",
       "      <td>6.257668</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>5.105945</td>\n",
       "      <td>5.313206</td>\n",
       "      <td>5.537334</td>\n",
       "      <td>5.620401</td>\n",
       "      <td>5.805135</td>\n",
       "      <td>5.976351</td>\n",
       "      <td>6.091310</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1097</th>\n",
       "      <td>22.877193</td>\n",
       "      <td>5.824621</td>\n",
       "      <td>2.912281</td>\n",
       "      <td>5.130604</td>\n",
       "      <td>162.065226</td>\n",
       "      <td>90.113408</td>\n",
       "      <td>1.522993</td>\n",
       "      <td>5.913365</td>\n",
       "      <td>2.939753</td>\n",
       "      <td>7.426243</td>\n",
       "      <td>...</td>\n",
       "      <td>5.575239</td>\n",
       "      <td>4.056123</td>\n",
       "      <td>4.501198</td>\n",
       "      <td>4.960657</td>\n",
       "      <td>5.184939</td>\n",
       "      <td>5.169418</td>\n",
       "      <td>5.403803</td>\n",
       "      <td>5.604422</td>\n",
       "      <td>5.480118</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1098</th>\n",
       "      <td>18.912281</td>\n",
       "      <td>5.693595</td>\n",
       "      <td>2.929825</td>\n",
       "      <td>5.228070</td>\n",
       "      <td>158.876430</td>\n",
       "      <td>74.254334</td>\n",
       "      <td>1.530849</td>\n",
       "      <td>5.798249</td>\n",
       "      <td>2.397661</td>\n",
       "      <td>7.270916</td>\n",
       "      <td>...</td>\n",
       "      <td>7.533869</td>\n",
       "      <td>4.255613</td>\n",
       "      <td>4.826312</td>\n",
       "      <td>5.403240</td>\n",
       "      <td>5.979993</td>\n",
       "      <td>6.177425</td>\n",
       "      <td>6.654314</td>\n",
       "      <td>7.047680</td>\n",
       "      <td>7.348266</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>25.464286</td>\n",
       "      <td>6.616246</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>9.285714</td>\n",
       "      <td>163.099820</td>\n",
       "      <td>100.684505</td>\n",
       "      <td>1.421288</td>\n",
       "      <td>6.649411</td>\n",
       "      <td>7.740823</td>\n",
       "      <td>8.113419</td>\n",
       "      <td>...</td>\n",
       "      <td>7.724212</td>\n",
       "      <td>4.424847</td>\n",
       "      <td>5.028803</td>\n",
       "      <td>5.577369</td>\n",
       "      <td>6.031136</td>\n",
       "      <td>6.363244</td>\n",
       "      <td>6.711588</td>\n",
       "      <td>7.060610</td>\n",
       "      <td>7.424184</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>19.133333</td>\n",
       "      <td>5.909660</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>5.466667</td>\n",
       "      <td>165.378392</td>\n",
       "      <td>75.050748</td>\n",
       "      <td>1.361830</td>\n",
       "      <td>5.976143</td>\n",
       "      <td>4.137963</td>\n",
       "      <td>7.473203</td>\n",
       "      <td>...</td>\n",
       "      <td>3.446011</td>\n",
       "      <td>3.481240</td>\n",
       "      <td>3.907010</td>\n",
       "      <td>4.281861</td>\n",
       "      <td>4.720172</td>\n",
       "      <td>4.574711</td>\n",
       "      <td>4.320816</td>\n",
       "      <td>4.406719</td>\n",
       "      <td>4.342993</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>19.211268</td>\n",
       "      <td>5.922814</td>\n",
       "      <td>2.788732</td>\n",
       "      <td>5.802817</td>\n",
       "      <td>163.585967</td>\n",
       "      <td>75.352194</td>\n",
       "      <td>1.368851</td>\n",
       "      <td>5.990213</td>\n",
       "      <td>3.049394</td>\n",
       "      <td>7.502897</td>\n",
       "      <td>...</td>\n",
       "      <td>5.234112</td>\n",
       "      <td>4.248495</td>\n",
       "      <td>4.791650</td>\n",
       "      <td>5.245707</td>\n",
       "      <td>5.667723</td>\n",
       "      <td>5.793776</td>\n",
       "      <td>5.960844</td>\n",
       "      <td>5.839187</td>\n",
       "      <td>5.411088</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1102</th>\n",
       "      <td>27.312500</td>\n",
       "      <td>6.923266</td>\n",
       "      <td>3.406250</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>170.933703</td>\n",
       "      <td>108.146947</td>\n",
       "      <td>1.321618</td>\n",
       "      <td>6.921481</td>\n",
       "      <td>8.122396</td>\n",
       "      <td>8.348757</td>\n",
       "      <td>...</td>\n",
       "      <td>6.003578</td>\n",
       "      <td>3.921973</td>\n",
       "      <td>4.409763</td>\n",
       "      <td>4.898772</td>\n",
       "      <td>5.348595</td>\n",
       "      <td>5.559479</td>\n",
       "      <td>5.495630</td>\n",
       "      <td>5.703366</td>\n",
       "      <td>5.926259</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>31.785714</td>\n",
       "      <td>6.290479</td>\n",
       "      <td>3.357143</td>\n",
       "      <td>7.658730</td>\n",
       "      <td>161.230794</td>\n",
       "      <td>126.116779</td>\n",
       "      <td>1.718917</td>\n",
       "      <td>6.371939</td>\n",
       "      <td>5.511035</td>\n",
       "      <td>7.836350</td>\n",
       "      <td>...</td>\n",
       "      <td>5.931955</td>\n",
       "      <td>3.789855</td>\n",
       "      <td>4.367864</td>\n",
       "      <td>4.954506</td>\n",
       "      <td>5.430442</td>\n",
       "      <td>5.715330</td>\n",
       "      <td>6.065365</td>\n",
       "      <td>6.387320</td>\n",
       "      <td>5.916728</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>20.761905</td>\n",
       "      <td>6.057493</td>\n",
       "      <td>3.119048</td>\n",
       "      <td>6.476190</td>\n",
       "      <td>164.681027</td>\n",
       "      <td>81.668748</td>\n",
       "      <td>1.401802</td>\n",
       "      <td>6.120762</td>\n",
       "      <td>3.509921</td>\n",
       "      <td>7.596294</td>\n",
       "      <td>...</td>\n",
       "      <td>6.846092</td>\n",
       "      <td>3.974998</td>\n",
       "      <td>4.517704</td>\n",
       "      <td>5.125079</td>\n",
       "      <td>5.599347</td>\n",
       "      <td>5.933364</td>\n",
       "      <td>6.335871</td>\n",
       "      <td>6.708575</td>\n",
       "      <td>6.648544</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1105 rows  1525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature-0  feature-1  feature-2  feature-3   feature-4   feature-5  \\\n",
       "0     37.977273   6.758452   3.636364  10.792929  160.801682  151.109783   \n",
       "1     19.408163   5.933978   2.816327   5.877551  162.949911   76.153796   \n",
       "2     40.265306   7.425645   3.734694  13.160998  172.099640  161.790879   \n",
       "3     43.976744   7.648293   3.837209  14.392765  168.885456  175.277251   \n",
       "4     24.320988   6.534011   3.567901   8.913580  163.076959   96.019681   \n",
       "5     20.924051   6.134299   3.037975   6.506329  165.707039   82.761541   \n",
       "6     34.150000   6.740695   3.733333  10.214815  164.252922  135.639059   \n",
       "7     23.833333   6.395508   3.141026   8.717949  163.221967   94.106131   \n",
       "8     32.380952   6.152543   2.857143   6.402116  164.380868  128.391104   \n",
       "9     45.228571   6.608449   3.714286   9.180952  159.167580  180.141749   \n",
       "10    24.757143   6.241471   3.071429   6.949206  167.005923   97.692813   \n",
       "11    32.096774   7.206768   3.838710  13.806452  160.469926  127.646528   \n",
       "12    29.833333   6.436864   3.476190   8.296296  162.859109  118.257591   \n",
       "13    36.500000   6.700508   3.653846  10.709402  163.669041  145.244215   \n",
       "14    38.851852   7.402900   3.777778  12.979424  170.018323  154.527750   \n",
       "15    27.534483   6.269883   3.500000   7.973180  161.381053  109.038626   \n",
       "16    23.837500   6.722804   3.125000   7.875000  172.043543   93.939718   \n",
       "17    55.263158   7.389305   3.684211  12.257310  170.139845  220.606751   \n",
       "18    43.057692   7.061350   3.942308  11.871795  163.441127  171.586122   \n",
       "19    42.886792   7.052642   3.943396  12.100629  164.829574  170.922688   \n",
       "20    35.761905   6.858500   2.666667   9.640212  168.739205  141.875852   \n",
       "21    21.528571   6.482779   2.942857   6.400000  171.920444   84.572241   \n",
       "22    23.054795   6.395890   3.164384   8.301370  162.696886   90.885447   \n",
       "23    19.160000   6.067396   2.560000   5.920000  167.413784   75.058797   \n",
       "24    17.131868   5.762305   3.000000   4.109890  165.080662   66.885356   \n",
       "25    28.432432   6.603514   4.324324  11.027027  153.571767  112.900411   \n",
       "26    21.500000   6.207650   2.954545   7.272727  164.257367   85.172313   \n",
       "27    28.428571   6.252383   2.885714   7.384127  164.322747  112.473778   \n",
       "28    38.658537   6.752610   3.682927  10.753388  159.812730  153.837907   \n",
       "29    21.670588   6.292286   2.882353   6.776471  167.982829   85.273347   \n",
       "...         ...        ...        ...        ...         ...         ...   \n",
       "1075  23.849057   6.443296   3.584906   8.490566  165.656310   94.855312   \n",
       "1076  20.528302   6.044432   3.603774   6.075472  162.424590   80.699025   \n",
       "1077  20.440000   5.968498   3.700000   6.080000  161.064334   80.380964   \n",
       "1078  24.333333   5.957881   3.062500   5.509259  164.383290   95.963738   \n",
       "1079  23.375000   6.322417   3.203125   8.375000  162.588954   92.261393   \n",
       "1080  41.800000   9.073735   3.350000  21.400000  180.387654  166.508642   \n",
       "1081  18.390244   5.605241   3.000000   4.731707  158.624476   72.159797   \n",
       "1082  23.458333   6.397181   3.500000   8.375000  166.060607   93.350066   \n",
       "1083  25.809524   5.873886   3.380952   5.962963  162.051123  102.034265   \n",
       "1084  21.414634   5.975237   3.487805   6.926829  161.339153   84.405257   \n",
       "1085  19.000000   5.884906   2.759259   5.370370  162.831246   74.502082   \n",
       "1086  23.318182   6.330339   3.363636   8.045455  163.149857   92.872249   \n",
       "1087  20.560000   6.123796   2.680000   5.920000  165.088853   80.788719   \n",
       "1088  22.380952   6.182614   3.349206   7.396825  164.113227   89.449143   \n",
       "1089  25.161290   5.774461   2.806452   4.143369  166.212698   99.236969   \n",
       "1090  18.074074   5.806478   2.555556   4.518519  164.385174   70.742453   \n",
       "1091  21.284553   6.179102   3.219512   6.975610  164.084221   83.736404   \n",
       "1092  19.658537   5.914988   3.048780   5.756098  166.863082   77.226327   \n",
       "1093  17.720930   5.750228   2.697674   4.604651  164.527087   69.324920   \n",
       "1094  17.225806   5.579190   2.548387   4.193548  161.392477   67.383958   \n",
       "1095  25.841270   6.257190   3.301587   7.499118  165.929853  102.114039   \n",
       "1096  19.416667   6.040173   2.826389   5.763889  164.563288   76.121662   \n",
       "1097  22.877193   5.824621   2.912281   5.130604  162.065226   90.113408   \n",
       "1098  18.912281   5.693595   2.929825   5.228070  158.876430   74.254334   \n",
       "1099  25.464286   6.616246   3.750000   9.285714  163.099820  100.684505   \n",
       "1100  19.133333   5.909660   2.733333   5.466667  165.378392   75.050748   \n",
       "1101  19.211268   5.922814   2.788732   5.802817  163.585967   75.352194   \n",
       "1102  27.312500   6.923266   3.406250  10.750000  170.933703  108.146947   \n",
       "1103  31.785714   6.290479   3.357143   7.658730  161.230794  126.116779   \n",
       "1104  20.761905   6.057493   3.119048   6.476190  164.681027   81.668748   \n",
       "\n",
       "      feature-6  feature-7  feature-8  feature-9 ...   feature-1515  \\\n",
       "0      1.791689   6.818675   8.138413   8.270161 ...       5.658393   \n",
       "1      1.381401   6.002651   5.080499   7.514421 ...       4.830811   \n",
       "2      1.603976   7.410120  10.114794   8.805738 ...       6.397659   \n",
       "3      1.622298   7.629033  12.180817   9.070719 ...       5.879135   \n",
       "4      1.380679   6.566695   4.417010   8.058783 ...       8.148663   \n",
       "5      1.381957   6.187547   4.684599   7.660347 ...       6.087556   \n",
       "6      1.620887   6.781702   8.631090   8.248393 ...       6.198225   \n",
       "7      1.435936   6.443753   5.834402   7.904135 ...       6.582328   \n",
       "8      1.687697   6.232890   4.476844   7.736528 ...       0.000000   \n",
       "9      1.981354   6.690537   8.428546   8.221041 ...       5.214936   \n",
       "10     1.408460   6.289021   5.919754   7.789862 ...       6.175997   \n",
       "11     1.591140   7.228381  11.071685   8.598289 ...       7.242977   \n",
       "12     1.600911   6.497038   6.283812   7.960386 ...       5.658611   \n",
       "13     1.785651   6.764423   7.295706   8.158660 ...       5.768126   \n",
       "14     1.476580   7.381493  11.933931   8.867678 ...       3.218876   \n",
       "15     1.618478   6.345124   4.994148   7.791228 ...       8.934004   \n",
       "16     1.156748   6.710586   6.512587   8.234018 ...       5.638355   \n",
       "17     1.969099   7.412105  10.847813   8.885381 ...       0.000000   \n",
       "18     1.834505   7.104088   9.456211   8.542274 ...       6.706174   \n",
       "19     1.833416   7.095513   8.143800   8.514148 ...       6.976085   \n",
       "20     1.456355   6.864043  11.470610   8.439979 ...       0.000000   \n",
       "21     1.127633   6.481077   5.269841   8.031493 ...       5.036953   \n",
       "22     1.370996   6.435699   6.796613   7.942436 ...       6.182085   \n",
       "23     1.241288   6.107552   6.423333   7.651508 ...       0.000000   \n",
       "24     1.294529   5.828765   2.738324   7.375662 ...       6.448889   \n",
       "25     1.738632   6.683730   6.055743   8.059202 ...       8.293510   \n",
       "26     1.392726   6.258106   5.808081   7.738226 ...       7.443490   \n",
       "27     1.536709   6.312869   6.333953   7.825781 ...       3.791267   \n",
       "28     1.814410   6.815551   8.392444   8.278695 ...       5.800228   \n",
       "29     1.296235   6.326075   4.991830   7.822375 ...       6.577992   \n",
       "...         ...        ...        ...        ... ...            ...   \n",
       "1075   1.429258   6.483247   5.732180   7.917090 ...       8.145414   \n",
       "1076   1.402911   6.109594   3.789570   7.610526 ...       7.481855   \n",
       "1077   1.456437   6.045898   3.411944   7.531384 ...       7.520150   \n",
       "1078   1.496938   6.033144   3.958207   7.545887 ...       7.057353   \n",
       "1079   1.451475   6.377366   5.672743   7.837564 ...       6.824985   \n",
       "1080   1.000430   8.896735  19.768056  10.349211 ...       0.000000   \n",
       "1081   1.549658   5.717783   2.198509   7.184370 ...       7.163209   \n",
       "1082   1.430009   6.439448   5.322917   7.869501 ...       8.177485   \n",
       "1083   1.656037   5.975364   2.317542   7.431153 ...       7.555979   \n",
       "1084   1.534660   6.062522   3.468157   7.496018 ...       7.446209   \n",
       "1085   1.381487   5.956572   3.660108   7.471168 ...       5.514688   \n",
       "1086   1.484800   6.384780   4.174874   7.815815 ...       6.172744   \n",
       "1087   1.335059   6.174052   4.833333   7.684390 ...       0.000000   \n",
       "1088   1.521852   6.248717   4.873016   7.649627 ...       7.231355   \n",
       "1089   1.531852   5.860845   2.388103   7.386058 ...       3.954843   \n",
       "1090   1.348570   5.877785   3.391975   7.397610 ...       0.000000   \n",
       "1091   1.363989   6.230333   4.664747   7.732390 ...       8.626406   \n",
       "1092   1.395619   5.985012   2.447832   7.443992 ...       6.598232   \n",
       "1093   1.357124   5.825972   3.025840   7.340613 ...       5.196423   \n",
       "1094   1.452391   5.679165   2.590502   7.175556 ...       3.840795   \n",
       "1095   1.475191   6.312625   4.628210   7.790568 ...       8.508140   \n",
       "1096   1.298125   6.090735   4.419416   7.632631 ...       6.257668   \n",
       "1097   1.522993   5.913365   2.939753   7.426243 ...       5.575239   \n",
       "1098   1.530849   5.798249   2.397661   7.270916 ...       7.533869   \n",
       "1099   1.421288   6.649411   7.740823   8.113419 ...       7.724212   \n",
       "1100   1.361830   5.976143   4.137963   7.473203 ...       3.446011   \n",
       "1101   1.368851   5.990213   3.049394   7.502897 ...       5.234112   \n",
       "1102   1.321618   6.921481   8.122396   8.348757 ...       6.003578   \n",
       "1103   1.718917   6.371939   5.511035   7.836350 ...       5.931955   \n",
       "1104   1.401802   6.120762   3.509921   7.596294 ...       6.846092   \n",
       "\n",
       "      feature-1516  feature-1517  feature-1518  feature-1519  feature-1520  \\\n",
       "0         4.151040      4.540632      4.953183      5.351562      5.311048   \n",
       "1         3.817712      4.123094      4.426343      4.823804      4.652173   \n",
       "2         4.223177      4.685597      5.116870      5.333926      5.504569   \n",
       "3         4.280132      4.563045      5.007714      5.159773      5.393628   \n",
       "4         4.624973      5.173321      5.720312      6.259342      6.626469   \n",
       "5         4.430817      4.820282      5.183187      5.595176      5.489454   \n",
       "6         4.471639      4.801970      5.237107      5.493833      5.573816   \n",
       "7         4.600158      5.032071      5.499726      5.978728      5.995208   \n",
       "8         3.449988      3.865979      4.506730      4.765906      4.965028   \n",
       "9         3.828641      4.234107      4.682131      4.890349      5.192957   \n",
       "10        4.363099      4.716264      5.155457      5.591686      5.680173   \n",
       "11        4.094345      4.639572      5.151845      5.678037      5.850765   \n",
       "12        4.051785      4.519067      4.935373      5.281616      5.221369   \n",
       "13        3.931826      4.356709      4.844187      5.326662      5.340239   \n",
       "14        3.688879      3.761200      4.110874      4.442651      4.406719   \n",
       "15        4.644391      5.241747      5.930586      6.610360      7.088878   \n",
       "16        4.234107      4.532599      4.804021      5.043425      5.181784   \n",
       "17        3.617652      3.868593      4.357510      4.523146      4.789573   \n",
       "18        4.508108      4.898772      5.374989      5.679959      5.755742   \n",
       "19        4.572130      5.115746      5.566195      5.936711      6.080505   \n",
       "20        3.135494      2.484907      2.397895      2.302585      2.197225   \n",
       "21        3.931826      4.234107      4.442651      4.672829      4.753590   \n",
       "22        4.330733      4.605170      4.912655      5.181784      5.393628   \n",
       "23        2.833213      2.944439      2.944439      3.044522      3.044522   \n",
       "24        4.262680      4.624973      5.010635      5.351858      5.590987   \n",
       "25        4.363099      5.012301      5.675469      6.310259      6.686641   \n",
       "26        4.304065      4.828314      5.422745      6.018593      6.373640   \n",
       "27        3.772761      4.081766      4.514972      4.873765      4.997212   \n",
       "28        4.098503      4.492841      4.938513      5.120237      5.262042   \n",
       "29        4.385147      4.798885      5.216633      5.602695      5.857442   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1075      4.375757      5.032071      5.702949      6.339698      6.834210   \n",
       "1076      4.151040      4.734003      5.287636      5.850225      6.211102   \n",
       "1077      4.166665      4.725173      5.302683      5.881406      6.250458   \n",
       "1078      4.131159      4.601413      5.158696      5.690359      6.054403   \n",
       "1079      4.430817      4.919981      5.421641      5.892335      6.042336   \n",
       "1080      3.178054      2.890372      2.995732      3.135494      2.995732   \n",
       "1081      3.951244      4.559126      5.140200      5.685703      5.885409   \n",
       "1082      4.297285      4.978456      5.658611      6.322790      6.818753   \n",
       "1083      4.073291      4.650383      5.246695      5.735564      6.103816   \n",
       "1084      4.212128      4.888468      5.525951      5.975081      6.299380   \n",
       "1085      3.901973      4.327438      4.732904      5.096431      5.080239   \n",
       "1086      4.060443      4.569543      5.057837      5.539301      5.568821   \n",
       "1087      3.238678      3.669951      4.112921      4.525180      4.251170   \n",
       "1088      4.545951      5.133590      5.739994      6.264469      6.422816   \n",
       "1089      3.409496      3.868593      4.382808      4.539297      4.549261   \n",
       "1090      3.198673      3.630985      4.009603      4.409003      3.997053   \n",
       "1091      4.923624      5.447814      5.972218      6.539676      7.003520   \n",
       "1092      3.886705      4.389809      4.926801      5.261718      5.494090   \n",
       "1093      3.676301      4.123094      4.647990      5.085665      5.169063   \n",
       "1094      3.349904      3.682610      4.071161      4.447785      4.003918   \n",
       "1095      4.551242      5.176856      5.812076      6.371825      6.850507   \n",
       "1096      4.762174      5.105945      5.313206      5.537334      5.620401   \n",
       "1097      4.056123      4.501198      4.960657      5.184939      5.169418   \n",
       "1098      4.255613      4.826312      5.403240      5.979993      6.177425   \n",
       "1099      4.424847      5.028803      5.577369      6.031136      6.363244   \n",
       "1100      3.481240      3.907010      4.281861      4.720172      4.574711   \n",
       "1101      4.248495      4.791650      5.245707      5.667723      5.793776   \n",
       "1102      3.921973      4.409763      4.898772      5.348595      5.559479   \n",
       "1103      3.789855      4.367864      4.954506      5.430442      5.715330   \n",
       "1104      3.974998      4.517704      5.125079      5.599347      5.933364   \n",
       "\n",
       "      feature-1521  feature-1522  feature-1523    y  \n",
       "0         5.560922      5.643015      5.715999  0.0  \n",
       "1         4.795274      4.860781      5.001426  0.0  \n",
       "2         5.797956      6.009581      6.200889  0.0  \n",
       "3         5.640132      5.472271      5.741399  0.0  \n",
       "4         7.062406      7.472998      7.829842  0.0  \n",
       "5         5.604998      5.847522      5.987080  0.0  \n",
       "6         5.764799      5.865760      5.998937  0.0  \n",
       "7         6.179952      6.364051      6.481290  0.0  \n",
       "8         3.840795      3.595598      0.000000  0.0  \n",
       "9         5.342334      5.402677      5.303305  0.0  \n",
       "10        5.977302      6.030986      6.214671  0.0  \n",
       "11        6.134888      6.451753      6.793466  0.0  \n",
       "12        5.465948      5.520210      5.499982  0.0  \n",
       "13        5.395331      5.454787      5.557552  0.0  \n",
       "14        4.543295      4.605170      4.290459  0.0  \n",
       "15        7.641069      8.134765      8.607916  0.0  \n",
       "16        5.337538      5.497168      5.568345  0.0  \n",
       "17        4.523146      3.944006      0.000000  0.0  \n",
       "18        6.060582      6.240763      6.434747  0.0  \n",
       "19        6.198796      6.333335      6.652742  0.0  \n",
       "20        2.772589      0.000000      0.000000  0.0  \n",
       "21        4.762174      4.890349      4.969813  0.0  \n",
       "22        5.652489      5.831882      6.040255  0.0  \n",
       "23        2.708050      0.000000      0.000000  0.0  \n",
       "24        5.834811      6.077642      6.293419  0.0  \n",
       "25        7.157565      7.635515      8.018008  0.0  \n",
       "26        6.751321      7.048332      7.302612  0.0  \n",
       "27        4.848606      4.455074      4.352694  0.0  \n",
       "28        5.561162      5.568821      5.681878  0.0  \n",
       "29        6.151851      6.484856      6.339973  0.0  \n",
       "...            ...           ...           ...  ...  \n",
       "1075      7.294229      7.631557      8.007074  1.0  \n",
       "1076      6.591717      6.991033      7.274674  1.0  \n",
       "1077      6.631384      7.030719      7.325005  1.0  \n",
       "1078      6.455334      6.873095      6.939417  1.0  \n",
       "1079      6.333613      6.572807      6.736893  1.0  \n",
       "1080      3.218876      2.302585      0.000000  1.0  \n",
       "1081      6.272759      6.609097      6.919128  1.0  \n",
       "1082      7.288522      7.655010      8.058613  1.0  \n",
       "1083      6.545754      6.979000      7.346393  1.0  \n",
       "1084      6.681394      7.067183      7.171165  1.0  \n",
       "1085      5.365684      5.482980      5.400140  1.0  \n",
       "1086      5.728170      5.901779      5.917717  1.0  \n",
       "1087      4.104707      3.446011      3.056357  1.0  \n",
       "1088      6.771282      7.089077      7.031227  1.0  \n",
       "1089      4.743845      4.850075      4.323304  1.0  \n",
       "1090      4.066888      3.725693      2.784239  1.0  \n",
       "1091      7.460310      7.890512      8.276919  1.0  \n",
       "1092      5.865848      6.204873      6.369954  1.0  \n",
       "1093      5.358942      5.594479      5.380329  1.0  \n",
       "1094      4.150055      4.274928      4.123094  1.0  \n",
       "1095      7.327036      7.760788      8.122028  1.0  \n",
       "1096      5.805135      5.976351      6.091310  1.0  \n",
       "1097      5.403803      5.604422      5.480118  1.0  \n",
       "1098      6.654314      7.047680      7.348266  1.0  \n",
       "1099      6.711588      7.060610      7.424184  1.0  \n",
       "1100      4.320816      4.406719      4.342993  1.0  \n",
       "1101      5.960844      5.839187      5.411088  1.0  \n",
       "1102      5.495630      5.703366      5.926259  1.0  \n",
       "1103      6.065365      6.387320      5.916728  1.0  \n",
       "1104      6.335871      6.708575      6.648544  1.0  \n",
       "\n",
       "[1105 rows x 1525 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = _data_ = pd.read_csv('./data/train.csv')\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:10:22.740072Z",
     "start_time": "2018-04-30T20:10:22.382259Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c72bdaa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data.y.unique())\n",
    "plt.hist(data.y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:03:55.087094Z",
     "start_time": "2018-04-30T20:03:54.896235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "790 315 0.7149321266968326\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum().sum())\n",
    "\n",
    "num_1 = data[data.y == 1].shape[0] \n",
    "num_0 = data[data.y == 0].shape[0]\n",
    "\n",
    "print(num_1, num_0, num_1 / (num_0 + num_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:20:16.244188Z",
     "start_time": "2018-05-01T08:20:16.159887Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pp_pipeline(_data_)\n",
    "\n",
    "(train_X, train_y,\n",
    " test_X, test_y,\n",
    " validate_X, validate_y) = split_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:10:25.849656Z",
     "start_time": "2018-04-30T20:10:25.685800Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def weight_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.random_normal(shape=shape,\n",
    "                                                              stddev=0.1))\n",
    "def bias_init(name, shape):\n",
    "    return tf.get_variable(name, initializer=tf.constant(0.1, shape=shape))\n",
    "\n",
    "def elastic_net(x, l1, l2):\n",
    "    return l1 * ( (1-l2) / 2 * tf.norm(x, 2) ** 2 + \n",
    "                   l2 * tf.norm(x, 1))\n",
    "\n",
    "def batch_data(*matrxs, batch_size):\n",
    "    for batch_i in range(matrxs[0].shape[0] // batch_size):\n",
    "        yield tuple(x.iloc[batch_i * batch_size : (batch_i + 1) * batch_size] \n",
    "                     for x in matrxs)\n",
    "        \n",
    "def split_data(data, train_size=0.5, test_size=0.25, validate_size=0.25):\n",
    "    train_data, test_val_data = ms.train_test_split(data, train_size=train_size)\n",
    "    test_data, val_data = ms.train_test_split(test_val_data,\n",
    "                                              train_size=test_size / (test_size+validate_size))\n",
    "    \n",
    "    train_X, train_y = train_data.drop('y', axis=1), train_data[['y']]\n",
    "    test_X, test_y = test_data.drop('y', axis=1), test_data[['y']]\n",
    "    validate_X, validate_y = val_data.drop('y', axis=1), val_data[['y']]\n",
    "    \n",
    "    return (train_X, train_y, test_X, test_y, validate_X, validate_y)\n",
    "\n",
    "def pp_pipeline(data):\n",
    "    scaler = pp.StandardScaler()\n",
    "    \n",
    "    data_pp = scaler.fit_transform(data)\n",
    "    \n",
    "    data_pp = pd.DataFrame(data_pp,\n",
    "                        index=data.index,\n",
    "                        columns=data.columns)\n",
    "    \n",
    "    #restore target\n",
    "    data_pp.y = data.y\n",
    "    \n",
    "    return data_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:10:05.864014Z",
     "start_time": "2018-05-01T08:10:05.074115Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DFS(BaseEstimator):\n",
    "    def __init__(self, layers_sizes=[128, 64, 2], batch_size=32, lambda1=1e-3, lambda2=1.,\n",
    "                 alpha1=1e-3, alpha2=0., num_epochs=10, verbose=0, N=None):\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.num_layers = len(layers_sizes)\n",
    "        self.batch_size = batch_size\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.alpha1 = alpha1\n",
    "        self.alpha2 = alpha2\n",
    "        self.num_epochs = num_epochs\n",
    "        self.verbose = verbose\n",
    "        self.N = N\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, test_data=None):\n",
    "        self._build_graph_(X.shape[1])\n",
    "        self.features = X.columns #Persisting for `select_most_important_ftrs`\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch_i in range(self.num_epochs):\n",
    "            X_cur = X.sample(frac=1, random_state=epoch_i)\n",
    "            y_cur = y.sample(frac=1, random_state=epoch_i)\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in batch_data(X_cur, y_cur, \n",
    "                                               batch_size=self.batch_size):\n",
    "                train_loss, _ = self.sess.run([self.total_loss, self.train_step],\n",
    "                                               feed_dict = {self.x: batch_X,\n",
    "                                                            self.y: batch_y})\n",
    "                epoch_loss += train_loss\n",
    "            epoch_loss /= X.shape[0] // self.batch_size\n",
    "            \n",
    "            train_predict = self.predict(X_cur)\n",
    "            train_accuracy = mtcs.accuracy_score(y_cur, train_predict)\n",
    "            if self.verbose:\n",
    "                if test_data is not None:\n",
    "                    test_X, test_y = test_data\n",
    "                    test_predict = self.predict(test_X)\n",
    "                    test_accuracy = mtcs.accuracy_score(test_y, test_predict)\n",
    "                    print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}.\"\n",
    "                          f\"Train accuracy: {train_accuracy}. Test accuracy: {test_accuracy}.\")\n",
    "                else:\n",
    "                    print(f\"==> Epoch: {epoch_i}. Train loss: {epoch_loss}. \"\n",
    "                          f\"Train accuracy: {train_accuracy}.\")\n",
    "                \n",
    "        return self\n",
    "       \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        predictions_proba = self.sess.run(self.predictions, feed_dict={self.x: X})\n",
    "        \n",
    "        return predictions_proba\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions_proba = self.predict_proba(X)\n",
    "        \n",
    "        return list(map(np.argmax, predictions_proba))\n",
    "    \n",
    "    \n",
    "    def get_features_weights(self):\n",
    "        weights = self.sess.run(self.features_weights)\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs(self, N):\n",
    "        weights = self.get_features_weights()\n",
    "        feature_weight = sorted(zip(weights, self.features), \n",
    "                                key=lambda x: abs(x[0]))\n",
    "        \n",
    "        return map(lambda x: x[1], feature_weight[-N:])\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None):\n",
    "        if N:\n",
    "            features = list(self.select_most_important_ftrs(N))\n",
    "        else:\n",
    "            features = list(self.select_most_important_ftrs(self.N))\n",
    "        \n",
    "        return X[features]\n",
    "    \n",
    "     \n",
    "    def _build_graph_(self, num_features):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        ###Placeholders \n",
    "        x = tf.placeholder(tf.float32, [None, num_features], 'x_ph')\n",
    "        y = tf.placeholder(tf.int32, [None], 'y_ph')\n",
    "        \n",
    "        ###Weights initialization\n",
    "        w = tf.get_variable(\"dfs_features_weight\", \n",
    "                            initializer = tf.constant(1., shape=[num_features]))\n",
    "        self.layers_sizes = [num_features] + self.layers_sizes\n",
    "        W, b = [], []\n",
    "        for layer_i in range(self.num_layers):\n",
    "            W.append(weight_init(f\"layer_{layer_i}_weights\",\n",
    "                                 shape=[self.layers_sizes[layer_i],\n",
    "                                        self.layers_sizes[layer_i+1]]))\n",
    "            b.append(bias_init(f\"layer_{layer_i}_bias\",\n",
    "                               shape=[self.layers_sizes[layer_i+1]]))\n",
    "        \n",
    "        ###Input transformations\n",
    "        logits = x * w #feature selection\n",
    "        for layer_i in range(self.num_layers):\n",
    "            if layer_i != self.num_layers - 1:\n",
    "                logits = tf.nn.tanh(tf.matmul(logits, W[layer_i]) + b[layer_i])\n",
    "            else:\n",
    "                logits = tf.matmul(logits, W[layer_i]) + b[layer_i]\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "            \n",
    "        ###Loss calculation\n",
    "        logloss = tf.reduce_sum(\n",
    "                        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, \n",
    "                                                                       logits=logits))\n",
    "        w_loss = elastic_net(w, self.lambda1, self.lambda2)\n",
    "        W_loss = tf.reduce_sum([elastic_net(W_i, self.alpha1, self.alpha2) for W_i in W])\n",
    "        \n",
    "        total_loss = tf.reduce_sum(logloss + w_loss + W_loss)\n",
    "        \n",
    "        ###Optimizer\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(total_loss)\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.predictions = predictions\n",
    "        self.total_loss = total_loss\n",
    "        self.train_step = train_step\n",
    "        self.features_weights = w\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T19:51:34.522914Z",
     "start_time": "2018-04-30T19:51:23.872464Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Epoch: 0. Train loss: 20.95456891901353.Train accuracy: 0.8351449275362319. Test accuracy: 0.7536231884057971.\n",
      "==> Epoch: 1. Train loss: 12.762776206521426.Train accuracy: 0.9456521739130435. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 2. Train loss: 9.08758960050695.Train accuracy: 0.9637681159420289. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 3. Train loss: 6.838204552145565.Train accuracy: 0.9710144927536232. Test accuracy: 0.8188405797101449.\n",
      "==> Epoch: 4. Train loss: 5.353214488309972.Train accuracy: 0.9927536231884058. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 5. Train loss: 4.301174023572137.Train accuracy: 0.9981884057971014. Test accuracy: 0.8442028985507246.\n",
      "==> Epoch: 6. Train loss: 3.583045917398789.Train accuracy: 0.9981884057971014. Test accuracy: 0.8260869565217391.\n",
      "==> Epoch: 7. Train loss: 3.1851401188794304.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 8. Train loss: 2.95365205933066.Train accuracy: 1.0. Test accuracy: 0.8188405797101449.\n",
      "==> Epoch: 9. Train loss: 2.818457982119392.Train accuracy: 1.0. Test accuracy: 0.8260869565217391.\n",
      "==> Epoch: 10. Train loss: 2.7245663474587833.Train accuracy: 1.0. Test accuracy: 0.8260869565217391.\n",
      "==> Epoch: 11. Train loss: 2.65560806498808.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 12. Train loss: 2.6058782549465405.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 13. Train loss: 2.5639125599580654.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 14. Train loss: 2.540072385002585.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 15. Train loss: 2.516804639030905.Train accuracy: 1.0. Test accuracy: 0.8188405797101449.\n",
      "==> Epoch: 16. Train loss: 2.498416718314676.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 17. Train loss: 2.4801288773031795.Train accuracy: 1.0. Test accuracy: 0.8188405797101449.\n",
      "==> Epoch: 18. Train loss: 2.465712687548469.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 19. Train loss: 2.4529750066645004.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 20. Train loss: 2.440108201083015.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 21. Train loss: 2.428684851702522.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 22. Train loss: 2.4167918036965763.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 23. Train loss: 2.4082342316122616.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 24. Train loss: 2.398668681873995.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 25. Train loss: 2.388799218570485.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 26. Train loss: 2.380131146487068.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 27. Train loss: 2.3713068260866055.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 28. Train loss: 2.363390978644876.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 29. Train loss: 2.3556337356567383.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 30. Train loss: 2.3471133007722744.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 31. Train loss: 2.3391076256247127.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 32. Train loss: 2.3319051405962776.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 33. Train loss: 2.3239885919234333.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 34. Train loss: 2.316179682226742.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 35. Train loss: 2.3084878360523895.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 36. Train loss: 2.300882143132827.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 37. Train loss: 2.2928822180804085.Train accuracy: 1.0. Test accuracy: 0.822463768115942.\n",
      "==> Epoch: 38. Train loss: 2.285295107785393.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 39. Train loss: 2.277549617430743.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 40. Train loss: 2.2698959042044247.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 41. Train loss: 2.262169866000905.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 42. Train loss: 2.254467080621158.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 43. Train loss: 2.246766118442311.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 44. Train loss: 2.238993195926442.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 45. Train loss: 2.2313761290381935.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 46. Train loss: 2.2235234484953037.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 47. Train loss: 2.215578822528615.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 48. Train loss: 2.207807316499598.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 49. Train loss: 2.1999063632067513.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 50. Train loss: 2.1918292746824375.Train accuracy: 1.0. Test accuracy: 0.8297101449275363.\n",
      "==> Epoch: 51. Train loss: 2.1840683151693905.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 52. Train loss: 2.17604923248291.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 53. Train loss: 2.168132178923663.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 54. Train loss: 2.1600881744833553.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 55. Train loss: 2.1520459371454574.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 56. Train loss: 2.143694470910465.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 57. Train loss: 2.135784065022188.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 58. Train loss: 2.1276205988491284.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 59. Train loss: 2.120927754570456.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 60. Train loss: 2.114722967147827.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 61. Train loss: 2.1084935665130615.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 62. Train loss: 2.1022589487188004.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 63. Train loss: 2.0960802611182716.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 64. Train loss: 2.0897181034088135.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 65. Train loss: 2.083319159115062.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 66. Train loss: 2.0770844431484448.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 67. Train loss: 2.070637590744916.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 68. Train loss: 2.0642572711495792.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 69. Train loss: 2.0577841506284824.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 70. Train loss: 2.0512344135957608.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 71. Train loss: 2.0446997249827668.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 72. Train loss: 2.0380651389851288.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 73. Train loss: 2.031332576976103.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 74. Train loss: 2.0248599613414093.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 75. Train loss: 2.0181612547706154.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n",
      "==> Epoch: 76. Train loss: 2.011453530367683.Train accuracy: 1.0. Test accuracy: 0.8333333333333334.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-171-bf60af11070d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDFS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-169-0677f3d96a3c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, num_epochs, test_data)\u001b[0m\n\u001b[1;32m     24\u001b[0m             for batch_X, batch_y in batch_data(X_cur, y_cur, \n\u001b[1;32m     25\u001b[0m                                                batch_size=self.batch_size):\n\u001b[0;32m---> 26\u001b[0;31m                 train_loss, _ = self.sess.run([self.total_loss, self.train_step],\n\u001b[0m\u001b[1;32m     27\u001b[0m                                                feed_dict = {self.x: batch_X,\n\u001b[1;32m     28\u001b[0m                                                             self.y: batch_y})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dfs = DFS([128, 64, 2])\n",
    "\n",
    "dfs.fit(train_X, train_y['y'], num_epochs=200, test_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:07:08.828252Z",
     "start_time": "2018-04-30T16:07:08.498387Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 149.,   14.,   16.,   36.,  118.,  386.,  499.,  271.,   30.,    5.]),\n",
       " array([-0.01604488,  0.08977009,  0.19558506,  0.30140003,  0.40721501,\n",
       "         0.51302998,  0.61884495,  0.72465992,  0.8304749 ,  0.93628987,\n",
       "         1.04210484]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d3e41668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(dfs.get_features_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:29:16.228831Z",
     "start_time": "2018-05-01T08:29:16.131393Z"
    }
   },
   "outputs": [],
   "source": [
    "class LassoFS(BaseEstimator):\n",
    "    def __init__(self, N=None):\n",
    "        self.N = N\n",
    "        self.est = lm.LogisticRegression(penalty='l1')\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.est.fit(train_X, train_y)\n",
    "        self.features = train_X.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None):\n",
    "        if N:\n",
    "            features = list(self.select_most_important_ftrs(N))\n",
    "        else:\n",
    "            features = list(self.select_most_important_ftrs(self.N))\n",
    "        \n",
    "        return X[features]\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.est.predict(X)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def coef_(self):\n",
    "        return self.est.coef_\n",
    "\n",
    "\n",
    "    def select_most_important_ftrs(self, N):\n",
    "        feature_weight = sorted(zip(self.est.coef_[0], self.features),\n",
    "                                key=lambda x: abs(x[0]))\n",
    "\n",
    "        return list(map(lambda x: x[1], feature_weight[-N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:22:36.588789Z",
     "start_time": "2018-05-01T08:22:36.130699Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851449275362\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3aff9540b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = LassoFS();\n",
    "est.fit(train_X, train_y.y);\n",
    "print(mtcs.accuracy_score(test_y, est.predict(test_X)))\n",
    "plt.hist(est.coef_[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:22:52.560361Z",
     "start_time": "2018-05-01T08:22:52.471712Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForestFS(BaseEstimator):\n",
    "    def __init__(self, N=None):\n",
    "        self.N = N\n",
    "        self.est = ens.RandomForestClassifier()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.est.fit(X, y)\n",
    "        self.features = X.columns\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def transform(self, X, N=None):\n",
    "        if N:\n",
    "            features = self.select_most_important_ftrs(N)\n",
    "        else:\n",
    "            features = self.select_most_important_ftrs(self.N)\n",
    "            \n",
    "        return X[features]\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.est.predict(X)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        return self.est.feature_importances_\n",
    "    \n",
    "    \n",
    "    def select_most_important_ftrs(self, N):\n",
    "        feature_weight = sorted( zip(self.est.feature_importances_,\n",
    "                                     self.features),\n",
    "                                 key=lambda x: x[0])\n",
    "        return list(map(lambda x: x[1], feature_weight[-N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:23:09.918825Z",
     "start_time": "2018-05-01T08:23:09.494223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.836956521739\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3b1534bc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "est = RandomForestFS();\n",
    "est.fit(train_X, train_y);\n",
    "print(mtcs.accuracy_score(test_y, est.predict(test_X)))\n",
    "plt.hist(est.feature_importances_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:49:24.842583Z",
     "start_time": "2018-04-30T16:49:24.706909Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomForestFS(BaseEstimator):\n",
    "    def __init__(self)\n",
    "\n",
    "est = ens.RandomForestClassifier()\n",
    "\n",
    "train_X_dfs = dfs.transform(train_X, 500)\n",
    "test_X_dfs = dfs.transform(test_X, 500)\n",
    "\n",
    "est.fit(train_X_dfs, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:09:58.247073Z",
     "start_time": "2018-04-30T16:09:58.233526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87681159420289856"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X_dfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:17:34.271072Z",
     "start_time": "2018-04-30T16:17:33.834249Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d9ce8358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(array([  1.,   2.,   0.,   8.,  14.,  20.,  26.,  22.,   4.,   3.]),\n",
       " array([ 0.79710145,  0.80724638,  0.8173913 ,  0.82753623,  0.83768116,\n",
       "         0.84782609,  0.85797101,  0.86811594,  0.87826087,  0.8884058 ,\n",
       "         0.89855072]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f48d5f7cc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(acc_p)\n",
    "plt.show()\n",
    "plt.hist(acc_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:30:31.617460Z",
     "start_time": "2018-04-30T16:30:31.607461Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.8637867739884362, 0.062351653820433672)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ztest(acc_p, acc_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_important_ftrs(est, features, N):\n",
    "    feature_weight = sorted(zip(est.coef_[0], features),\n",
    "                            key=lambda x: abs(x[0]))\n",
    "    \n",
    "    return list(map(lambda x: x[1], feature_weight[-N:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:50:43.173156Z",
     "start_time": "2018-04-30T16:49:42.223366Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:95: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:128: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=1000,\n",
       "       n_jobs=40, nthread=None, objective='binary:logistic',\n",
       "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=False, subsample=1)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = XGBClassifier(n_estimators=1000, n_jobs=40, silent=False)\n",
    "\n",
    "est.fit(train_X_dfs, train_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:46:40.858161Z",
     "start_time": "2018-04-30T16:46:40.800863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8876811594202898"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcs.accuracy_score(test_y, est.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T16:46:44.087000Z",
     "start_time": "2018-04-30T16:46:44.049767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., ...,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:29:19.376210Z",
     "start_time": "2018-05-01T08:29:19.363978Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('select_ftrs', LassoFS()),\n",
    "    ('main', ens.RandomForestClassifier())\n",
    "])\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "#      'select_ftrs__lambda1': [1e-3, 5e-3],\n",
    "#      'select_ftrs__num_epochs': [10, 100]\n",
    "     'select_ftrs__N': [0, 100]\n",
    "#      'main__n_estimators': [10, 25, 50]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:29:21.750449Z",
     "start_time": "2018-05-01T08:29:19.821290Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "[CV] select_ftrs__N=0 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................... select_ftrs__N=0, score=0.875, total=   0.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[CV] select_ftrs__N=0 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... select_ftrs__N=0, score=0.8260869565217391, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.7s remaining:    0.0s\n",
      "[CV] select_ftrs__N=0 ................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ....... select_ftrs__N=0, score=0.7989130434782609, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s remaining:    0.0s\n",
      "[CV] select_ftrs__N=100 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... select_ftrs__N=100, score=0.8804347826086957, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    1.2s remaining:    0.0s\n",
      "[CV] select_ftrs__N=100 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... select_ftrs__N=100, score=0.8369565217391305, total=   0.2s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.5s remaining:    0.0s\n",
      "[CV] select_ftrs__N=100 ..............................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ..... select_ftrs__N=100, score=0.8478260869565217, total=   0.3s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('select_ftrs', LassoFS(N=None)), ('main', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "   ..._jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'select_ftrs__N': [0, 100]}], pre_dispatch='2*n_jobs',\n",
       "       refit=True, return_train_score='warn', scoring='accuracy',\n",
       "       verbose=100)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = ms.GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid,\n",
    "                       verbose=100, scoring='accuracy')\n",
    "grid.fit(train_X, train_y.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-01T08:29:26.891739Z",
     "start_time": "2018-05-01T08:29:26.883249Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.85507246376811596, {'select_ftrs__N': 100})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_, grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-30T20:41:47.832183Z",
     "start_time": "2018-04-30T20:41:47.809605Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.59288263,  9.28491696,  1.65082256,  9.04132112]),\n",
       " 'mean_score_time': array([ 0.0058229 ,  0.00650056,  0.00610256,  0.00602126]),\n",
       " 'mean_test_score': array([ 0.86050725,  0.8423913 ,  0.8442029 ,  0.86050725]),\n",
       " 'mean_train_score': array([ 1.,  1.,  1.,  1.]),\n",
       " 'param_select_ftrs__lambda1': masked_array(data = [0.001 0.001 0.005 0.005],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_select_ftrs__num_epochs': masked_array(data = [10 100 10 100],\n",
       "              mask = [False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': [{'select_ftrs__lambda1': 0.001, 'select_ftrs__num_epochs': 10},\n",
       "  {'select_ftrs__lambda1': 0.001, 'select_ftrs__num_epochs': 100},\n",
       "  {'select_ftrs__lambda1': 0.005, 'select_ftrs__num_epochs': 10},\n",
       "  {'select_ftrs__lambda1': 0.005, 'select_ftrs__num_epochs': 100}],\n",
       " 'rank_test_score': array([1, 4, 3, 1], dtype=int32),\n",
       " 'split0_test_score': array([ 0.84782609,  0.83152174,  0.81521739,  0.8423913 ]),\n",
       " 'split0_train_score': array([ 1.,  1.,  1.,  1.]),\n",
       " 'split1_test_score': array([ 0.83152174,  0.80978261,  0.83152174,  0.83152174]),\n",
       " 'split1_train_score': array([ 1.,  1.,  1.,  1.]),\n",
       " 'split2_test_score': array([ 0.90217391,  0.88586957,  0.88586957,  0.9076087 ]),\n",
       " 'split2_train_score': array([ 1.,  1.,  1.,  1.]),\n",
       " 'std_fit_time': array([ 0.05695909,  0.38536184,  0.08812519,  0.2596785 ]),\n",
       " 'std_score_time': array([ 0.00019598,  0.0002795 ,  0.00028952,  0.00024663]),\n",
       " 'std_test_score': array([ 0.03020531,  0.03199913,  0.03020531,  0.03360007]),\n",
       " 'std_train_score': array([ 0.,  0.,  0.,  0.])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "0px",
    "width": "250px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "1182px",
    "left": "0px",
    "right": "1792.4815673828125px",
    "top": "52.22426223754883px",
    "width": "184px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
